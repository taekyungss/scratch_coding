{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:2.5em\">2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í† í°í™” (1)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6Qj3ZjBA5j7"
   },
   "source": [
    "[ì°¸ì¡°]\n",
    "- [Natural Language Processing with Python (NLTK Books)](https://www.nltk.org/book/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color: crimson\">[RemarkğŸ]</span> ì¥¬í”¼í„° ì…€ì—ì„œ íŒŒì´ì¬ íŒ¨í‚¤ì§€ ì„¤ì¹˜ì‹œ ê¶Œì¥ë˜ëŠ” ë°©ë²•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import sys\n",
    "\n",
    "!conda install --yes --prefix {sys.prefix} package_to_install\n",
    "# ë˜ëŠ”\n",
    "!{sys.executable} -m pip install package_to_install\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ì°¸ì¡°] [Installing Python Packages from a Jupyter Notebook](https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3bN6iK49Gp4"
   },
   "source": [
    "# 2.1 ë‹¨ì–´ í† í°í™”(Word Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sxtCj_x9Dtm"
   },
   "source": [
    "### Tokenizer ë¹„êµ - ì„ íƒì˜ ë¬¸ì œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì„ íƒì˜ ë¬¸ì œëŠ” ë‚´ê°€ ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” í…ìŠ¤íŠ¸ì˜ íŠ¹ì„±ê³¼ í† í°í™” ëª©ì ì— ë”°ë¼ ë§ëŠ” tokenizerë¥¼ ì„ íƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nzfDhsYm852i"
   },
   "outputs": [],
   "source": [
    "sentence = \"Mr. Jone's house isn't far from here, New York.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë„ì–´ì“°ê¸°(space)ë¡œ í† í°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.', \"Jone's\", 'house', \"isn't\", 'far', 'from', 'here,', 'New', 'York.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ë©€ë¦¬', 'ê°€ì§€', 'ë§ˆ.', 'ê¹€', 'ë°•ì‚¬ë‹˜', 'ë„ˆë¥¼', 'ë³´ëŸ¬', 'ì˜¤ì‹¤ê±°ì•¼.', 'ë„¤ê°€', 'ì—†ìœ¼ë©´', 'ë‹¹í™©ìŠ¤ëŸ¬ìš¸ê±°ì•¼.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"ë©€ë¦¬ ê°€ì§€ ë§ˆ. ê¹€ ë°•ì‚¬ë‹˜ ë„ˆë¥¼ ë³´ëŸ¬ ì˜¤ì‹¤ê±°ì•¼. ë„¤ê°€ ì—†ìœ¼ë©´ ë‹¹í™©ìŠ¤ëŸ¬ìš¸ê±°ì•¼.\"\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK `word_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\taekyung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\taekyung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Jone's house isn't far from here, New York.\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lTs0hQ8HRDYl",
    "outputId": "75e4ad63-1081-4446-b38f-29ffd0b457bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Jone', \"'s\", 'house', 'is', \"n't\", 'far', 'from', 'here', ',', 'New', 'York', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK `WordPunctTokenizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìœ„ì˜ word_tokenizeì™€ëŠ” ë‹¬ë¦¬ êµ¬ë‘ì (') ì„ ë³„ë„ì˜ í† í°ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ í† í°í™”ë¥¼ ì§„í–‰\n",
    "\n",
    "ê°„ë‹¨í•˜ê³  ë¹ ë¥´ê²Œ ë™ì‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WFDQA6JLR5HT",
    "outputId": "9b3a1f99-708e-4dd0-cf44-3d85b2a07c08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr', '.', 'Jone', \"'\", 's', 'house', 'isn', \"'\", 't', 'far', 'from', 'here', ',', 'New', 'York', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(WordPunctTokenizer().tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'do', 'uh', 'main', 'mainly', 'business', 'data', 'processing']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(\"I do uh main mainly business data processing\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK `TreebankWordTokenizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¢€ ë” ì •êµí•œ í† í°í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤. \n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´, \"don't\"ì™€ \"won't\"ê³¼ ê°™ì€ ì¶•ì•½í˜•ì— ëŒ€í•´ì„œë„ ì´ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì¸ì‹í•˜ì—¬ \"do\", \"n't\", \"wo\", \"n't\"ë¡œ ë¶„ë¦¬\n",
    "\n",
    " êµ¬ì–´ì²´ë‚˜ ë¬¸ë²•ì ìœ¼ë¡œ ë³µì¡í•œ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•  ë•Œ ìœ ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Jone', \"'s\", 'house', 'is', \"n't\", 'far', 'from', 'here', ',', 'New', 'York', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EhsH1n5uR6g7",
    "outputId": "26db1c27-6e07-4418-ea94-6e88cecf4443"
   },
   "source": [
    "#### PyTorch torchtextê°€ ì œê³µí•˜ëŠ” tokenizer ì¤‘ `basic_english`\n",
    "- torchtextê°€ ì œê³µí•˜ëŠ” tokenizer: `basic_english`, `spacy`, `moses`, `toktok`, `revtok`, `subword`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from torchtext) (4.65.0)\n",
      "Requirement already satisfied: requests in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from torchtext) (2.31.0)\n",
      "Requirement already satisfied: torch==2.2.1 in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from torchtext) (2.2.1+cu118)\n",
      "Requirement already satisfied: numpy in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from torchtext) (1.24.1)\n",
      "Requirement already satisfied: torchdata==0.7.1 in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from torchtext) (0.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch==2.2.1->torchtext) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch==2.2.1->torchtext) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch==2.2.1->torchtext) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch==2.2.1->torchtext) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch==2.2.1->torchtext) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch==2.2.1->torchtext) (2024.2.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from torchdata==0.7.1->torchtext) (1.26.18)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->torchtext) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from tqdm->torchtext) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from jinja2->torch==2.2.1->torchtext) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from sympy->torch==2.2.1->torchtext) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mr', '.', 'jone', \"'\", 's', 'house', 'isn', \"'\", 't', 'far', 'from', 'here', ',', 'new', 'york', '.']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "print(get_tokenizer(\"basic_english\")(sentence))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow `text_to_word_sequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "['mr', \"jone's\", 'house', \"isn't\", 'far', 'from', 'here', 'new', 'york']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "print(text_to_word_sequence(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- í…ì„œí”Œë¡œìš° tensorflow.keras.preprocessing.textì˜ `text_to_word_sequence` ë‚˜ `Tokenizer`ëŠ” `tf.Tensor`ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ\n",
    "- ê¶Œì¥ ë°©ë²• ì°¸ê³ : [í…ìŠ¤íŠ¸ ë¡œë“œ í•˜ê¸°](https://www.tensorflow.org/tutorials/load_data/text?hl=ko)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6lWTou49PLA"
   },
   "source": [
    "# 2.2. ë¬¸ì¥ ë¶„í• (Sentence Segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK `sent_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2WixyvHR9US",
    "outputId": "a5cb0c43-f899-4f99-812c-cde44051c209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't go far.\", 'Dr. Kim is coming to see you.', \"I'd be embarrassed without you.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Don't go far. Dr. Kim is coming to see you. I'd be embarrassed without you.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ë©€ë¦¬ ê°€ì§€ ë§ˆ.', 'ê¹€ ë°•ì‚¬ë‹˜ ë„ˆë¥¼ ë³´ëŸ¬ ì˜¤ì‹¤ê±°ì•¼.', 'ë„¤ê°€ ì—†ìœ¼ë©´ ë‹¹í™©ìŠ¤ëŸ¬ìš¸ê±°ì•¼.']\n"
     ]
    }
   ],
   "source": [
    "text = \"ë©€ë¦¬ ê°€ì§€ ë§ˆ. ê¹€ ë°•ì‚¬ë‹˜ ë„ˆë¥¼ ë³´ëŸ¬ ì˜¤ì‹¤ê±°ì•¼. ë„¤ê°€ ì—†ìœ¼ë©´ ë‹¹í™©ìŠ¤ëŸ¬ìš¸ê±°ì•¼.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go to the IP 192.168.56.31 server, save the log file, and send the results to aaa@gmail.com.', \"After that, let's go have lunch.\"]\n"
     ]
    }
   ],
   "source": [
    "text = \"Go to the IP 192.168.56.31 server, save the log file, and send the results to aaa@gmail.com. After that, let's go have lunch.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IP 192.168.56.31 ì„œë²„ì— ë“¤ì–´ê°€ì„œ ë¡œê·¸ íŒŒì¼ ì €ì¥í•´ì„œ aaa@gmail.com ë¡œ ê²°ê³¼ ì¢€ ë³´ë‚´ì¤˜.', 'ê·¸ í›„ ì ì‹¬ ë¨¹ìœ¼ëŸ¬ ê°€ì.']\n"
     ]
    }
   ],
   "source": [
    "text = \"IP 192.168.56.31 ì„œë²„ì— ë“¤ì–´ê°€ì„œ ë¡œê·¸ íŒŒì¼ ì €ì¥í•´ì„œ aaa@gmail.com ë¡œ ê²°ê³¼ ì¢€ ë³´ë‚´ì¤˜. ê·¸ í›„ ì ì‹¬ ë¨¹ìœ¼ëŸ¬ ê°€ì.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KSS `split_sentences`\n",
    "- KSS(Korean Sentence Splitter) - ë°•ìƒê¸¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kss in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (4.5.4)\n",
      "Requirement already satisfied: emoji==1.2.0 in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from kss) (1.2.0)\n",
      "Requirement already satisfied: regex in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from kss) (2023.10.3)\n",
      "Requirement already satisfied: pecab in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from kss) (1.0.8)\n",
      "Requirement already satisfied: networkx in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from kss) (3.2.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from pecab->kss) (1.24.1)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from pecab->kss) (15.0.0)\n",
      "Requirement already satisfied: pytest in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from pecab->kss) (8.0.2)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from pytest->pecab->kss) (2.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from pytest->pecab->kss) (23.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=1.3.0 in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from pytest->pecab->kss) (1.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from pytest->pecab->kss) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from pytest->pecab->kss) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from pytest->pecab->kss) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KIDooFnDSM-7",
    "outputId": "d1ff8037-d3b5-4dda-d71a-5703a0c4386b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
      "For your information, Kss also supports mecab backend.\n",
      "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
      "Please refer to following web sites for details:\n",
      "- mecab: https://cleancode-ws.tistory.com/97\n",
      "- konlpy.tag.Mecab: https://uwgdqo.tistory.com/363\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ë©€ë¦¬ ê°€ì§€ ë§ˆ.', 'ê¹€ ë°•ì‚¬ë‹˜ ë„ˆë¥¼ ë³´ëŸ¬ ì˜¤ì‹¤ê±°ì•¼.', 'ë„¤ê°€ ì—†ìœ¼ë©´ ë‹¹í™©ìŠ¤ëŸ¬ìš¸ê±°ì•¼.']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "text = \"ë©€ë¦¬ ê°€ì§€ ë§ˆ. ê¹€ ë°•ì‚¬ë‹˜ ë„ˆë¥¼ ë³´ëŸ¬ ì˜¤ì‹¤ê±°ì•¼. ë„¤ê°€ ì—†ìœ¼ë©´ ë‹¹í™©ìŠ¤ëŸ¬ìš¸ê±°ì•¼.\"\n",
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [MS Windows ì— MeCab ì„¤ì¹˜í•˜ëŠ” ë°©ë²•](https://wonhwa.tistory.com/49)\n",
    "- [Windows Python 3.xì— MeCab ì„¤ì¹˜ë²•](https://cleancode-ws.tistory.com/97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IP 192.168.56.31 ì„œë²„ì— ë“¤ì–´ê°€ì„œ ë¡œê·¸ íŒŒì¼ ì €ì¥í•´ì„œ aaa@gmail.com ë¡œ ê²°ê³¼ ì¢€ ë³´ë‚´ì¤˜.', 'ê·¸ í›„ ì ì‹¬ ë¨¹ìœ¼ëŸ¬ ê°€ì.']\n"
     ]
    }
   ],
   "source": [
    "text = \"IP 192.168.56.31 ì„œë²„ì— ë“¤ì–´ê°€ì„œ ë¡œê·¸ íŒŒì¼ ì €ì¥í•´ì„œ aaa@gmail.com ë¡œ ê²°ê³¼ ì¢€ ë³´ë‚´ì¤˜. ê·¸ í›„ ì ì‹¬ ë¨¹ìœ¼ëŸ¬ ê°€ì.\"\n",
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Go to the IP 192.168.56.31 server, save the log file, and send the results to aaa@gmail.com. After that, let's go have lunch.\"]\n"
     ]
    }
   ],
   "source": [
    "text = \"Go to the IP 192.168.56.31 server, save the log file, and send the results to aaa@gmail.com. After that, let's go have lunch.\"\n",
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ë¬¸ì¥ ë¶„í•  ì‹¤íŒ¨!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8znWPo59hX9"
   },
   "source": [
    "# 2.3 í† í°í™” ë° POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "MrD41LYKSQGb"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f-pa-nMwSibh",
    "outputId": "bf22c9b2-98de-4f81-af2d-812fb169a3fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‹¨ì–´ í† í°í™” : ['Do', \"n't\", 'go', 'far', '.', 'Dr.', 'Kim', 'is', 'coming', 'to', 'see', 'you', '.', 'I', \"'d\", 'be', 'embarrassed', 'without', 'you', '.']\n",
      "í’ˆì‚¬ íƒœê¹… : [('Do', 'VBP'), (\"n't\", 'RB'), ('go', 'VB'), ('far', 'RB'), ('.', '.'), ('Dr.', 'NNP'), ('Kim', 'NNP'), ('is', 'VBZ'), ('coming', 'VBG'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('.', '.'), ('I', 'PRP'), (\"'d\", 'MD'), ('be', 'VB'), ('embarrassed', 'VBN'), ('without', 'IN'), ('you', 'PRP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text = \"Don't go far. Dr. Kim is coming to see you. I'd be embarrassed without you.\"\n",
    "tokenized_sentence = word_tokenize(text)\n",
    "\n",
    "print('ë‹¨ì–´ í† í°í™” :',tokenized_sentence)\n",
    "print('í’ˆì‚¬ íƒœê¹… :',pos_tag(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'go', 'far', '.']\n",
      "[('Do', 'VBP'), (\"n't\", 'RB'), ('go', 'VB'), ('far', 'RB'), ('.', '.')]\n",
      "['Dr.', 'Kim', 'is', 'coming', 'to', 'see', 'you', '.']\n",
      "[('Dr.', 'NNP'), ('Kim', 'NNP'), ('is', 'VBZ'), ('coming', 'VBG'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('.', '.')]\n",
      "['I', \"'d\", 'be', 'embarrassed', 'without', 'you', '.']\n",
      "[('I', 'PRP'), (\"'d\", 'MD'), ('be', 'VB'), ('embarrassed', 'VBN'), ('without', 'IN'), ('you', 'PRP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "for sent in sentences:\n",
    "    tokenized = word_tokenize(sent)\n",
    "    print(tokenized)\n",
    "    print(pos_tag(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoNLPy\n",
    "- ì§€ì›í•˜ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°: Okt(Open Korea Text), ë©”ìº…(Mecab), ì½”ëª¨ë€(Komoran), í•œë‚˜ëˆ”(Hannanum), ê¼¬ê¼¬ë§ˆ(Kkma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from konlpy) (1.5.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from konlpy) (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from konlpy) (1.24.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (23.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "c9BJZNPZSnd7"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "GAYMyRBg9noI"
   },
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "kkma = Kkma()\n",
    "text = \"3ì›”ì— ëŒ€í•™ìƒë“¤ì´ ê°•í•œ ì´ìœ ëŠ”? ê°œê°•í•´ì„œ!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OKT(Open Korea Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oH4Ye8cg9q1H",
    "outputId": "672a8e4e-0452-454c-c881-80e184cd3c19",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT í˜•íƒœì†Œ ë¶„ì„ : ['3ì›”', 'ì—', 'ëŒ€í•™ìƒ', 'ë“¤', 'ì´', 'ê°•í•œ', 'ì´ìœ ', 'ëŠ”', '?', 'ê°œê°•', 'í•´ì„œ', '!']\n",
      "OKT í’ˆì‚¬ íƒœê¹… : [('3ì›”', 'Number'), ('ì—', 'Foreign'), ('ëŒ€í•™ìƒ', 'Noun'), ('ë“¤', 'Suffix'), ('ì´', 'Josa'), ('ê°•í•œ', 'Adjective'), ('ì´ìœ ', 'Noun'), ('ëŠ”', 'Josa'), ('?', 'Punctuation'), ('ê°œê°•', 'Noun'), ('í•´ì„œ', 'Verb'), ('!', 'Punctuation')]\n",
      "OKT ëª…ì‚¬ ì¶”ì¶œ : ['ëŒ€í•™ìƒ', 'ì´ìœ ', 'ê°œê°•']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('OKT í˜•íƒœì†Œ ë¶„ì„ :',okt.morphs(text))\n",
    "print('OKT í’ˆì‚¬ íƒœê¹… :',okt.pos(text))\n",
    "print('OKT ëª…ì‚¬ ì¶”ì¶œ :',okt.nouns(text)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ê¼¬ê¼¬ë§ˆ(kma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbuqewYRTXYT",
    "outputId": "48607d49-8368-4316-d752-e23fa2a2501d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¼¬ê¼¬ë§ˆ í˜•íƒœì†Œ ë¶„ì„ : ['3', 'ì›”', 'ì—', 'ëŒ€í•™ìƒ', 'ë“¤', 'ì´', 'ê°•í•˜', 'ã„´', 'ì´ìœ ', 'ëŠ”', '?', 'ê°œê°•', 'í•˜', 'ì–´ì„œ', '!']\n",
      "ê¼¬ê¼¬ë§ˆ í’ˆì‚¬ íƒœê¹… : [('3', 'NR'), ('ì›”', 'NNM'), ('ì—', 'JKM'), ('ëŒ€í•™ìƒ', 'NNG'), ('ë“¤', 'XSN'), ('ì´', 'JKS'), ('ê°•í•˜', 'VV'), ('ã„´', 'ETD'), ('ì´ìœ ', 'NNG'), ('ëŠ”', 'JX'), ('?', 'SF'), ('ê°œê°•', 'NNG'), ('í•˜', 'XSV'), ('ì–´ì„œ', 'ECD'), ('!', 'SF')]\n",
      "ê¼¬ê¼¬ë§ˆ ëª…ì‚¬ ì¶”ì¶œ : ['3', '3ì›”', 'ì›”', 'ëŒ€í•™ìƒ', 'ì´ìœ ', 'ê°œê°•']\n"
     ]
    }
   ],
   "source": [
    "print('ê¼¬ê¼¬ë§ˆ í˜•íƒœì†Œ ë¶„ì„ :',kkma.morphs(text))\n",
    "print('ê¼¬ê¼¬ë§ˆ í’ˆì‚¬ íƒœê¹… :',kkma.pos(text))\n",
    "print('ê¼¬ê¼¬ë§ˆ ëª…ì‚¬ ì¶”ì¶œ :',kkma.nouns(text))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 ë¶ˆìš©ì–´(Stopwords) ì œê±°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK `stopwords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\taekyung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\taekyung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words_list = stopwords.words('english')\n",
    "print(len(stop_words_list))\n",
    "print(stop_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'lying', 'in', 'ponds', 'distributing', 'swords', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'masses', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n",
      "['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'lying', 'ponds', 'distributing', 'swords', 'basis', 'system', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'mandate', 'masses', ',', 'farcical', 'aquatic', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "text =\"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "word_tokens = word_tokenize(text)\n",
    "result = []\n",
    "for word in word_tokens: \n",
    "    if word not in stop_words_list: \n",
    "        result.append(word) \n",
    "print(word_tokens)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoNLPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¶ˆìš©ì–´ ì œê±° ì „ : ['ê³ ê¸°', 'ë¥¼', 'ì•„ë¬´ë ‡ê²Œë‚˜', 'êµ¬', 'ìš°ë ¤', 'ê³ ', 'í•˜ë©´', 'ì•ˆ', 'ë¼', '.', 'ê³ ê¸°', 'ë¼ê³ ', 'ë‹¤', 'ê°™ì€', 'ê²Œ', 'ì•„ë‹ˆê±°ë“ ', '.', '\\n', 'ì˜ˆì»¨ëŒ€', 'ì‚¼ê²¹ì‚´', 'ì„', 'êµ¬ìš¸', 'ë•Œ', 'ëŠ”', 'ì¤‘ìš”í•œ', 'ê²Œ', 'ìˆì§€', '.']\n",
      "ë¶ˆìš©ì–´ ì œê±° í›„ : ['ê³ ê¸°', 'êµ¬', 'ìš°ë ¤', 'í•˜ë©´', '.', 'ê³ ê¸°', 'ë¼ê³ ', 'ë‹¤', 'ì•„ë‹ˆê±°ë“ ', '.', 'ì˜ˆì»¨ëŒ€', 'ì‚¼ê²¹ì‚´', 'ì„', 'ì¤‘ìš”í•œ', 'ìˆì§€', '.']\n"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "text = \"\"\"ê³ ê¸°ë¥¼ ì•„ë¬´ë ‡ê²Œë‚˜ êµ¬ìš°ë ¤ê³  í•˜ë©´ ì•ˆ ë¼. ê³ ê¸°ë¼ê³  ë‹¤ ê°™ì€ ê²Œ ì•„ë‹ˆê±°ë“ .\n",
    "ì˜ˆì»¨ëŒ€ ì‚¼ê²¹ì‚´ì„ êµ¬ìš¸ ë•ŒëŠ” ì¤‘ìš”í•œ ê²Œ ìˆì§€.\"\"\"\n",
    "stop_words = \"\\n ë¥¼ ì•„ë¬´ë ‡ê²Œë‚˜ ê³  ì•ˆ ë¼ ê°™ì€ ê²Œ êµ¬ìš¸ ë•Œ ëŠ”\"\n",
    "stop_words_list = set(stop_words.split(' '))\n",
    "word_tokens = okt.morphs(text)\n",
    "\n",
    "result = []\n",
    "for word in word_tokens: \n",
    "    if word not in stop_words_list: \n",
    "        result.append(word) \n",
    "\n",
    "print('ë¶ˆìš©ì–´ ì œê±° ì „ :',word_tokens) \n",
    "print('ë¶ˆìš©ì–´ ì œê±° í›„ :',result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `many-stop-words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: many-stop-words in c:\\users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install many-stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì§€ì› ì–¸ì–´ :  22\n",
      "['ar', 'ca', 'cs', 'de', 'el', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja', 'kr', 'nl', 'no', 'pl', 'pt', 'ru', 'sk', 'sv', 'tr', 'zh']\n"
     ]
    }
   ],
   "source": [
    "from many_stop_words import get_stop_words, available_languages\n",
    "import many_stop_words\n",
    "print(\"ì§€ì› ì–¸ì–´ : \", len(many_stop_words.available_languages))\n",
    "print(many_stop_words.available_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595\n",
      "{'ë¹„êµì ', 'ì´ë•Œ', 'ê³¼', 'ë§í• ê²ƒë„ ì—†ê³ ', 'ë¶€ë¥˜ì˜ ì‚¬ëŒë“¤', 'ë¡œ ì¸í•˜ì—¬', 'ê·¼ê±°í•˜ì—¬', 'ì•„ì´ì¿ ', 'ì£¼ë£©ì£¼ë£©', 'ì•„ë˜ìœ—', 'ì´ì ìœ¼ë¡œ ë³´ë©´', 'ì •ë„ì— ì´ë¥´ë‹¤', 'ë¥¼', 'í•œ ì´ìœ ëŠ”', 'ì—°ê´€ë˜ë‹¤', 'ë¹„ë¡', 'ìê¸°', 'ê³¼ì—°', 'ì–´ê¸°ì—¬ì°¨', 'í•´ìš”', 'ì—¬ê¸°', 'í•˜ê³¤í•˜ì˜€ë‹¤', 'ë§ˆì €ë„', 'ê´€ê³„ì—†ì´', 'ì´ ë˜ë‹¤', 'í•˜ê¸°ë§Œ í•˜ë©´', 'ëŒ€í•´ì„œ', 'ë¶•ë¶•', 'í• ì§€ì–¸ì •', 'ì´ ì •ë„ì˜', 'ì´ì²œì¹ ', 'ì¦‰', 'ì—¬ëŸ', 'í•˜ì§€ë§ˆ', 'ì—¬ëŸ¬ë¶„', 'ì €ê¸°', 'ì¼ë°˜ì ìœ¼ë¡œ', 'ì–¼ë§ˆë‚˜', 'í• ë§ì •', 'ì— ìˆë‹¤', 'ì´', 'ê³ ë ¤í•˜ë©´', 'í•˜êµ¬ë‚˜', 'ì ì‹œ', 'ë“±ë“±', 'í˜¹ì‹œ', 'ì ê¹', 'ì„¤ë ¹', 'ë¹„ì¶”ì–´ ë³´ì•„', 'ìœ¼ë¡œì¨', 'í•˜ê¸° ë•Œë¬¸ì—', 'ì „ë¶€', 'í—‰í—‰', 'ë•Œê°€ ë˜ì–´', 'ê±°ë°”', 'ê´€í•œ', 'ì˜ì§€í•˜ì—¬', 'í„ë ', 'ë•Œë¬¸ì—', 'ë„ˆí¬', 'í•˜ê¸°ë³´ë‹¤ëŠ”', 'ì¹ ', 'ì¼ê³±', 'ì‹¬ì§€ì–´', 'ê¸°ëŒ€ì—¬', 'í•œì¼ ìœ¼ë¡œëŠ”', 'ì´ë§Œí¼', 'í•˜', 'ë§ˆì €', 'ì•„ì´êµ¬', 'ì…ì¥ì—ì„œ', 'ëª¨ë‘', 'ë‹¹ì¥', 'í• ë•Œ', 'ì–¸ì œ', 'ì–´ì°Œëì–´', 'í–¥í•˜ì—¬', 'ê½ˆë‹¹', 'í•  í˜ì´ ìˆë‹¤', 'ì¼', 'í• ë¿', 'ë§ˆìŒëŒ€ë¡œ', 'ê´€í•˜ì—¬', 'í•˜ì§€ë§Œ', 'ë§ˆì¹˜', 'ë‹¤ì„¯', 'ì—¬ë¶€', 'ì¤„ì€ëª¨ë¥¸ë‹¤', 'í‰¤', 'ëœë°”ì—ì•¼', 'ë‹¤ë¥¸', 'ì´ ì™¸ì—', 'ì‹œì´ˆì—', 'íƒ€ë‹¤', 'í¥', 'ìš°ë¥´ë¥´', 'í•˜ëŠ” í¸ì´ ë‚«ë‹¤', 'ê·¸ë¦¬í•˜ì—¬', 'ì´ìš©í•˜ì—¬', 'ê·¸ì €', 'í† í•˜ë‹¤', 'ì´ì—ˆë‹¤', 'ì„¤ì‚¬', 'ìš”ì»¨ëŒ€', 'ë‹¤ìˆ˜', 'ì´ì™€ê°™ë‹¤ë©´', 'ì™œ', 'íœ˜ìµ', 'ë†€ë¼ë‹¤', 'ë„ì°©í•˜ë‹¤', 'í˜•ì‹ìœ¼ë¡œ ì“°ì—¬', 'í½', 'í•˜ë‚˜', 'í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´', 'ì¤‘ì—ì„œ', 'ìš”ë§Œí¼', 'ë¬¼ë¡ ', 'í•˜ë©´ì„œ', 'ì´ê²ƒ', 'í•˜ë¬¼ë©°', 'ëŒ€ë¡œ í•˜ë‹¤', 'í•˜í•˜', 'í•˜ë”ë¼ë„', 'ì–´ëŠê³³', 'ì™¸ì—ë„', 'ê²¨ìš°', 'ë“±', 'ìš”ë§Œí•œê±¸', 'ë¬´ì—‡ë•Œë¬¸ì—', 'ì—¬ì „íˆ', 'ì—', 'í–¥í•´ì„œ', 'ì•„ì´', 'ëª‡', 'ë‘˜', 'ì— í•œí•˜ë‹¤', 'ë¬´ìŠ¨', 'ê°ì', 'ê¸°íƒ€', 'ë¹„í•˜ë©´', 'í—ˆê±±', 'ì‹œì‘í•˜ì—¬', 'ì €ìª½', 'ì— ê°€ì„œ', 'ë„ˆ', 'ì´ëŸ°', 'í•˜ì§€ ì•Šë„ë¡', 'í•´ë´ìš”', 'í•˜ì§€ë§ˆë¼', 'ë§Œì•½ì—', 'ì–´ì°Œí•˜ì—¬', 'ì‹¤ë¡œ', 'íƒ•íƒ•', 'ë”êµ°ë‹¤ë‚˜', 'ì„ì— í‹€ë¦¼ì—†ë‹¤', 'ì‚ê±±', 'ìª½ìœ¼ë¡œ', 'ì—¬', 'ë¿ì´ë‹¤', 'ìš´ìš´', 'í•˜ìë§ˆì', 'í—ˆ', 'ë¬´ë¦ì“°ê³ ', 'ì¢€', 'ì–¼ë§ˆê°„', 'ë”°ì§€ì§€ ì•Šë‹¤', 'ì´ ë•Œë¬¸ì—', 'í•˜ë“ ì§€', 'ê´€ê³„ê°€ ìˆë‹¤', 'ë•Œ', 'í•˜ëŠ”ê²ƒë„', 'ì–´ì§¸ì„œ', 'ë´ë¼', 'ì¡¸ì¡¸', 'í•œ ê¹Œë‹­ì—', 'ë‘ë²ˆì§¸ë¡œ', 'ì´ë¦¬í•˜ì—¬', 'ì˜í•´ë˜ë‹¤', 'í–ˆì–´ìš”', 'ì´ìª½', 'ê·¸ë ‡ì§€ ì•Šìœ¼ë©´', 'ì €', 'í˜¹ì€', 'ë§‰ë¡ í•˜ê³ ', 'ìœ„í•˜ì—¬', 'ì¦‰ì‹œ', 'ëˆ„ê°€ ì•Œê² ëŠ”ê°€', 'ê°€ë ¹', 'ê²Œë‹¤ê°€', 'ê·¸ë˜ë„', 'ê¹Œì§€ë„', 'ì§€ë§Œ', 'ì˜¤í˜¸', 'ê²°êµ­', 'ì´ì ìœ¼ë¡œ', 'ë¥™', 'ë§Œì€ ì•„ë‹ˆë‹¤', 'ì–´ì´', 'ì•—', 'ë§Œì¼', 'ì–´ëŠê²ƒ', 'ì¿µ', 'ê·¸ëŸ°ë°', 'ê°–ê³ ë§í•˜ìë©´', 'ë‚˜ë¨¸ì§€ëŠ”', 'ê¹Œì•…', 'ì•„', 'í•˜ê²Œë ê²ƒì´ë‹¤', 'ë¬´ì—‡', 'ì•„ë‹ˆë©´', 'ê·¸ëŸ¬ë‹ˆê¹Œ', 'ë³´ëŠ”ë°ì„œ', 'í•˜ë ¤ê³ í•˜ë‹¤', 'ì–¼ë§ˆ ì•ˆ ë˜ëŠ” ê²ƒ', 'ë™ì‹œì—', 'í• ì¤„ì•Œë‹¤', 'ë³¸ëŒ€ë¡œ', 'í•  ì§€ê²½ì´ë‹¤', 'ê·¸ëŸ¬í•œì¦‰', 'ì–´ë– í•œ', 'ê´€ë ¨ì´ ìˆë‹¤', 'ì•½ê°„', 'ê°™ì´', 'ë¹„ê¸¸ìˆ˜ ì—†ë‹¤', 'ì¤„ì€ ëª°ëë‹¤', 'ì˜', 'ì³‡', 'ê´€í•´ì„œëŠ”', 'ë„ˆí¬ë“¤', 'ì˜ˆë¥¼ ë“¤ìë©´', 'ë¹„ê±±ê±°ë¦¬ë‹¤', 'ì˜í•´', 'ì–‘ì', 'í•´ë„ì¢‹ë‹¤', 'í•˜ëŠ” ê¹€ì—', 'ì™€ë¥´ë¥´', 'ì§€ë§ê³ ', 'ì•„ì´ì•¼', 'í•œë‹¤ë©´ ëª°ë¼ë„', 'í•´ì•¼í•œë‹¤', 'ì•„ì•¼', 'ì—¬ì„¯', 'ë°˜ë“œì‹œ', 'í•˜ê² ëŠ”ê°€', 'ê·¸ëŸ°ì¦‰', 'ì˜ì°¨', 'ì´ì  ', 'ê¹Œì§€ ë¯¸ì¹˜ë‹¤', 'ì¡°ì°¨ë„', 'ë¡œ', 'ê²Œìš°ë‹¤', 'ì˜ˆí•˜ë©´', 'ì‡ë”°ë¼', 'ìœ¼ë¡œ ì¸í•˜ì—¬', 'ìˆë‹¤', 'ìš°ì„ ', 'ì˜¤ë¡œì§€', 'ì™€', 'ì²«ë²ˆì§¸ë¡œ', 'ë™ë™', 'ì´ ë°–ì—', 'ë°˜ëŒ€ë¡œ', 'ëŒ€í•´ ë§í•˜ìë©´', 'ì˜ˆì»¨ëŒ€', 'í• ìˆ˜ìˆì–´', 'í•œë§ˆë””', 'í• ìˆ˜ìˆë‹¤', 'ê²¬ì§€ì—ì„œ', 'ê·¸ë ‡ì§€ì•Šìœ¼ë©´', 'ì•Œ ìˆ˜ ìˆë‹¤', 'ë“¤', 'ì–´ë–¤ê²ƒ', 'ë¹„ìŠ·í•˜ë‹¤', 'ì–´ë–»í•´', 'ë¹„ë¡œì†Œ', 'í•´ì„œëŠ” ì•ˆëœë‹¤', 'ì–´ëŠë•Œ', 'ì‚ê±±ê±°ë¦¬ë‹¤', 'ì´ì™€ ê°™ì€', 'ì €í¬', 'ë”ë¶ˆì–´', 'í˜ì…ì–´', 'ì´ê³³', 'ì–´ë–¤', 'ìœ¡', 'ì´ìœ ë§Œìœ¼ë¡œ', 'ë‹¤ë¥¸ ë°©ë©´ìœ¼ë¡œ', 'ì´ìƒ', 'ê·¸ëŸ¼', 'ì´ëŸ¬ì´ëŸ¬í•˜ë‹¤', 'ê²°ê³¼ì— ì´ë¥´ë‹¤', 'ì¡°ì°¨', 'ì œ', 'êµ¬', 'ë¶€í„°', 'ë´', 'ì–´ì¨‹ë“ ', 'ì˜ˆë¥¼ ë“¤ë©´', 'ë§¤', 'ì´ì²œêµ¬', 'ì¤‘ì˜í•˜ë‚˜', 'ê°', 'í˜¼ì', 'ì‚¬', 'ë§Œí¼', 'ë§¤ë²ˆ', 'í‹ˆíƒ€', 'ì–´ëŠìª½', 'ìš°ë¦¬ë“¤', 'ê·¸ë¦¬ê³ ', 'ì˜¤íˆë ¤', 'í•œì ì´ìˆë‹¤', 'ì•¼', 'ê·¸ë˜ì„œ', 'í•˜ë„ë‹¤', 'ë…„', 'ì´ì ìœ¼ë¡œ ë§í•˜ë©´', 'ìì‹ ', 'ê³§', 'í›¨ì”¬', 'ì˜¤ìë§ˆì', 'ë§Œì´ ì•„ë‹ˆë‹¤', 'ë°˜ëŒ€ë¡œ ë§í•˜ìë©´', 'ì–¸ì  ê°€', 'ë’¤ì´ì–´', 'ì´ë ‡ê²Œë§í•˜ìë©´', 'ì–¼ë§ˆë“ ì§€', 'ì¢ì¢', 'ê·¸ëŸ¬ë©´', 'ì´ë ‡ê²Œë˜ë©´', 'ê·¸', 'ì´ë ‡ê²Œ ë§ì€ ê²ƒ', 'ê·¸ë•Œ', 'ì´ì™¸ì—ë„', 'ì‹œê°„', 'ê·¸ë§Œì´ë‹¤', 'ì´ì²œíŒ”', 'ì–´ë–¤ê²ƒë“¤', 'í†µí•˜ì—¬', 'ì•„ì´ê³ ', 'ì•ˆ ê·¸ëŸ¬ë©´', 'ì†¨', 'ì‘ë‹¹', 'ê°œì˜ì¹˜ì•Šê³ ', 'ë¶ˆë¬¸í•˜ê³ ', 'ë‹¹ì‹ ', 'ì•„ë‹ˆë‚˜ë‹¤ë¥¼ê°€', 'ê·¸ë ‡ê²Œ í•¨ìœ¼ë¡œì¨', 'ë°”ì™€ê°™ì´', 'íŒ”', 'ì „ì', 'í•œë‹¤ë©´', 'ì™€ ê°™ì€ ì‚¬ëŒë“¤', 'ì„', 'í•˜ê²Œí•˜ë‹¤', 'ëˆ„êµ¬', 'ê°™ë‹¤', 'ì œì™¸í•˜ê³ ', 'êµ¬í† í•˜ë‹¤', 'í•  ì¤„ ì•ˆë‹¤', 'ìœ™ìœ™', 'ê·¸ëŸ¬ë‚˜', 'ì— ëŒ€í•´', 'ë³´ë‹¤ë”', 'ë¼ìµ', 'ë¡œì¨', 'ì´ë˜', 'ì°¸ë‚˜', 'ì˜¤ë¥´ë‹¤', 'ìŠµë‹ˆê¹Œ', 'ê·¸ë“¤', 'ìŠµë‹ˆë‹¤', 'ë‹¤ìŒìœ¼ë¡œ', 'ë˜', 'ë‹¤ìŒ', 'ì•„ë¬´ê±°ë‚˜', 'ë§í•˜ìë©´', 'ë°”ê¾¸ì–´ë§í•˜ìë©´', 'ì•ì˜ê²ƒ', 'ëŒ€í•˜ë©´', 'ê·¸ìœ„ì—', 'í—ˆí—ˆ', 'ì–´ëŠí•´', 'ê³ ë¡œ', 'ì¦ˆìŒí•˜ì—¬', 'ì—ì„œ', 'í•˜ëŠ”ê²ƒì´ ë‚«ë‹¤', 'ê·¸ì¤‘ì—ì„œ', 'ë„·', 'ì–´ë””', 'ë”©ë™', 'ì–´ëŠ ë…„ë„', 'ë°', 'ì—‰ì—‰', 'ë‹¤ì†Œ', 'ê·¸ëŸ¬ë¯€ë¡œ', 'í•  ìƒê°ì´ë‹¤', 'ë’¤ë”°ë¼', 'ìƒëŒ€ì ìœ¼ë¡œ ë§í•˜ìë©´', 'ì›”', 'ì•Œì•˜ì–´', 'ë ¹', 'ì™œëƒí•˜ë©´', 'ê·¸ëŸ° ê¹Œë‹­ì—', 'ìœ„ì—ì„œ ì„œìˆ í•œë°”ì™€ê°™ì´', 'ì½¸ì½¸', 'ë‚˜', 'ì´ì§€ë§Œ', 'ì•„í™‰', 'í• ì§€ë¼ë„', 'í• ë§Œí•˜ë‹¤', 'ìë§ˆì', 'íƒ€ì¸', 'ì´ì–´ì„œ', 'ì¢‹ì•„', 'í—‰', 'ì¸ ë“¯í•˜ë‹¤', 'ê²ƒê³¼ ê°™ì´', 'ë‹µë‹¤', 'ì…‹', 'ê¸°ì ìœ¼ë¡œ', 'í—ë–¡í—ë–¡', 'ì˜', 'ë‚¨ë“¤', 'ë™ì•ˆ', 'í•˜ê¸° ìœ„í•˜ì—¬', 'í•˜ê³ ìˆì—ˆë‹¤', 'ì•„ë¬´ë„', 'ì–´ë–»ê²Œ', 'ì ì—ì„œ ë³´ì•„', 'ë©”ì“°ê²ë‹¤', 'í•˜ì—¬ê¸ˆ', 'ê·¸ëŸ¬ë‹ˆ', 'ë¶ˆêµ¬í•˜ê³ ', 'ì¡°ê¸ˆ', 'ë³´ë“œë“', 'ê·¸ë ‡ì§€', 'ì—¬ë³´ì‹œì˜¤', 'ë°–ì— ì•ˆëœë‹¤', 'ì´ë ‡êµ¬ë‚˜', 'ìš”ë§Œí•œ ê²ƒ', 'ì¼ë‹¨', 'ê±°ì˜', 'ê·¸ì— ë”°ë¥´ëŠ”', 'ì•„ë‹ˆì—ˆë‹¤ë©´', 'ê·¸ë˜', 'í•˜ë§ˆí„°ë©´', 'ì‰¿', 'í•œí•­ëª©', 'ë‘¥ë‘¥', 'ê²°ë¡ ì„ ë‚¼ ìˆ˜ ìˆë‹¤', 'ë”°ë¼', 'ìƒê°í•œëŒ€ë¡œ', 'í•˜ëŠ”ê²ƒë§Œ ëª»í•˜ë‹¤', 'ì‹œí‚¤ë‹¤', 'ë”ìš±ì´ëŠ”', 'ëœì´ìƒ', 'ê¸°ì¤€ìœ¼ë¡œ', 'ì´ë¡œ ì¸í•˜ì—¬', 'ëšëš', 'ê¹Œì§€', 'ë‚¨ì§“', 'ìš°ì— ì¢…í•©í•œê²ƒê³¼ê°™ì´', 'ì œê°ê¸°', 'ë˜í•œ', 'ì˜í•´ì„œ', 'ë„ë‹¬í•˜ë‹¤', 'ì´ì™€ ë°˜ëŒ€ë¡œ', 'ê°ê°', 'íŒ', 'ìš°ë¦¬', 'ì €ê²ƒë§Œí¼', 'í•˜ëŠ”ë°”', 'ì…ê°í•˜ì—¬', 'ì£¼ì €í•˜ì§€ ì•Šê³ ', 'ë§Œ ëª»í•˜ë‹¤', 'ì—ê²Œ', 'ì°¸', 'í•˜ê¸°ëŠ”í•œë°', 'ì¼ì§€ë¼ë„', 'ë§Œì•½', 'ì‚¼', 'íˆ­', 'í•˜ê¸°ì—', 'ë‹¤ìŒì—', 'ë…¼í•˜ì§€ ì•Šë‹¤', 'ë¼ í•´ë„', 'í•˜ë„ë¡ì‹œí‚¤ë‹¤', 'ë°”ê¾¸ì–´ë§í•˜ë©´', 'ì–¼ë§ˆ', 'ì–´ì©”ìˆ˜ ì—†ë‹¤', 'ë”êµ¬ë‚˜', 'ê°€ê¹ŒìŠ¤ë¡œ', 'ë¿ë§Œì•„ë‹ˆë¼', 'ê°€', 'ê·¸ë ‡ì§€ ì•Šë‹¤ë©´', 'êµ¬ì²´ì ìœ¼ë¡œ', 'ê²ƒ', 'ë¿ë§Œ ì•„ë‹ˆë¼', 'ê°ì¢…', 'ì', 'í•˜ë©´ëœë‹¤', 'ì–¼ë§ˆë§Œí¼', 'ì¼ê²ƒì´ë‹¤', 'ë‹¤ë§Œ', 'ë„¤', 'í•œë°', 'ê¹Œë‹­ìœ¼ë¡œ', 'ë¬´ë µ', 'ë¡œë¶€í„°', 'ê±°ë‹ˆì™€', 'ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ ', 'ìœ„í•´ì„œ', 'ì†Œì¸', 'ì´ë¼ë©´', 'ë”±', 'ì•„í•˜', 'ì•„ë‹ˆë¼ë©´', 'ìœ¼ë¡œì„œ', 'ëŒ€í•˜ì—¬', 'ì €ê²ƒ', 'í•˜ê³  ìˆë‹¤', 'íœ´', 'ë”ë¼ë„', 'ì´ë¥´ê¸°ê¹Œì§€', 'ì´ëŸ´ì •ë„ë¡œ', 'ì™€ì•„', 'í•˜ì—¬ì•¼', 'ì–´ëŠ', 'ë‹¤ì‹œ ë§í•˜ìë©´', 'ì–´', 'í•˜ëŠë‹ˆ', 'ë”°ìœ„', 'ì´ë²ˆ', 'ì´ë´', 'ì•ì—ì„œ', 'ê·¸ì¹˜ì§€ ì•Šë‹¤', 'ì¼ë•Œ', 'ëª¨', 'ë°”ê¾¸ì–´ì„œ ë§í•˜ë©´', 'ëŒ•ê·¸', 'ë”°ë¼ì„œ', 'ì°¨ë¼ë¦¬', 'ì•„ìš¸ëŸ¬', 'ì•Šê¸° ìœ„í•˜ì—¬', 'ë°”ê¿” ë§í•˜ë©´', 'ì•„ë‹ˆ', 'ê²ƒë“¤', 'í•˜ë©´ í• ìˆ˜ë¡', 'ì•Šê¸° ìœ„í•´ì„œ', 'ì— ë‹¬ë ¤ ìˆë‹¤', 'ì „í›„', 'ìœ¼ë¡œ', 'ì„¤ë§ˆ', 'ê·¼ê±°ë¡œ', 'ì–´ë•Œ', 'ì‹œê°', 'ì§€ë“ ì§€', 'ì˜†ì‚¬ëŒ', 'ë°”ë¡œ', 'ì´ëŸ¬í•œ', 'ë‹¨ì§€', 'ë²„ê¸ˆ', 'ê²¸ì‚¬ê²¸ì‚¬', 'ì´ì™€ ê°™ë‹¤', 'ì˜¤ì§', 'ì—°ì´ì„œ', 'ì¾…ì¾…', 'ì‘', 'ì˜ê±°í•˜ì—¬', 'ë°”ê¾¸ì–´ì„œ í•œë‹¤ë©´', 'ì–´ì°Œí•˜ë“ ì§€', 'í•œ í›„', 'ë‹¤ì‹œë§í•˜ë©´', 'ì–¼ë§ˆí¼', 'ì˜¤', 'ì—¬ì°¨', 'íí', 'í–¥í•˜ë‹¤', 'í•¨ê»˜', 'ì¸ì  ', 'ì†Œìƒ', 'í•  ë”°ë¦„ì´ë‹¤', 'ê³µë™ìœ¼ë¡œ', 'ì˜ˆ', 'ë”ìš±ë”', 'ê·¸ë ‡ì§€ë§Œ', 'í•˜ë„ë¡í•˜ë‹¤', 'ë“ ê°„ì—', 'ì–´ì°Œ', 'ìê¸°ì§‘', 'ì´ì²œìœ¡', 'ì–´ì°Œëë“ ', 'ì§„ì§œë¡œ', 'í•´ë„ëœë‹¤'}\n"
     ]
    }
   ],
   "source": [
    "print(len(many_stop_words.get_stop_words('kr')))\n",
    "print(many_stop_words.get_stop_words('kr'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Normalizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizationì€ ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…ì„ í¬í•¨í•œë‹¤:\n",
    "- í…ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ lowercaseë¡œ ë°”ê¾¸ëŠ” ì¼\n",
    "- stemmingì„ í†µí•´ ì ‘ì‚¬ë“¤ì„ ë–¼ì–´ë²„ë¦¬ê±°ë‚˜,\n",
    "- lemmatizationì„ í†µí•´ ë‹¨ì–´ë¥¼ ì‚¬ì „ í‘œì œì–´ë¡œ ë‚˜íƒ€ë‚˜ëŠ” í˜•íƒœë¡œ ë°”ê¾¸ëŠ” ì‘ì—…ë“¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'lying', 'in', 'ponds', 'distributing', 'swords', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'masses', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "text =\"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming (ì–´ê°„ ì¶”ì¶œ)\n",
    "-  REì„ ì‚¬ìš©í•˜ì—¬ ìˆ˜ì‘ì—…ìœ¼ë¡œ í•  ìˆ˜ë„ ìˆìœ¼ë‚˜, NLTK ë‚´ì¥ stemmerëŠ” ë¶ˆê·œì¹™ì ì¸ ê²½ìš°ë“¤ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë” íš¨ìœ¨ì ì´ë‹¤.\n",
    "- NLTK ë‚´ì¥ stemmer:\n",
    "  - Porter stemmer\n",
    "  - Lancaster stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "print([porter.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "lancaster = nltk.LancasterStemmer()\n",
    "print([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stemmingì€ ì˜ ì •ì˜ëœ ê³¼ì •ì´ ì•„ë‹ˆë©° ëª©ì ì— ë§ëŠ” ê²ƒì„ ì„ íƒí•´ ì‚¬ìš©í•œë‹¤\n",
    "- í…ìŠ¤íŠ¸ ì¸í…ì‹± ë“± ì¼ë°˜ì ì¸ NLP ì‘ì—…ì— ìˆì–´ Porter stemmer ì„±ëŠ¥ì´ Lancaster stemmer ë³´ë‹¤ ì¢‹ìŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmerë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì¸ë±ì‹±\n",
    "- `concordance` viewëŠ” ì£¼ì–´ì§„ ë‹¨ì–´ê°€ í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ì¶œí˜„í•˜ëŠ” ê²ƒì„ ì£¼ë³€ ë‹¨ì–´ë“¤(context)ê³¼ í•¨ê»˜ ë³´ì—¬ì¤€ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedText(object):\n",
    "\n",
    "    def __init__(self, stemmer, text):\n",
    "        self._text = text\n",
    "        self._stemmer = stemmer\n",
    "        self._index = nltk.Index((self._stem(word), i) # nltk.Index source code: https://tedboy.github.io/nlps/_modules/nltk/util.html#Index\n",
    "                                 for (i, word) in enumerate(text))  \n",
    "    def concordance(self, word, width=40):\n",
    "        key = self._stem(word)\n",
    "        wc = int(width/4)                # words of context, ì™œ 4ë¡œ ë‚˜ëˆ´ì„ê¹Œ?\n",
    "        for i in self._index[key]:\n",
    "            lcontext = ' '.join(self._text[i-wc:i])\n",
    "            rcontext = ' '.join(self._text[i:i+wc])\n",
    "            ldisplay = '{:>{width}}'.format(lcontext[-width:], width=width)  # ':>{width}' width ê¸¸ì´ ë§Œí¼ í‘œì‹œí•˜ë˜ ì˜¤ë¥¸ìª½ ë§ì¶¤í•œë‹¤\n",
    "            rdisplay = '{:{width}}'.format(rcontext[:width], width=width)\n",
    "            print(ldisplay, rdisplay)\n",
    "\n",
    "    def _stem(self, word):\n",
    "        return self._stemmer.stem(word).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r king ! DENNIS : Listen , strange women lying in ponds distributing swords is no\n",
      " beat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of\n",
      "       Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded !   \n",
      "doctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well  \n",
      "ere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which \n",
      "   you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --\n",
      "h it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k\n",
      "not stop our fight ' til each one of you lies dead , and the Holy Grail returns t\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "grail = nltk.corpus.webtext.words('grail.txt')\n",
    "text = IndexedText(porter, grail)\n",
    "text.concordance('lie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization (í‘œì œì–´ ì¶”ì¶œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\taekyung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\taekyung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"\"\"Reading stories is a great way to improve your vocabulary\n",
    "and we have lots of great stories for you to watch.\"\"\"\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reading', 'stories', 'is', 'a', 'great', 'way', 'to', 'improve', 'your', 'vocabulary', 'and', 'we', 'have', 'lots', 'of', 'great', 'stories', 'for', 'you', 'to', 'watch', '.']\n",
      "['Reading', 'story', 'is', 'a', 'great', 'way', 'to', 'improve', 'your', 'vocabulary', 'and', 'we', 'have', 'lot', 'of', 'great', 'story', 'for', 'you', 'to', 'watch', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "print(tokens)\n",
    "print([lemmatizer.lemmatize(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print(words)\n",
    "print([lemmatizer.lemmatize(t) for t in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- WordNet lemmatizerëŠ” ì ‘ì‚¬ë¥¼ ì˜ë¼ë‚¸ ë‹¨ì–´ê°€ ì‚¬ì „ì— ìˆì„ ë•Œë§Œ ì ‘ì‚¬ë¥¼ ìë¥¸ë‹¤\n",
    "- ì¶”ê°€ ì²˜ë¦¬ ë•Œë¬¸ì— stemmer ë³´ë‹¤ ëŠë¦¬ë‹¤\n",
    "- 'lying'ì„ ì²˜ë¦¬í•˜ì§€ ì•Šì•˜ì§€ë§Œ 'women'ì€ 'woman'ìœ¼ë¡œ ë°”ê¿¨ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 ì •ê·œí‘œí˜„ì‹ (Regular Expressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- regular expressionì„ í‘œì‹œí•  ë•Œ Â«ed$Â» ì²˜ëŸ¼ Â«ì •ê·œì‹Â» ê¸°í˜¸ë¥¼ ì‚¬ìš©í•  ê²ƒì„.\n",
    "- Pythonì—ì„œì˜ regular expressionì€ `re` libraryë¥¼ ì‚¬ìš©í•œë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) ê¸°ë³¸ Meta-characters ì‚¬ìš©í•˜ê¸°\n",
    "- 'ed'ë¡œ ëë‚˜ëŠ” ë‹¨ì–´ ì°¾ê¸°\n",
    "- `$`: end of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abaissed',\n",
       " 'abandoned',\n",
       " 'abased',\n",
       " 'abashed',\n",
       " 'abatised',\n",
       " 'abed',\n",
       " 'aborted',\n",
       " 'abridged',\n",
       " 'abscessed',\n",
       " 'absconded',\n",
       " 'absorbed',\n",
       " 'abstracted',\n",
       " 'abstricted',\n",
       " 'accelerated',\n",
       " 'accepted',\n",
       " 'accidented',\n",
       " 'accoladed',\n",
       " 'accolated',\n",
       " 'accomplished',\n",
       " 'accosted',\n",
       " 'accredited',\n",
       " 'accursed',\n",
       " 'accused',\n",
       " 'accustomed',\n",
       " 'acetated',\n",
       " 'acheweed',\n",
       " 'aciculated',\n",
       " 'aciliated',\n",
       " 'acknowledged',\n",
       " 'acorned',\n",
       " 'acquainted',\n",
       " 'acquired',\n",
       " 'acquisited',\n",
       " 'acred',\n",
       " 'aculeated',\n",
       " 'addebted',\n",
       " 'added',\n",
       " 'addicted',\n",
       " 'addlebrained',\n",
       " 'addleheaded',\n",
       " 'addlepated',\n",
       " 'addorsed',\n",
       " 'adempted',\n",
       " 'adfected',\n",
       " 'adjoined',\n",
       " 'admired',\n",
       " 'admitted',\n",
       " 'adnexed',\n",
       " 'adopted',\n",
       " 'adossed',\n",
       " 'adreamed',\n",
       " 'adscripted',\n",
       " 'aduncated',\n",
       " 'advanced',\n",
       " 'advised',\n",
       " 'aeried',\n",
       " 'aethered',\n",
       " 'afeared',\n",
       " 'affected',\n",
       " 'affectioned',\n",
       " 'affined',\n",
       " 'afflicted',\n",
       " 'affricated',\n",
       " 'affrighted',\n",
       " 'affronted',\n",
       " 'aforenamed',\n",
       " 'afterfeed',\n",
       " 'aftershafted',\n",
       " 'afterthoughted',\n",
       " 'afterwitted',\n",
       " 'agazed',\n",
       " 'aged',\n",
       " 'agglomerated',\n",
       " 'aggrieved',\n",
       " 'agminated',\n",
       " 'agnamed',\n",
       " 'agonied',\n",
       " 'agreed',\n",
       " 'agueweed',\n",
       " 'ahungered',\n",
       " 'aiguilletted',\n",
       " 'ailweed',\n",
       " 'airbrained',\n",
       " 'airified',\n",
       " 'aiseweed',\n",
       " 'aisled',\n",
       " 'alarmed',\n",
       " 'alated',\n",
       " 'alimonied',\n",
       " 'aliped',\n",
       " 'alleyed',\n",
       " 'allied',\n",
       " 'alligatored',\n",
       " 'allseed',\n",
       " 'almsdeed',\n",
       " 'aloed',\n",
       " 'altared',\n",
       " 'alveolated',\n",
       " 'amazed',\n",
       " 'ameed',\n",
       " 'amiced',\n",
       " 'amphitheatered',\n",
       " 'ampullated',\n",
       " 'amused',\n",
       " 'anchored',\n",
       " 'angled',\n",
       " 'anguiped',\n",
       " 'anguished',\n",
       " 'angulated',\n",
       " 'angulinerved',\n",
       " 'anhungered',\n",
       " 'animated',\n",
       " 'aniseed',\n",
       " 'annodated',\n",
       " 'annulated',\n",
       " 'anomaliped',\n",
       " 'anserated',\n",
       " 'anteflected',\n",
       " 'anteflexed',\n",
       " 'antimoniated',\n",
       " 'antimoniureted',\n",
       " 'antimoniuretted',\n",
       " 'antiquated',\n",
       " 'antired',\n",
       " 'antiweed',\n",
       " 'antlered',\n",
       " 'apertured',\n",
       " 'apexed',\n",
       " 'apicifixed',\n",
       " 'apiculated',\n",
       " 'apocopated',\n",
       " 'apostrophied',\n",
       " 'appearanced',\n",
       " 'appellatived',\n",
       " 'appendaged',\n",
       " 'appendiculated',\n",
       " 'applied',\n",
       " 'appressed',\n",
       " 'aralkylated',\n",
       " 'arbored',\n",
       " 'arched',\n",
       " 'architraved',\n",
       " 'arcked',\n",
       " 'arcuated',\n",
       " 'ared',\n",
       " 'areolated',\n",
       " 'ariled',\n",
       " 'arillated',\n",
       " 'armchaired',\n",
       " 'armed',\n",
       " 'armied',\n",
       " 'armillated',\n",
       " 'armored',\n",
       " 'armoried',\n",
       " 'arpeggiated',\n",
       " 'arpeggioed',\n",
       " 'arrased',\n",
       " 'arrowed',\n",
       " 'arrowheaded',\n",
       " 'arrowweed',\n",
       " 'arseneted',\n",
       " 'arsenetted',\n",
       " 'arseniureted',\n",
       " 'articled',\n",
       " 'articulated',\n",
       " 'ashamed',\n",
       " 'ashlared',\n",
       " 'ashweed',\n",
       " 'aspersed',\n",
       " 'asphyxied',\n",
       " 'assented',\n",
       " 'assessed',\n",
       " 'assigned',\n",
       " 'assistanted',\n",
       " 'associated',\n",
       " 'assonanced',\n",
       " 'assorted',\n",
       " 'assumed',\n",
       " 'assured',\n",
       " 'asteriated',\n",
       " 'astonied',\n",
       " 'aswooned',\n",
       " 'atrophiated',\n",
       " 'atrophied',\n",
       " 'attached',\n",
       " 'attired',\n",
       " 'attrited',\n",
       " 'augmented',\n",
       " 'aurated',\n",
       " 'auricled',\n",
       " 'auriculated',\n",
       " 'authorized',\n",
       " 'autoinhibited',\n",
       " 'autosensitized',\n",
       " 'autosled',\n",
       " 'averted',\n",
       " 'avowed',\n",
       " 'awearied',\n",
       " 'awned',\n",
       " 'awninged',\n",
       " 'axed',\n",
       " 'axhammered',\n",
       " 'axised',\n",
       " 'axled',\n",
       " 'axseed',\n",
       " 'axweed',\n",
       " 'azoted',\n",
       " 'azured',\n",
       " 'babied',\n",
       " 'babished',\n",
       " 'babyfied',\n",
       " 'baccated',\n",
       " 'backboned',\n",
       " 'backed',\n",
       " 'backhanded',\n",
       " 'backwatered',\n",
       " 'baconweed',\n",
       " 'badgerweed',\n",
       " 'bagged',\n",
       " 'bagwigged',\n",
       " 'baked',\n",
       " 'balanced',\n",
       " 'balconied',\n",
       " 'baldachined',\n",
       " 'baldricked',\n",
       " 'balled',\n",
       " 'ballweed',\n",
       " 'balsamweed',\n",
       " 'balustered',\n",
       " 'balustraded',\n",
       " 'bandannaed',\n",
       " 'banded',\n",
       " 'bandoleered',\n",
       " 'bangled',\n",
       " 'banked',\n",
       " 'bankweed',\n",
       " 'bannered',\n",
       " 'barbated',\n",
       " 'barbed',\n",
       " 'barebacked',\n",
       " 'bareboned',\n",
       " 'barefaced',\n",
       " 'barefooted',\n",
       " 'barehanded',\n",
       " 'bareheaded',\n",
       " 'barelegged',\n",
       " 'barenecked',\n",
       " 'barmybrained',\n",
       " 'barred',\n",
       " 'barreled',\n",
       " 'bartizaned',\n",
       " 'basebred',\n",
       " 'based',\n",
       " 'basehearted',\n",
       " 'basifixed',\n",
       " 'basilweed',\n",
       " 'basined',\n",
       " 'basinerved',\n",
       " 'basqued',\n",
       " 'bastioned',\n",
       " 'bated',\n",
       " 'bathroomed',\n",
       " 'battered',\n",
       " 'batteried',\n",
       " 'battled',\n",
       " 'battlemented',\n",
       " 'bayed',\n",
       " 'bayoneted',\n",
       " 'beached',\n",
       " 'beaded',\n",
       " 'beaked',\n",
       " 'bealtared',\n",
       " 'beamed',\n",
       " 'beanweed',\n",
       " 'beaproned',\n",
       " 'bearded',\n",
       " 'beautied',\n",
       " 'beavered',\n",
       " 'beballed',\n",
       " 'bebannered',\n",
       " 'bebed',\n",
       " 'bebelted',\n",
       " 'bebled',\n",
       " 'bebothered',\n",
       " 'bebouldered',\n",
       " 'bebuttoned',\n",
       " 'becassocked',\n",
       " 'bechained',\n",
       " 'bechignoned',\n",
       " 'becircled',\n",
       " 'becoiffed',\n",
       " 'becombed',\n",
       " 'becousined',\n",
       " 'becrinolined',\n",
       " 'becuffed',\n",
       " 'becurtained',\n",
       " 'becushioned',\n",
       " 'bed',\n",
       " 'bedaggered',\n",
       " 'bedangled',\n",
       " 'bedded',\n",
       " 'bediademed',\n",
       " 'bediamonded',\n",
       " 'beedged',\n",
       " 'beefheaded',\n",
       " 'beeheaded',\n",
       " 'beeswinged',\n",
       " 'beetled',\n",
       " 'beetleheaded',\n",
       " 'beetleweed',\n",
       " 'beeweed',\n",
       " 'befamilied',\n",
       " 'befanned',\n",
       " 'befathered',\n",
       " 'beferned',\n",
       " 'befetished',\n",
       " 'befezzed',\n",
       " 'befilleted',\n",
       " 'befilmed',\n",
       " 'beforested',\n",
       " 'befountained',\n",
       " 'befrocked',\n",
       " 'befrogged',\n",
       " 'befurbelowed',\n",
       " 'befurred',\n",
       " 'begabled',\n",
       " 'begarlanded',\n",
       " 'begartered',\n",
       " 'beggarweed',\n",
       " 'beglobed',\n",
       " 'begoggled',\n",
       " 'begowned',\n",
       " 'behatted',\n",
       " 'behaviored',\n",
       " 'beheadlined',\n",
       " 'behooped',\n",
       " 'beinked',\n",
       " 'bekilted',\n",
       " 'beknived',\n",
       " 'beknotted',\n",
       " 'belaced',\n",
       " 'belated',\n",
       " 'belatticed',\n",
       " 'belavendered',\n",
       " 'beledgered',\n",
       " 'belfried',\n",
       " 'beliked',\n",
       " 'belimousined',\n",
       " 'belled',\n",
       " 'bellied',\n",
       " 'bellmouthed',\n",
       " 'bellweed',\n",
       " 'beloved',\n",
       " 'belozenged',\n",
       " 'belted',\n",
       " 'bemazed',\n",
       " 'bemedaled',\n",
       " 'bemedalled',\n",
       " 'bemitered',\n",
       " 'bemitred',\n",
       " 'bemused',\n",
       " 'bemuslined',\n",
       " 'bended',\n",
       " 'beneaped',\n",
       " 'beneficed',\n",
       " 'beneighbored',\n",
       " 'benempted',\n",
       " 'benighted',\n",
       " 'bennetweed',\n",
       " 'benumbed',\n",
       " 'benweed',\n",
       " 'benzoated',\n",
       " 'benzoinated',\n",
       " 'bepastured',\n",
       " 'bepatched',\n",
       " 'beperiwigged',\n",
       " 'bepewed',\n",
       " 'bepillared',\n",
       " 'bepistoled',\n",
       " 'beplaided',\n",
       " 'beplumed',\n",
       " 'beribanded',\n",
       " 'beribboned',\n",
       " 'beringed',\n",
       " 'beringleted',\n",
       " 'berobed',\n",
       " 'berouged',\n",
       " 'berried',\n",
       " 'berthed',\n",
       " 'beruffed',\n",
       " 'beruffled',\n",
       " 'beshawled',\n",
       " 'besieged',\n",
       " 'beslushed',\n",
       " 'besotted',\n",
       " 'bespecked',\n",
       " 'bespectacled',\n",
       " 'besped',\n",
       " 'bespeed',\n",
       " 'bespelled',\n",
       " 'bespurred',\n",
       " 'bestatued',\n",
       " 'bestayed',\n",
       " 'bestrapped',\n",
       " 'bestubbled',\n",
       " 'besweatered',\n",
       " 'betattered',\n",
       " 'betaxed',\n",
       " 'betowered',\n",
       " 'betrothed',\n",
       " 'betrousered',\n",
       " 'betted',\n",
       " 'betuckered',\n",
       " 'beturbaned',\n",
       " 'betusked',\n",
       " 'betutored',\n",
       " 'betwattled',\n",
       " 'beuniformed',\n",
       " 'beveled',\n",
       " 'bevelled',\n",
       " 'bevesseled',\n",
       " 'bevesselled',\n",
       " 'bevined',\n",
       " 'bevoiled',\n",
       " 'bewaitered',\n",
       " 'bewhiskered',\n",
       " 'bewigged',\n",
       " 'bewildered',\n",
       " 'bewinged',\n",
       " 'bewired',\n",
       " 'bewrathed',\n",
       " 'biangulated',\n",
       " 'biarcuated',\n",
       " 'biarticulated',\n",
       " 'bicarbureted',\n",
       " 'biciliated',\n",
       " 'bicolored',\n",
       " 'bicorned',\n",
       " 'bidented',\n",
       " 'bifanged',\n",
       " 'bifidated',\n",
       " 'biflected',\n",
       " 'biforked',\n",
       " 'biformed',\n",
       " 'bifronted',\n",
       " 'bifurcated',\n",
       " 'bigeminated',\n",
       " 'bighearted',\n",
       " 'bigmouthed',\n",
       " 'bigoted',\n",
       " 'bigwigged',\n",
       " 'bilamellated',\n",
       " 'bilaminated',\n",
       " 'billed',\n",
       " 'bilobated',\n",
       " 'bilobed',\n",
       " 'bilsted',\n",
       " 'bimaculated',\n",
       " 'bimotored',\n",
       " 'bindweed',\n",
       " 'bineweed',\n",
       " 'binominated',\n",
       " 'binucleated',\n",
       " 'biparted',\n",
       " 'bipectinated',\n",
       " 'biped',\n",
       " 'bipennated',\n",
       " 'bipinnated',\n",
       " 'bipinnatiparted',\n",
       " 'bipinnatisected',\n",
       " 'biradiated',\n",
       " 'birdmouthed',\n",
       " 'birdseed',\n",
       " 'birdweed',\n",
       " 'birostrated',\n",
       " 'birthbed',\n",
       " 'bisexed',\n",
       " 'bishopweed',\n",
       " 'bistered',\n",
       " 'bistipuled',\n",
       " 'bisubstituted',\n",
       " 'bitted',\n",
       " 'bitterhearted',\n",
       " 'bitterweed',\n",
       " 'bituberculated',\n",
       " 'bitumed',\n",
       " 'bivalved',\n",
       " 'bivaulted',\n",
       " 'bivocalized',\n",
       " 'blackhearted',\n",
       " 'blackseed',\n",
       " 'blackshirted',\n",
       " 'bladderseed',\n",
       " 'bladderweed',\n",
       " 'bladed',\n",
       " 'blakeberyed',\n",
       " 'blamed',\n",
       " 'blanked',\n",
       " 'blanketed',\n",
       " 'blanketweed',\n",
       " 'blasted',\n",
       " 'bleached',\n",
       " 'bleared',\n",
       " 'bleed',\n",
       " 'blended',\n",
       " 'blessed',\n",
       " 'blighted',\n",
       " 'blinded',\n",
       " 'blindfolded',\n",
       " 'blindweed',\n",
       " 'blinked',\n",
       " 'blinkered',\n",
       " 'blistered',\n",
       " 'blisterweed',\n",
       " 'blithehearted',\n",
       " 'bloated',\n",
       " 'blobbed',\n",
       " 'blocked',\n",
       " 'blockheaded',\n",
       " 'blooded',\n",
       " 'bloodied',\n",
       " 'bloodshed',\n",
       " 'bloodstained',\n",
       " 'bloodweed',\n",
       " 'blossomed',\n",
       " 'blotched',\n",
       " 'bloused',\n",
       " 'blowzed',\n",
       " 'bludgeoned',\n",
       " 'bluebelled',\n",
       " 'bluehearted',\n",
       " 'blueweed',\n",
       " 'blunderheaded',\n",
       " 'blunthearted',\n",
       " 'blurred',\n",
       " 'bobbed',\n",
       " 'bobsled',\n",
       " 'bobtailed',\n",
       " 'bodiced',\n",
       " 'bodied',\n",
       " 'boiled',\n",
       " 'boldhearted',\n",
       " 'bolectioned',\n",
       " 'boled',\n",
       " 'boleweed',\n",
       " 'bolled',\n",
       " 'bombed',\n",
       " 'bonded',\n",
       " 'boned',\n",
       " 'boneheaded',\n",
       " 'bonneted',\n",
       " 'booked',\n",
       " 'booted',\n",
       " 'bootied',\n",
       " 'boozed',\n",
       " 'bordered',\n",
       " 'bordured',\n",
       " 'bosomed',\n",
       " 'bossed',\n",
       " 'bosselated',\n",
       " 'botched',\n",
       " 'botherheaded',\n",
       " 'bothsided',\n",
       " 'bottled',\n",
       " 'bottomed',\n",
       " 'boughed',\n",
       " 'bounded',\n",
       " 'bountied',\n",
       " 'bowed',\n",
       " 'boweled',\n",
       " 'bowlegged',\n",
       " 'bowstringed',\n",
       " 'braced',\n",
       " 'braceleted',\n",
       " 'brackened',\n",
       " 'bracted',\n",
       " 'braided',\n",
       " 'brambled',\n",
       " 'branched',\n",
       " 'branded',\n",
       " 'brandied',\n",
       " 'brangled',\n",
       " 'bravehearted',\n",
       " 'brawned',\n",
       " 'brazenfaced',\n",
       " 'breasted',\n",
       " 'breastweed',\n",
       " 'breathed',\n",
       " 'brecciated',\n",
       " 'bred',\n",
       " 'breeched',\n",
       " 'breed',\n",
       " 'breviped',\n",
       " 'bridebed',\n",
       " 'brideweed',\n",
       " 'bridged',\n",
       " 'bridled',\n",
       " 'briered',\n",
       " 'brimmed',\n",
       " 'bristled',\n",
       " 'broadhearted',\n",
       " 'brocaded',\n",
       " 'brocked',\n",
       " 'brokenhearted',\n",
       " 'bromoiodized',\n",
       " 'bronzed',\n",
       " 'brooked',\n",
       " 'brookweed',\n",
       " 'broomweed',\n",
       " 'broozled',\n",
       " 'browed',\n",
       " 'brownweed',\n",
       " 'bruckled',\n",
       " 'brushed',\n",
       " 'buboed',\n",
       " 'bucked',\n",
       " 'buckled',\n",
       " 'buckskinned',\n",
       " 'buffed',\n",
       " 'bugled',\n",
       " 'bugleweed',\n",
       " 'bugseed',\n",
       " 'bugweed',\n",
       " 'bulbed',\n",
       " 'bulked',\n",
       " 'bulkheaded',\n",
       " 'bullated',\n",
       " 'bulldogged',\n",
       " 'bulleted',\n",
       " 'bulletheaded',\n",
       " 'bullheaded',\n",
       " 'bullweed',\n",
       " 'bummed',\n",
       " 'bundlerooted',\n",
       " 'bundweed',\n",
       " 'bunted',\n",
       " 'buried',\n",
       " 'burled',\n",
       " 'burned',\n",
       " 'burnoosed',\n",
       " 'burntweed',\n",
       " 'burred',\n",
       " 'burroweed',\n",
       " 'burseed',\n",
       " 'burweed',\n",
       " 'bushed',\n",
       " 'busied',\n",
       " 'busked',\n",
       " 'buskined',\n",
       " 'busted',\n",
       " 'bustled',\n",
       " 'busybodied',\n",
       " 'buttered',\n",
       " 'butterfingered',\n",
       " 'butterweed',\n",
       " 'butteryfingered',\n",
       " 'buttocked',\n",
       " 'buttoned',\n",
       " 'buttonweed',\n",
       " 'cabled',\n",
       " 'caboshed',\n",
       " 'caddiced',\n",
       " 'caddised',\n",
       " 'cadenced',\n",
       " 'cadweed',\n",
       " 'caftaned',\n",
       " 'caged',\n",
       " 'cairned',\n",
       " 'caissoned',\n",
       " 'calced',\n",
       " 'calcified',\n",
       " 'calcined',\n",
       " 'calculated',\n",
       " 'calibered',\n",
       " 'calicoed',\n",
       " 'caligated',\n",
       " 'calpacked',\n",
       " 'calved',\n",
       " 'calycled',\n",
       " 'calyculated',\n",
       " 'camailed',\n",
       " 'camerated',\n",
       " 'cammed',\n",
       " 'campanulated',\n",
       " 'campshed',\n",
       " 'camused',\n",
       " 'canaliculated',\n",
       " 'cancellated',\n",
       " 'cancered',\n",
       " 'cancerweed',\n",
       " 'candied',\n",
       " 'candlelighted',\n",
       " 'candlesticked',\n",
       " 'candyweed',\n",
       " 'canioned',\n",
       " 'cankered',\n",
       " 'cankerweed',\n",
       " 'canned',\n",
       " 'cannelated',\n",
       " 'cannelured',\n",
       " 'cannoned',\n",
       " 'cannulated',\n",
       " 'canted',\n",
       " 'cantilevered',\n",
       " 'cantoned',\n",
       " 'cantred',\n",
       " 'caped',\n",
       " 'capernoited',\n",
       " 'capeweed',\n",
       " 'capitaled',\n",
       " 'capitated',\n",
       " 'capped',\n",
       " 'capriped',\n",
       " 'capsulated',\n",
       " 'capuched',\n",
       " 'carapaced',\n",
       " 'carbolated',\n",
       " 'carboyed',\n",
       " 'carbuncled',\n",
       " 'carcaneted',\n",
       " 'carded',\n",
       " 'carinated',\n",
       " 'carkled',\n",
       " 'carnaged',\n",
       " 'carnationed',\n",
       " 'carpetweed',\n",
       " 'carried',\n",
       " 'carrotweed',\n",
       " 'carucated',\n",
       " 'carunculated',\n",
       " 'cased',\n",
       " 'casemated',\n",
       " 'casemented',\n",
       " 'caseweed',\n",
       " 'casqued',\n",
       " 'castellated',\n",
       " 'castled',\n",
       " 'castorized',\n",
       " 'catamited',\n",
       " 'cataracted',\n",
       " 'catarrhed',\n",
       " 'catchweed',\n",
       " 'catenated',\n",
       " 'caterpillared',\n",
       " 'catfaced',\n",
       " 'catfooted',\n",
       " 'cathedraled',\n",
       " 'caudated',\n",
       " 'caverned',\n",
       " 'cavitied',\n",
       " 'cayenned',\n",
       " 'cedared',\n",
       " 'ceilinged',\n",
       " 'celebrated',\n",
       " 'cellated',\n",
       " 'celled',\n",
       " 'cellulated',\n",
       " 'celluloided',\n",
       " 'centered',\n",
       " 'centriffed',\n",
       " 'centuried',\n",
       " 'cerated',\n",
       " 'cered',\n",
       " 'certified',\n",
       " 'chafeweed',\n",
       " 'chaffseed',\n",
       " 'chaffweed',\n",
       " 'chafted',\n",
       " 'chained',\n",
       " 'chaliced',\n",
       " 'chambered',\n",
       " 'chamberleted',\n",
       " 'chamberletted',\n",
       " 'chanceled',\n",
       " 'channeled',\n",
       " 'channelled',\n",
       " 'chaped',\n",
       " 'chapleted',\n",
       " 'chapournetted',\n",
       " 'chapped',\n",
       " 'charioted',\n",
       " 'charqued',\n",
       " 'chartered',\n",
       " 'chasmed',\n",
       " 'chasteweed',\n",
       " 'chasubled',\n",
       " 'checked',\n",
       " 'checkered',\n",
       " 'checkrowed',\n",
       " 'cheered',\n",
       " 'cheliped',\n",
       " 'cherried',\n",
       " 'chickenbreasted',\n",
       " 'chickenhearted',\n",
       " 'chickenweed',\n",
       " 'chickweed',\n",
       " 'chicqued',\n",
       " 'chiggerweed',\n",
       " 'chignoned',\n",
       " 'childbed',\n",
       " 'childed',\n",
       " 'chilled',\n",
       " 'chined',\n",
       " 'chinned',\n",
       " 'chipped',\n",
       " 'chiseled',\n",
       " 'chitinized',\n",
       " 'chokered',\n",
       " 'chokeweed',\n",
       " 'cholterheaded',\n",
       " 'chopped',\n",
       " 'choppered',\n",
       " 'chorded',\n",
       " 'chowderheaded',\n",
       " 'christened',\n",
       " 'chubbed',\n",
       " 'chuckleheaded',\n",
       " 'churchified',\n",
       " 'churled',\n",
       " 'ciliated',\n",
       " 'cingulated',\n",
       " 'cinnamoned',\n",
       " 'cinquefoiled',\n",
       " 'circled',\n",
       " 'circumscribed',\n",
       " 'circumstanced',\n",
       " 'cirrated',\n",
       " 'cirrhosed',\n",
       " 'cirriped',\n",
       " 'cisted',\n",
       " 'citied',\n",
       " 'citified',\n",
       " 'citrated',\n",
       " 'civilized',\n",
       " 'clammed',\n",
       " 'clammyweed',\n",
       " 'clanned',\n",
       " 'clapped',\n",
       " 'classed',\n",
       " 'classified',\n",
       " 'clavated',\n",
       " 'clavellated',\n",
       " 'clawed',\n",
       " 'claybrained',\n",
       " 'clayweed',\n",
       " 'cleaded',\n",
       " 'cleanhanded',\n",
       " 'cleanhearted',\n",
       " 'clearheaded',\n",
       " 'clearhearted',\n",
       " 'clearweed',\n",
       " 'cled',\n",
       " 'cleeked',\n",
       " 'clefted',\n",
       " 'clerestoried',\n",
       " 'cliented',\n",
       " 'cliffed',\n",
       " 'cliffweed',\n",
       " 'clipped',\n",
       " 'cloaked',\n",
       " 'clocked',\n",
       " 'clodpated',\n",
       " 'cloistered',\n",
       " 'closed',\n",
       " 'closefisted',\n",
       " 'closehanded',\n",
       " 'closehearted',\n",
       " 'closemouthed',\n",
       " 'clotweed',\n",
       " 'clouded',\n",
       " 'clouted',\n",
       " 'clovered',\n",
       " 'clubbed',\n",
       " 'clubfisted',\n",
       " 'clubfooted',\n",
       " 'clubweed',\n",
       " 'clustered',\n",
       " 'coaged',\n",
       " 'coaggregated',\n",
       " 'coated',\n",
       " 'coattailed',\n",
       " 'cobbed',\n",
       " 'cocashweed',\n",
       " 'cochleated',\n",
       " 'cockaded',\n",
       " 'cocked',\n",
       " 'cockeyed',\n",
       " 'cockled',\n",
       " 'cockneybred',\n",
       " 'cockscombed',\n",
       " 'cockweed',\n",
       " 'codheaded',\n",
       " 'coed',\n",
       " 'coelongated',\n",
       " 'coembedded',\n",
       " 'coequated',\n",
       " 'coexpanded',\n",
       " 'coffeeweed',\n",
       " 'cogged',\n",
       " 'coifed',\n",
       " 'coiled',\n",
       " 'coldhearted',\n",
       " 'coleseed',\n",
       " 'colicweed',\n",
       " 'collared',\n",
       " 'collected',\n",
       " 'collied',\n",
       " 'colloped',\n",
       " 'colonnaded',\n",
       " 'colored',\n",
       " 'columnated',\n",
       " 'columned',\n",
       " 'combed',\n",
       " 'combined',\n",
       " 'compacted',\n",
       " 'complected',\n",
       " 'complexioned',\n",
       " 'complicated',\n",
       " 'componed',\n",
       " 'componented',\n",
       " 'composed',\n",
       " 'compressed',\n",
       " 'comprised',\n",
       " 'compulsed',\n",
       " 'conamed',\n",
       " 'concamerated',\n",
       " 'concealed',\n",
       " 'conceded',\n",
       " 'conceited',\n",
       " 'concentrated',\n",
       " 'concerned',\n",
       " 'concerted',\n",
       " 'conched',\n",
       " 'conchyliated',\n",
       " 'condemned',\n",
       " 'condensed',\n",
       " 'conditioned',\n",
       " 'conduplicated',\n",
       " 'coned',\n",
       " 'confated',\n",
       " 'conferted',\n",
       " 'confined',\n",
       " 'confirmed',\n",
       " 'conflated',\n",
       " 'confounded',\n",
       " 'confused',\n",
       " 'congested',\n",
       " 'conjoined',\n",
       " 'conjugated',\n",
       " 'connected',\n",
       " 'conred',\n",
       " 'consecrated',\n",
       " 'considered',\n",
       " 'consolidated',\n",
       " 'constrained',\n",
       " 'constricted',\n",
       " 'consumpted',\n",
       " 'contagioned',\n",
       " 'contented',\n",
       " 'contextured',\n",
       " 'continued',\n",
       " 'contorted',\n",
       " 'contortioned',\n",
       " 'contracted',\n",
       " 'contractured',\n",
       " 'contusioned',\n",
       " 'converted',\n",
       " 'convexed',\n",
       " 'convinced',\n",
       " 'convoluted',\n",
       " 'coolheaded',\n",
       " 'coolweed',\n",
       " 'copied',\n",
       " 'copleased',\n",
       " 'copped',\n",
       " 'coppernosed',\n",
       " 'copperytailed',\n",
       " 'coppiced',\n",
       " 'coppled',\n",
       " 'copsewooded',\n",
       " 'copygraphed',\n",
       " 'coraled',\n",
       " 'corded',\n",
       " 'corduroyed',\n",
       " 'cored',\n",
       " 'coreflexed',\n",
       " 'corked',\n",
       " 'cornered',\n",
       " 'cornified',\n",
       " 'cornuated',\n",
       " 'cornuted',\n",
       " 'corollated',\n",
       " 'coronaled',\n",
       " 'coronated',\n",
       " 'coroneted',\n",
       " 'coronetted',\n",
       " 'corpusculated',\n",
       " 'corrected',\n",
       " 'correlated',\n",
       " 'corridored',\n",
       " ...]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wordlist if re.search('ed$', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 8 ê¸€ì ë‹¨ì–´ ì¤‘ 3ë²ˆì§¸ê°€ j ì´ê³  6ë²ˆì§¸ê°€ tì¸ ë‹¨ì–´ ì°¾ê¸° \n",
    "- `.`: any single character\n",
    "- `^`: start of a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abjectly',\n",
       " 'adjuster',\n",
       " 'dejected',\n",
       " 'dejectly',\n",
       " 'injector',\n",
       " 'majestic',\n",
       " 'objectee',\n",
       " 'objector',\n",
       " 'rejecter',\n",
       " 'rejector',\n",
       " 'unjilted',\n",
       " 'unjolted',\n",
       " 'unjustly']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wordlist if re.search('^..j..t..$', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:crimson\">[ì—°ìŠµğŸ˜‰1]</span> ìœ„ì˜ ì˜ˆì—ì„œ `^` ê¸°í˜¸ë¥¼ ì—†ì•¤ë‹¤ë©´ ê²°ê³¼ëŠ”?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `?`: ì•ì˜ ê¸€ìëŠ” optionì´ë¼ëŠ” ì˜ë¯¸. (ì˜ˆ) Â«^e-?mail$Â» ëŠ” 'email'ê³¼ 'e-mail' ëª¨ë‘ ì¼ì¹˜í•œë‹¤. \n",
    "- ë‹¤ìŒê³¼ ê°™ì€ ì½”ë“œë¡œ ì´ëŸ¬í•œ ë‹¨ì–´ì˜ ì´ ë¹ˆë„ìˆ˜ë¥¼ ì…€ìˆ˜ ìˆë‹¤.\n",
    "```python\n",
    "sum(1 for w in text if re.search('^e-?mail$', w))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë²”ìœ„(range) ë° í´ë¡œì €(closure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/9key_pad.png\" width=\"200\" style=\"margin-left: auto; margin-right: auto\">\n",
    "<p style=\"text-align: center;\">í‚¤íŒ¨ë“œë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì…ë ¥</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **textonyms**: ê°™ì€ keystroke ìˆœì„œë¡œ ì…ë ¥ë˜ëŠ” ë‘˜ ë˜ëŠ” ê·¸ ì´ìƒì˜ ë‹¨ì–´ë“¤\n",
    "- (ì˜ˆ) holeê³¼ goldì˜ í‚¤ ìˆœì„œëŠ” 4653\n",
    "- ì´ì™€ ê°™ì€ í‚¤ ìˆœì„œì˜ ë‹¤ë¥¸ ë‹¨ì–´ë¥¼ ì°¾ì•„ë³´ì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gold', 'golf', 'hold', 'hole']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:crimson\">[ì—°ìŠµğŸ˜‰2]</span> ìˆ«ì íŒ¨ë“œì˜ ì¼ë¶€ë¶„ë§Œ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ì¸ \"finger-twister\"ë¥¼ ì°¾ì•„ë³´ì."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ì˜ˆë¥¼ ë“¤ì–´, Â«^\\[ghijklmno\\]\\+\\\\$Â» ë˜ëŠ” ë” ì¶•ì•½í•´ì„œ Â«^[g-o]+\\\\$Â»ëŠ” ê°€ìš´ë° í–‰ì˜ 4, 5, 6 í‚¤ë§Œ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ì™€ matchí•  ê²ƒì´ë‹¤. Â«^[a-fj-o]+$Â» ëŠ” 2, 3, 5, 6 í‚¤ë“¤ë§Œ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ë“¤ê³¼ ë§¤ì¹˜ëœë‹¤. ì¶”ê°€ë¡œ, ì—¬ê¸°ì„œ `-` ì™€ `+`ëŠ” ë¬´ìŠ¨ ì˜ë¯¸ì¸ê°€? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `+`: ë°”ë¡œ ì•ì˜ ì•„ì´í…œì´ í•˜ë‚˜ ë˜ëŠ” ê·¸ ì´ìƒ ë°˜ë³µë¨\n",
    "- `*`: ë°”ë¡œ ì•ì˜ ì•„ì´í…œì´ ì œë¡œ ë˜ëŠ” ê·¸ ì´ìƒ ë°˜ë³µë¨\n",
    "- ì´ ë‘ ê¸°í˜¸ë¥¼ <span style=\"color:blue\">**Kleene closures**</span> ë˜ëŠ” ë‹¨ìˆœíˆ <span style=\"color: blue\">**closures**</span> ë¼ê³  í•¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '!',\n",
       " '!!',\n",
       " '!!!',\n",
       " '!!!!',\n",
       " '!!!!!',\n",
       " '!!!!!!',\n",
       " '!!!!!!!',\n",
       " '!!!!!!!!',\n",
       " '!!!!!!!!!',\n",
       " '!!!!!!!!!!',\n",
       " '!!!!!!!!!!!',\n",
       " '!!!!!!!!!!!!!',\n",
       " '!!!!!!!!!!!!!!!!',\n",
       " '!!!!!!!!!!!!!!!!!!!!!!',\n",
       " '!!!!!!!!!!!!!!!!!!!!!!!',\n",
       " '!!!!!!!!!!!!!!!!!!!!!!!!!!!',\n",
       " '!!!!!!!!!!!!!!!!!!!!!!!!!!!!',\n",
       " '!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!',\n",
       " '!!!!!!.',\n",
       " '!!!!!.',\n",
       " '!!!!....',\n",
       " '!!!.',\n",
       " '!!.',\n",
       " '!!...',\n",
       " '!.',\n",
       " '!...',\n",
       " '!=',\n",
       " '!?',\n",
       " '!??',\n",
       " '!???',\n",
       " '\"',\n",
       " '\"...',\n",
       " '\"?',\n",
       " '\"s',\n",
       " '#',\n",
       " '###',\n",
       " '####',\n",
       " '#14-19teens',\n",
       " '#40sPlus',\n",
       " '#prideIsland',\n",
       " '#prideisland',\n",
       " '#talkcity-20s',\n",
       " '#talkcity_adults',\n",
       " '$',\n",
       " '$$',\n",
       " '$27',\n",
       " '&',\n",
       " '&^',\n",
       " \"'\",\n",
       " \"''\",\n",
       " \"'.\",\n",
       " \"'d\",\n",
       " \"'ello\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'n'\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " '(',\n",
       " '( o Y o )',\n",
       " '(((',\n",
       " '((((',\n",
       " '(((((',\n",
       " '((((((',\n",
       " '(((((((',\n",
       " '((((((((',\n",
       " '(((((((((',\n",
       " '((((((((((',\n",
       " '(((((((((((',\n",
       " '((((((((((((',\n",
       " '(((((((((((((',\n",
       " '((((((((((((((',\n",
       " '(((((((((((((((',\n",
       " '(((((((((((((((((',\n",
       " '((((((((((((((((((',\n",
       " '((((((((((((((((((((',\n",
       " '(((((((((((((((((((((',\n",
       " '(((((((((((((((((((((((',\n",
       " '((((((((((((((((((((((((',\n",
       " '(((((((((((((((((((((((((',\n",
       " '((((((((((((((((((((((((((',\n",
       " '(((((..',\n",
       " '(*&(^',\n",
       " '(.',\n",
       " '(__I__)',\n",
       " ')',\n",
       " ')))',\n",
       " '))))',\n",
       " ')))))',\n",
       " ')))))))',\n",
       " '))))))))',\n",
       " ')))))))))',\n",
       " '))))))))))',\n",
       " ')))))))))))',\n",
       " '))))))))))))',\n",
       " ')))))))))))))',\n",
       " '))))))))))))))',\n",
       " ')))))))))))))))',\n",
       " ')))))))))))))))))',\n",
       " ')))))))))))))))))))',\n",
       " ')))))))))))))))))))))',\n",
       " '))))))))))))))))))))))',\n",
       " '))))))))))))))))))))))))))))',\n",
       " ')))))))))))))))))))))))))))))))',\n",
       " ')?',\n",
       " '*',\n",
       " '******',\n",
       " '*VBS*',\n",
       " '*WOW*',\n",
       " '*blush*',\n",
       " '*drools*',\n",
       " '*grins*',\n",
       " '*hugs*',\n",
       " '*smewchies*',\n",
       " '*sniffs*',\n",
       " '*spank*',\n",
       " '*waves*',\n",
       " '+',\n",
       " '+*+*+*+*',\n",
       " '++',\n",
       " ',',\n",
       " ',,',\n",
       " ',,,',\n",
       " ',,,,',\n",
       " ',,,,,',\n",
       " ',,,,,,,',\n",
       " ',,,,,,,,,,,',\n",
       " '-',\n",
       " '-(',\n",
       " '--',\n",
       " '-------------',\n",
       " '--------------------',\n",
       " '--------->',\n",
       " '-->',\n",
       " '-...)...-',\n",
       " '-17',\n",
       " '-21',\n",
       " '-6',\n",
       " '-_-',\n",
       " '-o',\n",
       " '-s',\n",
       " '-stay-',\n",
       " '.',\n",
       " '. .',\n",
       " '. . .',\n",
       " '. ...',\n",
       " '.(.',\n",
       " '.(..(.vMp3 v1.7.4.).)',\n",
       " '.(..(.vMp3 vi.p.t.)..).',\n",
       " '.)',\n",
       " '.).',\n",
       " '..',\n",
       " '.. .',\n",
       " '..(..',\n",
       " '...',\n",
       " '....',\n",
       " '.....',\n",
       " '......',\n",
       " '.......',\n",
       " '........',\n",
       " '.........',\n",
       " '..........',\n",
       " '...........',\n",
       " '............',\n",
       " '.............',\n",
       " '................',\n",
       " '..................',\n",
       " '...................',\n",
       " '....................',\n",
       " '........................',\n",
       " '..............................',\n",
       " '.45',\n",
       " '.:',\n",
       " '.;)',\n",
       " '.A.n.a.c.?.n.?.a.',\n",
       " '.op.',\n",
       " '.owner.',\n",
       " '/',\n",
       " '//',\n",
       " '//www.wunderground.com/cgi-bin/findweather/getForecast?query=95953#FIR',\n",
       " '0',\n",
       " '05.',\n",
       " '06.',\n",
       " '1',\n",
       " '1-900-anal-sex',\n",
       " '1.98',\n",
       " '1.99',\n",
       " '10',\n",
       " '100',\n",
       " '100%',\n",
       " '1012.',\n",
       " '1016.',\n",
       " '102.6',\n",
       " '10:49',\n",
       " '10th',\n",
       " '11',\n",
       " '12',\n",
       " '12%',\n",
       " '1200',\n",
       " '121.7',\n",
       " '1299',\n",
       " '13',\n",
       " '138',\n",
       " '14',\n",
       " '14-16',\n",
       " '147.7',\n",
       " '15',\n",
       " '16',\n",
       " '16.',\n",
       " '17',\n",
       " '18',\n",
       " '185',\n",
       " '18ST',\n",
       " '19',\n",
       " '1900',\n",
       " '1930',\n",
       " '1980',\n",
       " '1985',\n",
       " '1996',\n",
       " '1cos',\n",
       " '2',\n",
       " '2.3',\n",
       " '20',\n",
       " '20.',\n",
       " '2006',\n",
       " '20S',\n",
       " '20s',\n",
       " '21',\n",
       " '22',\n",
       " '220',\n",
       " '224',\n",
       " '23',\n",
       " '24',\n",
       " '246',\n",
       " '247',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '280',\n",
       " '28147',\n",
       " '29',\n",
       " '29.88.',\n",
       " '295',\n",
       " '29803',\n",
       " '2:55',\n",
       " '2DAY',\n",
       " '2Pac',\n",
       " '2nd',\n",
       " '3',\n",
       " '30',\n",
       " '30.',\n",
       " '30.00.',\n",
       " '300',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '3333333',\n",
       " '33982',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '360',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '39.3',\n",
       " '396',\n",
       " '3:45',\n",
       " '3~<-..4@.',\n",
       " '4',\n",
       " '4.20',\n",
       " '41',\n",
       " '423',\n",
       " '43',\n",
       " '43.',\n",
       " '45',\n",
       " '45.5',\n",
       " '453',\n",
       " '46',\n",
       " '46.',\n",
       " '47',\n",
       " '47.',\n",
       " '49',\n",
       " '4:03',\n",
       " '5',\n",
       " '50',\n",
       " '51',\n",
       " '53',\n",
       " '55',\n",
       " '55%',\n",
       " '55.',\n",
       " '56',\n",
       " '56.',\n",
       " '57',\n",
       " '57401',\n",
       " '579',\n",
       " '59',\n",
       " '59%',\n",
       " '6',\n",
       " '60',\n",
       " '60s',\n",
       " '64.8',\n",
       " '65%',\n",
       " '68%',\n",
       " '69',\n",
       " '6:38',\n",
       " '6:41',\n",
       " '6:51',\n",
       " '6:53',\n",
       " '7',\n",
       " '70%',\n",
       " '700',\n",
       " '73%',\n",
       " '73042',\n",
       " '75',\n",
       " '75%',\n",
       " '76%',\n",
       " '77',\n",
       " '7:45',\n",
       " '8',\n",
       " '80',\n",
       " '8082653953',\n",
       " '818',\n",
       " '85%',\n",
       " '9',\n",
       " '9.53',\n",
       " '90',\n",
       " '92129',\n",
       " '92780',\n",
       " '93',\n",
       " '93%',\n",
       " '95953',\n",
       " '98.5',\n",
       " '98.6',\n",
       " '99',\n",
       " '99701',\n",
       " '99703',\n",
       " '9:10',\n",
       " ':',\n",
       " ':(',\n",
       " ':)',\n",
       " ':):):)',\n",
       " ':-(',\n",
       " ':-)',\n",
       " ':-@',\n",
       " ':-o',\n",
       " ':.',\n",
       " ':/',\n",
       " ':@',\n",
       " ':D',\n",
       " ':O',\n",
       " ':P',\n",
       " ':]',\n",
       " ':beer:',\n",
       " ':blush:',\n",
       " ':love:',\n",
       " ':o *',\n",
       " ':p',\n",
       " ':tongue:',\n",
       " ':|',\n",
       " ';',\n",
       " '; ..',\n",
       " ';)',\n",
       " ';-(',\n",
       " ';-)',\n",
       " ';0',\n",
       " ';]',\n",
       " ';p',\n",
       " '<',\n",
       " '<,',\n",
       " '<-',\n",
       " '<--',\n",
       " '<---',\n",
       " '<----',\n",
       " '<----------',\n",
       " '<3',\n",
       " \"<3's\",\n",
       " '<33',\n",
       " '<333',\n",
       " '<3333',\n",
       " '<33333',\n",
       " '<333333333',\n",
       " '<3333333333333333',\n",
       " '<33333333333333333',\n",
       " '<<',\n",
       " '<<<',\n",
       " '<<<<',\n",
       " '<<<<,',\n",
       " '<<<<<',\n",
       " '<<<<<<',\n",
       " '<<<<<<,',\n",
       " '<<<<<<<',\n",
       " '<<<<<<<<<<<<<<',\n",
       " '<empty>',\n",
       " '<perk>',\n",
       " '<~~~',\n",
       " '=',\n",
       " \"='s\",\n",
       " '=(',\n",
       " '=)',\n",
       " '=-\\\\',\n",
       " '=/',\n",
       " '=D',\n",
       " '=O',\n",
       " '=[',\n",
       " '=]',\n",
       " '=p',\n",
       " '>',\n",
       " '>.>',\n",
       " '>.>->',\n",
       " '>:->',\n",
       " '>>>',\n",
       " '>>>>>>>>>>',\n",
       " '>>>>>>>>>>>',\n",
       " '>>>>>>>>>>>>',\n",
       " '>?',\n",
       " '>_>',\n",
       " '?',\n",
       " '?!',\n",
       " '?!?!',\n",
       " '?!?!?',\n",
       " \"?'\",\n",
       " '?.',\n",
       " '?..',\n",
       " '?....',\n",
       " '??',\n",
       " '??!!',\n",
       " '??!?!??!',\n",
       " '???',\n",
       " '????',\n",
       " '?????',\n",
       " '??????',\n",
       " '???????',\n",
       " '????????',\n",
       " '?????????',\n",
       " '??@',\n",
       " '@',\n",
       " '@$$',\n",
       " \"@-,'~\",\n",
       " \"@..3-,'~.\",\n",
       " 'A',\n",
       " 'ABOUT',\n",
       " 'ACTION',\n",
       " 'AFK',\n",
       " 'AGAIN',\n",
       " 'AHAHH',\n",
       " 'AHAHHA',\n",
       " 'AHHAH',\n",
       " 'AI',\n",
       " 'AKDT',\n",
       " 'AKST',\n",
       " 'ALL',\n",
       " 'AM',\n",
       " 'AND',\n",
       " 'ANY',\n",
       " 'ANYONE',\n",
       " 'AOL.COM',\n",
       " 'ARE',\n",
       " 'AROUND',\n",
       " 'ASS',\n",
       " 'AWAY',\n",
       " 'Aberdeen',\n",
       " 'About',\n",
       " 'Ack',\n",
       " 'Actually',\n",
       " 'Added',\n",
       " 'Advisory',\n",
       " 'Again',\n",
       " 'Ah',\n",
       " 'Ahh',\n",
       " 'Ahhh',\n",
       " 'Ahhhh',\n",
       " 'Aiken',\n",
       " 'Alaska',\n",
       " 'Albany',\n",
       " 'Almost',\n",
       " 'Always',\n",
       " 'Amazingness',\n",
       " 'American',\n",
       " 'Americans',\n",
       " 'Amy',\n",
       " 'An',\n",
       " 'And',\n",
       " 'Any',\n",
       " 'Anyone',\n",
       " 'Anyway',\n",
       " 'Apocalypse',\n",
       " 'Apparently',\n",
       " 'Are',\n",
       " 'Ark',\n",
       " 'Arkansas',\n",
       " 'As',\n",
       " 'Ask',\n",
       " 'At',\n",
       " 'Average',\n",
       " 'Aw',\n",
       " 'Away',\n",
       " 'Aww',\n",
       " 'Awww',\n",
       " 'B',\n",
       " 'BAAAAALLLLLLLLIIIIIIINNNNNNNNNNN',\n",
       " 'BE',\n",
       " 'BIG',\n",
       " 'BLONDES',\n",
       " 'BOOTS',\n",
       " 'BOOTY',\n",
       " 'BOY',\n",
       " 'BUT',\n",
       " 'BUt',\n",
       " 'BYE',\n",
       " 'Back',\n",
       " 'Barbieee',\n",
       " 'Barometer',\n",
       " 'Beach',\n",
       " 'Because',\n",
       " 'Been',\n",
       " 'Ben',\n",
       " 'Benjamin',\n",
       " 'Better',\n",
       " 'Bible',\n",
       " 'Biiiiiitch',\n",
       " 'Biographys',\n",
       " 'Birdgang',\n",
       " 'Bloooooooood',\n",
       " 'Bloooooooooood',\n",
       " 'Bloooooooooooood',\n",
       " 'Bone',\n",
       " 'Bonus',\n",
       " 'Books',\n",
       " 'Boone',\n",
       " 'Booyah',\n",
       " 'Borat',\n",
       " 'Born',\n",
       " 'Box',\n",
       " 'Boyz',\n",
       " 'Break',\n",
       " 'Breaking',\n",
       " 'Broken',\n",
       " 'Bud',\n",
       " 'Burger',\n",
       " 'But',\n",
       " 'Bwhaha',\n",
       " 'Bye',\n",
       " 'C',\n",
       " 'CA',\n",
       " 'CALI',\n",
       " 'CAN',\n",
       " 'CAPS',\n",
       " 'CDT',\n",
       " 'CHAT',\n",
       " 'CHATHIDE',\n",
       " 'CHIPS',\n",
       " 'CHOCO',\n",
       " 'CO',\n",
       " 'COM',\n",
       " 'COME',\n",
       " 'CSI',\n",
       " 'CST',\n",
       " 'CT',\n",
       " 'CUZ',\n",
       " 'California',\n",
       " 'Came',\n",
       " 'Can',\n",
       " 'CanEhda',\n",
       " 'Cardinals',\n",
       " 'Cardnials',\n",
       " 'Cards',\n",
       " 'Care',\n",
       " 'Carolina',\n",
       " 'Catterick',\n",
       " 'Ceiling',\n",
       " 'Chamillionaire',\n",
       " 'Change',\n",
       " 'Changing',\n",
       " 'Chat',\n",
       " 'Check',\n",
       " 'Checked',\n",
       " 'Cheeeez',\n",
       " 'Chica',\n",
       " 'Chickens',\n",
       " 'Children',\n",
       " 'China',\n",
       " 'Chingy',\n",
       " 'Chop',\n",
       " 'Chris',\n",
       " 'Christianity',\n",
       " 'Ciara',\n",
       " 'City',\n",
       " 'Cleveland',\n",
       " 'Clock',\n",
       " 'Coincidence',\n",
       " 'Come',\n",
       " 'Compliments',\n",
       " 'Connected',\n",
       " 'Connecticutt',\n",
       " 'Considerably',\n",
       " 'Constitution',\n",
       " 'Cookies',\n",
       " 'Cool',\n",
       " 'Could',\n",
       " 'Course',\n",
       " 'Covered',\n",
       " 'Cradle',\n",
       " 'Craig',\n",
       " 'Crazy',\n",
       " 'Cream',\n",
       " 'Cry',\n",
       " 'Ct',\n",
       " 'Ctrl',\n",
       " 'Cum',\n",
       " 'Current',\n",
       " 'Cute',\n",
       " 'Cyber',\n",
       " 'D',\n",
       " 'DAMN',\n",
       " 'DAamn',\n",
       " 'DELIGHTFUL',\n",
       " 'DETROIT',\n",
       " 'DING',\n",
       " 'DIRTY',\n",
       " 'DJ',\n",
       " 'DO',\n",
       " 'DOES',\n",
       " 'DOING',\n",
       " 'DON',\n",
       " 'DONT',\n",
       " 'DOWNS',\n",
       " 'DVD',\n",
       " 'Dakota',\n",
       " 'Damn',\n",
       " 'Dang',\n",
       " 'Daniel',\n",
       " 'Daveeee',\n",
       " 'David',\n",
       " 'Dawn',\n",
       " 'Dawnstar',\n",
       " 'Days',\n",
       " 'Death',\n",
       " 'Deep',\n",
       " 'Define',\n",
       " 'Denver',\n",
       " 'Depends',\n",
       " 'Devil',\n",
       " 'Dew',\n",
       " 'Diary',\n",
       " 'Did',\n",
       " 'Diego',\n",
       " 'Dipset',\n",
       " 'Dixie',\n",
       " 'Do',\n",
       " 'Does',\n",
       " 'Doing',\n",
       " 'Dokken',\n",
       " 'Dolls',\n",
       " 'Dood',\n",
       " 'Down',\n",
       " 'Downy',\n",
       " 'Dr',\n",
       " 'Dr.',\n",
       " 'Dreams',\n",
       " 'Drew',\n",
       " 'Drive',\n",
       " 'Drop',\n",
       " 'Dude',\n",
       " 'Dustin',\n",
       " 'Dying',\n",
       " 'ELSE',\n",
       " 'ENOUGH',\n",
       " 'EST',\n",
       " 'EVEN',\n",
       " 'EVERYTHING',\n",
       " 'Earth',\n",
       " 'Easily',\n",
       " 'Eastern',\n",
       " 'Eddie',\n",
       " 'Edgewood',\n",
       " 'Eggs',\n",
       " 'Elev',\n",
       " 'Elle',\n",
       " 'End',\n",
       " 'Eticket',\n",
       " 'Evanescence',\n",
       " 'Even',\n",
       " 'Everyone',\n",
       " 'Everytime',\n",
       " 'Evil',\n",
       " 'Eyes',\n",
       " 'F',\n",
       " 'F5',\n",
       " 'FACE',\n",
       " 'FEMALE',\n",
       " 'FF',\n",
       " 'FINE',\n",
       " 'FL',\n",
       " 'FOLKS',\n",
       " 'FROM',\n",
       " 'Fade',\n",
       " 'Fails',\n",
       " 'Fairbanks',\n",
       " 'Favorite',\n",
       " 'Females',\n",
       " 'Fergalicious',\n",
       " 'Fergie',\n",
       " 'Fetish',\n",
       " 'Fighting',\n",
       " 'Figured',\n",
       " 'Filth',\n",
       " 'Finally',\n",
       " 'Finding',\n",
       " 'Finger',\n",
       " 'Fingers',\n",
       " 'First',\n",
       " 'Fisher',\n",
       " 'Fishers',\n",
       " 'Fix',\n",
       " 'Fixed',\n",
       " 'Flames',\n",
       " 'Flatts',\n",
       " 'Florida',\n",
       " 'Foley',\n",
       " 'Food',\n",
       " 'For',\n",
       " 'Fort',\n",
       " 'Foxwoods',\n",
       " 'FreesBee',\n",
       " 'Friday',\n",
       " 'From',\n",
       " 'Froogle',\n",
       " 'G',\n",
       " 'G-Mobile',\n",
       " 'GA',\n",
       " 'GIRL',\n",
       " 'GIRLS',\n",
       " 'GN',\n",
       " 'GNG',\n",
       " 'GOING',\n",
       " 'GOOD',\n",
       " 'GUYS',\n",
       " 'Gay',\n",
       " 'Geographic',\n",
       " 'Get',\n",
       " 'Ghetto',\n",
       " 'Girl',\n",
       " 'Go',\n",
       " 'God',\n",
       " 'Good',\n",
       " 'Gorda',\n",
       " 'Gosh',\n",
       " 'Gothic',\n",
       " 'Gracemont',\n",
       " 'Great',\n",
       " 'Greetings',\n",
       " 'GrlZ',\n",
       " 'Groups',\n",
       " 'Gs',\n",
       " 'Guess',\n",
       " 'Guy',\n",
       " 'H',\n",
       " 'H0rny',\n",
       " 'HAHA',\n",
       " 'HAHAHA',\n",
       " 'HALO',\n",
       " 'HAVE',\n",
       " 'HE',\n",
       " 'HELLO',\n",
       " 'HERE',\n",
       " 'HEY',\n",
       " 'HHEEYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY',\n",
       " 'HI',\n",
       " 'HOT',\n",
       " 'HOTT',\n",
       " 'HOW',\n",
       " 'HUGE',\n",
       " 'HUH',\n",
       " 'Ha',\n",
       " 'Haha',\n",
       " 'Hahaaaa',\n",
       " 'Hahhaa',\n",
       " 'Hail',\n",
       " 'Hallo',\n",
       " 'Hand',\n",
       " 'Hard',\n",
       " 'Harmony',\n",
       " 'Have',\n",
       " 'Hay',\n",
       " 'He',\n",
       " 'Hello',\n",
       " 'Help',\n",
       " 'Her',\n",
       " 'Here',\n",
       " 'Hero',\n",
       " 'Hey',\n",
       " 'Heya',\n",
       " 'Heys',\n",
       " 'Heyy',\n",
       " 'Heyyy',\n",
       " 'Heyyyyyyy',\n",
       " 'Hi',\n",
       " 'High',\n",
       " 'Highway',\n",
       " 'Hill',\n",
       " 'History',\n",
       " 'Hiya',\n",
       " 'Hmm',\n",
       " 'Hold',\n",
       " 'Holla',\n",
       " 'Holland',\n",
       " 'HolocaustYourMom',\n",
       " 'Holy',\n",
       " 'Home',\n",
       " 'Horace',\n",
       " 'Hott',\n",
       " 'How',\n",
       " 'Howdy',\n",
       " 'Hug',\n",
       " 'Hughes',\n",
       " 'Hugs',\n",
       " 'Huh',\n",
       " 'Humidity',\n",
       " 'Hummmm',\n",
       " 'Hungry',\n",
       " 'I',\n",
       " 'ID',\n",
       " 'IF',\n",
       " 'II',\n",
       " 'IL',\n",
       " 'IM',\n",
       " 'IN',\n",
       " 'INTERESTING',\n",
       " 'IRC',\n",
       " 'IS',\n",
       " 'IT',\n",
       " 'Ico',\n",
       " 'Id',\n",
       " 'If',\n",
       " 'Im',\n",
       " 'Ima',\n",
       " 'Images',\n",
       " 'In',\n",
       " 'In.',\n",
       " 'Indeed',\n",
       " 'Indiantown',\n",
       " 'Iowa',\n",
       " 'Is',\n",
       " 'It',\n",
       " 'Its',\n",
       " 'Ive',\n",
       " 'JESUS',\n",
       " 'JOIN',\n",
       " 'JRZ',\n",
       " 'JTo',\n",
       " 'JUST',\n",
       " 'Jam',\n",
       " 'James',\n",
       " 'Jane',\n",
       " 'Jason',\n",
       " 'Jayse',\n",
       " 'Jerketts',\n",
       " 'Jess',\n",
       " 'Jesus',\n",
       " 'Jeter',\n",
       " 'Joe',\n",
       " 'Joey',\n",
       " 'John',\n",
       " 'Jon',\n",
       " 'Jones',\n",
       " 'Jonesboro',\n",
       " 'Jordison',\n",
       " 'Joshy',\n",
       " 'Judy',\n",
       " 'Just',\n",
       " 'Justin',\n",
       " 'K',\n",
       " 'K-Fed',\n",
       " 'KNOW',\n",
       " 'Kansas',\n",
       " 'Kellogs',\n",
       " 'Kent',\n",
       " 'Kentucky',\n",
       " 'Kewl',\n",
       " 'Kick',\n",
       " 'Kids',\n",
       " 'King',\n",
       " 'Kiss',\n",
       " 'Kittie',\n",
       " 'KoOL',\n",
       " 'Kold',\n",
       " 'LA',\n",
       " 'LATE',\n",
       " 'LATER',\n",
       " 'LAst',\n",
       " 'LIVE',\n",
       " 'LIX',\n",
       " 'LMAO',\n",
       " 'LOL',\n",
       " 'LOLOLOLLL',\n",
       " 'LONG',\n",
       " 'LONLEY',\n",
       " 'LOUD',\n",
       " 'LOUDER',\n",
       " 'LOVES',\n",
       " 'LOl',\n",
       " 'LPN',\n",
       " 'Ladies',\n",
       " 'Laguna',\n",
       " 'Lampert',\n",
       " 'Last',\n",
       " 'Laters',\n",
       " 'Lay',\n",
       " 'Lee',\n",
       " 'Leeches',\n",
       " 'Length',\n",
       " 'Let',\n",
       " 'Lets',\n",
       " 'Liam',\n",
       " 'Lies',\n",
       " 'Life',\n",
       " 'Like',\n",
       " 'Lil',\n",
       " 'Lime',\n",
       " 'Lion',\n",
       " 'Lithium',\n",
       " 'Little',\n",
       " 'Live',\n",
       " 'Lives',\n",
       " 'Living',\n",
       " 'Lmao',\n",
       " 'Lmfao',\n",
       " 'LoL',\n",
       " 'LoVe',\n",
       " 'Lol',\n",
       " 'London',\n",
       " 'Long',\n",
       " 'Look',\n",
       " 'Looking',\n",
       " 'Lord',\n",
       " 'Louisville',\n",
       " 'Lousiana',\n",
       " 'Love',\n",
       " 'Lovely',\n",
       " 'LuverZ',\n",
       " 'M',\n",
       " 'MAN',\n",
       " 'MATCH',\n",
       " 'MD',\n",
       " 'ME',\n",
       " 'MISHAP',\n",
       " 'MODE',\n",
       " 'MORE',\n",
       " 'MOUTH',\n",
       " 'MP3',\n",
       " 'MRIs',\n",
       " 'MSN',\n",
       " 'MUAH',\n",
       " 'MY',\n",
       " 'Maidstone',\n",
       " 'Male',\n",
       " 'Man',\n",
       " 'Maps',\n",
       " 'Marlaya',\n",
       " 'Martian',\n",
       " 'Marvin',\n",
       " 'Mary',\n",
       " 'Matt',\n",
       " 'Max',\n",
       " 'Maybe',\n",
       " 'Me',\n",
       " 'Meep',\n",
       " 'Meh',\n",
       " 'Memory',\n",
       " 'Men',\n",
       " 'Mercy',\n",
       " 'Messaging',\n",
       " 'Metallica',\n",
       " 'Michigan',\n",
       " 'Midwest',\n",
       " 'Mine',\n",
       " 'Mmm',\n",
       " 'Mo',\n",
       " 'Mom',\n",
       " 'Money',\n",
       " 'Mono',\n",
       " 'Mooooooooooooooooooooooooooo',\n",
       " 'Morgan',\n",
       " 'Mp3',\n",
       " 'Ms',\n",
       " 'MsUtah',\n",
       " 'Muahz',\n",
       " 'Music',\n",
       " 'My',\n",
       " 'N',\n",
       " 'N\"T',\n",
       " \"N'T\",\n",
       " 'NAME',\n",
       " 'NC',\n",
       " 'NICK',\n",
       " 'NIght',\n",
       " ...]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\n",
    "chat_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee',\n",
       " 'miiiiiinnnnnnnnnneeeeeeee',\n",
       " 'mine',\n",
       " 'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in chat_words if re.search('^m+i+n+e+$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'aaaaaaaaaaaaaaaaa',\n",
       " 'aaahhhh',\n",
       " 'ah',\n",
       " 'ahah',\n",
       " 'ahahah',\n",
       " 'ahh',\n",
       " 'ahhahahaha',\n",
       " 'ahhh',\n",
       " 'ahhhh',\n",
       " 'ahhhhhh',\n",
       " 'ahhhhhhhhhhhhhh',\n",
       " 'h',\n",
       " 'ha',\n",
       " 'haaa',\n",
       " 'hah',\n",
       " 'haha',\n",
       " 'hahaaa',\n",
       " 'hahah',\n",
       " 'hahaha',\n",
       " 'hahahaa',\n",
       " 'hahahah',\n",
       " 'hahahaha',\n",
       " 'hahahahaaa',\n",
       " 'hahahahahaha',\n",
       " 'hahahahahahaha',\n",
       " 'hahahahahahahahahahahahahahahaha',\n",
       " 'hahahhahah',\n",
       " 'hahhahahaha']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in chat_words if re.search('^[ha]+$', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`^`ê°€ square bracket ë‚´ì˜ ì²«ê¸€ìì— ìˆëŠ” ê²½ìš°\n",
    "- (ì˜ˆ) Â«[^aeiouAEIOU]Â» : ëª¨ìŒì´ ì•„ë‹Œ ëª¨ë“  ê¸€ì\n",
    "- (ì˜ˆ) Â«^[^aeiouAEIOU]+\\\\$Â» : ëª¨ìŒì´ ì•„ë‹Œ ê¸€ìë¡œë§Œ ëœ ë‹¨ì–´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\\`, `{}`, `()`, `|` ì˜ ì‚¬ìš©ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsj = sorted(set(nltk.corpus.treebank.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.0085',\n",
       " '0.05',\n",
       " '0.1',\n",
       " '0.16',\n",
       " '0.2',\n",
       " '0.25',\n",
       " '0.28',\n",
       " '0.3',\n",
       " '0.4',\n",
       " '0.5',\n",
       " '0.50',\n",
       " '0.54',\n",
       " '0.56',\n",
       " '0.60',\n",
       " '0.7',\n",
       " '0.82',\n",
       " '0.84',\n",
       " '0.9',\n",
       " '0.95',\n",
       " '0.99',\n",
       " '1.01',\n",
       " '1.1',\n",
       " '1.125',\n",
       " '1.14',\n",
       " '1.1650',\n",
       " '1.17',\n",
       " '1.18',\n",
       " '1.19',\n",
       " '1.2',\n",
       " '1.20',\n",
       " '1.24',\n",
       " '1.25',\n",
       " '1.26',\n",
       " '1.28',\n",
       " '1.35',\n",
       " '1.39',\n",
       " '1.4',\n",
       " '1.457',\n",
       " '1.46',\n",
       " '1.49',\n",
       " '1.5',\n",
       " '1.50',\n",
       " '1.55',\n",
       " '1.56',\n",
       " '1.5755',\n",
       " '1.5805',\n",
       " '1.6',\n",
       " '1.61',\n",
       " '1.637',\n",
       " '1.64',\n",
       " '1.65',\n",
       " '1.7',\n",
       " '1.75',\n",
       " '1.76',\n",
       " '1.8',\n",
       " '1.82',\n",
       " '1.8415',\n",
       " '1.85',\n",
       " '1.8500',\n",
       " '1.9',\n",
       " '1.916',\n",
       " '1.92',\n",
       " '10.19',\n",
       " '10.2',\n",
       " '10.5',\n",
       " '107.03',\n",
       " '107.9',\n",
       " '109.73',\n",
       " '11.10',\n",
       " '11.5',\n",
       " '11.57',\n",
       " '11.6',\n",
       " '11.72',\n",
       " '11.95',\n",
       " '112.9',\n",
       " '113.2',\n",
       " '116.3',\n",
       " '116.4',\n",
       " '116.7',\n",
       " '116.9',\n",
       " '118.6',\n",
       " '12.09',\n",
       " '12.5',\n",
       " '12.52',\n",
       " '12.68',\n",
       " '12.7',\n",
       " '12.82',\n",
       " '12.97',\n",
       " '120.7',\n",
       " '1206.26',\n",
       " '121.6',\n",
       " '126.1',\n",
       " '126.15',\n",
       " '127.03',\n",
       " '129.91',\n",
       " '13.1',\n",
       " '13.15',\n",
       " '13.5',\n",
       " '13.50',\n",
       " '13.625',\n",
       " '13.65',\n",
       " '13.73',\n",
       " '13.8',\n",
       " '13.90',\n",
       " '130.6',\n",
       " '130.7',\n",
       " '131.01',\n",
       " '132.9',\n",
       " '133.7',\n",
       " '133.8',\n",
       " '14.00',\n",
       " '14.13',\n",
       " '14.26',\n",
       " '14.28',\n",
       " '14.43',\n",
       " '14.5',\n",
       " '14.53',\n",
       " '14.54',\n",
       " '14.6',\n",
       " '14.75',\n",
       " '14.99',\n",
       " '141.9',\n",
       " '142.84',\n",
       " '142.85',\n",
       " '143.08',\n",
       " '143.80',\n",
       " '143.93',\n",
       " '148.9',\n",
       " '149.9',\n",
       " '15.5',\n",
       " '150.00',\n",
       " '153.3',\n",
       " '154.2',\n",
       " '16.05',\n",
       " '16.09',\n",
       " '16.125',\n",
       " '16.2',\n",
       " '16.5',\n",
       " '16.68',\n",
       " '16.7',\n",
       " '16.9',\n",
       " '169.9',\n",
       " '17.3',\n",
       " '17.4',\n",
       " '17.5',\n",
       " '17.95',\n",
       " '1738.1',\n",
       " '176.1',\n",
       " '18.3',\n",
       " '18.6',\n",
       " '18.95',\n",
       " '185.9',\n",
       " '188.84',\n",
       " '19.3',\n",
       " '19.50',\n",
       " '19.6',\n",
       " '19.94',\n",
       " '19.95',\n",
       " '191.9',\n",
       " '2.07',\n",
       " '2.1',\n",
       " '2.15',\n",
       " '2.19',\n",
       " '2.2',\n",
       " '2.25',\n",
       " '2.29',\n",
       " '2.3',\n",
       " '2.30',\n",
       " '2.35',\n",
       " '2.375',\n",
       " '2.4',\n",
       " '2.42',\n",
       " '2.44',\n",
       " '2.46',\n",
       " '2.47',\n",
       " '2.5',\n",
       " '2.50',\n",
       " '2.6',\n",
       " '2.62',\n",
       " '2.65',\n",
       " '2.7',\n",
       " '2.75',\n",
       " '2.8',\n",
       " '2.80',\n",
       " '2.87',\n",
       " '2.875',\n",
       " '2.9',\n",
       " '2.95',\n",
       " '20.07',\n",
       " '20.5',\n",
       " '21.1',\n",
       " '21.9',\n",
       " '2141.7',\n",
       " '2160.1',\n",
       " '2163.2',\n",
       " '22.75',\n",
       " '220.45',\n",
       " '221.4',\n",
       " '225.6',\n",
       " '23.25',\n",
       " '23.4',\n",
       " '23.5',\n",
       " '23.72',\n",
       " '234.4',\n",
       " '236.74',\n",
       " '236.79',\n",
       " '24.95',\n",
       " '25.50',\n",
       " '25.6',\n",
       " '251.2',\n",
       " '26.2',\n",
       " '26.5',\n",
       " '26.8',\n",
       " '263.07',\n",
       " '2645.90',\n",
       " '2691.19',\n",
       " '27.1',\n",
       " '27.4',\n",
       " '273.5',\n",
       " '278.7',\n",
       " '28.25',\n",
       " '28.36',\n",
       " '28.4',\n",
       " '28.5',\n",
       " '28.53',\n",
       " '28.6',\n",
       " '29.3',\n",
       " '29.4',\n",
       " '29.9',\n",
       " '292.32',\n",
       " '3.01',\n",
       " '3.04',\n",
       " '3.1',\n",
       " '3.16',\n",
       " '3.18',\n",
       " '3.19',\n",
       " '3.2',\n",
       " '3.20',\n",
       " '3.23',\n",
       " '3.253',\n",
       " '3.28',\n",
       " '3.3',\n",
       " '3.35',\n",
       " '3.375',\n",
       " '3.4',\n",
       " '3.42',\n",
       " '3.43',\n",
       " '3.5',\n",
       " '3.55',\n",
       " '3.6',\n",
       " '3.61',\n",
       " '3.625',\n",
       " '3.7',\n",
       " '3.75',\n",
       " '3.8',\n",
       " '3.80',\n",
       " '3.9',\n",
       " '30.6',\n",
       " '30.9',\n",
       " '319.75',\n",
       " '32.8',\n",
       " '334.5',\n",
       " '34.625',\n",
       " '341.20',\n",
       " '3436.58',\n",
       " '35.2',\n",
       " '35.7',\n",
       " '352.7',\n",
       " '352.9',\n",
       " '35500.64',\n",
       " '35564.43',\n",
       " '36.9',\n",
       " '361.8',\n",
       " '3648.82',\n",
       " '37.3',\n",
       " '37.5',\n",
       " '372.14',\n",
       " '372.9',\n",
       " '374.19',\n",
       " '374.20',\n",
       " '377.60',\n",
       " '38.3',\n",
       " '38.375',\n",
       " '38.5',\n",
       " '38.875',\n",
       " '387.8',\n",
       " '4.1',\n",
       " '4.10',\n",
       " '4.2',\n",
       " '4.25',\n",
       " '4.3',\n",
       " '4.4',\n",
       " '4.5',\n",
       " '4.55',\n",
       " '4.6',\n",
       " '4.7',\n",
       " '4.75',\n",
       " '4.8',\n",
       " '4.875',\n",
       " '4.898',\n",
       " '4.9',\n",
       " '40.21',\n",
       " '41.60',\n",
       " '415.6',\n",
       " '415.8',\n",
       " '42.1',\n",
       " '42.5',\n",
       " '422.5',\n",
       " '43.875',\n",
       " '434.4',\n",
       " '436.01',\n",
       " '446.62',\n",
       " '449.04',\n",
       " '45.2',\n",
       " '45.3',\n",
       " '45.75',\n",
       " '456.64',\n",
       " '46.1',\n",
       " '47.1',\n",
       " '47.125',\n",
       " '47.5',\n",
       " '47.6',\n",
       " '49.9',\n",
       " '494.50',\n",
       " '497.34',\n",
       " '5.1',\n",
       " '5.2180',\n",
       " '5.276',\n",
       " '5.29',\n",
       " '5.3',\n",
       " '5.39',\n",
       " '5.4',\n",
       " '5.435',\n",
       " '5.5',\n",
       " '5.57',\n",
       " '5.6',\n",
       " '5.63',\n",
       " '5.7',\n",
       " '5.70',\n",
       " '5.8',\n",
       " '5.82',\n",
       " '5.9',\n",
       " '5.92',\n",
       " '50.1',\n",
       " '50.38',\n",
       " '50.45',\n",
       " '51.25',\n",
       " '51.6',\n",
       " '55.1',\n",
       " '566.54',\n",
       " '57.50',\n",
       " '57.6',\n",
       " '57.7',\n",
       " '58.64',\n",
       " '59.6',\n",
       " '59.9',\n",
       " '6.03',\n",
       " '6.1',\n",
       " '6.20',\n",
       " '6.21',\n",
       " '6.25',\n",
       " '6.4',\n",
       " '6.40',\n",
       " '6.44',\n",
       " '6.5',\n",
       " '6.50',\n",
       " '6.53',\n",
       " '6.6',\n",
       " '6.7',\n",
       " '6.70',\n",
       " '6.79',\n",
       " '6.84',\n",
       " '6.9',\n",
       " '60.36',\n",
       " '618.1',\n",
       " '62.1',\n",
       " '62.5',\n",
       " '62.625',\n",
       " '63.79',\n",
       " '630.9',\n",
       " '64.5',\n",
       " '66.5',\n",
       " '7.15',\n",
       " '7.2',\n",
       " '7.20',\n",
       " '7.272',\n",
       " '7.3',\n",
       " '7.4',\n",
       " '7.40',\n",
       " '7.422',\n",
       " '7.45',\n",
       " '7.458',\n",
       " '7.5',\n",
       " '7.50',\n",
       " '7.52',\n",
       " '7.55',\n",
       " '7.60',\n",
       " '7.62',\n",
       " '7.63',\n",
       " '7.65',\n",
       " '7.74',\n",
       " '7.78',\n",
       " '7.79',\n",
       " '7.8',\n",
       " '7.80',\n",
       " '7.84',\n",
       " '7.88',\n",
       " '7.90',\n",
       " '7.95',\n",
       " '70.2',\n",
       " '70.7',\n",
       " '705.6',\n",
       " '72.7',\n",
       " '734.9',\n",
       " '737.5',\n",
       " '77.56',\n",
       " '77.6',\n",
       " '77.70',\n",
       " '8.04',\n",
       " '8.06',\n",
       " '8.07',\n",
       " '8.1',\n",
       " '8.12',\n",
       " '8.14',\n",
       " '8.15',\n",
       " '8.19',\n",
       " '8.2',\n",
       " '8.22',\n",
       " '8.25',\n",
       " '8.30',\n",
       " '8.35',\n",
       " '8.45',\n",
       " '8.467',\n",
       " '8.47',\n",
       " '8.48',\n",
       " '8.5',\n",
       " '8.50',\n",
       " '8.53',\n",
       " '8.55',\n",
       " '8.56',\n",
       " '8.575',\n",
       " '8.60',\n",
       " '8.64',\n",
       " '8.65',\n",
       " '8.70',\n",
       " '8.75',\n",
       " '8.9',\n",
       " '80.50',\n",
       " '80.8',\n",
       " '81.8',\n",
       " '811.9',\n",
       " '83.4',\n",
       " '84.29',\n",
       " '84.9',\n",
       " '85.1',\n",
       " '85.7',\n",
       " '86.12',\n",
       " '87.5',\n",
       " '88.32',\n",
       " '89.7',\n",
       " '89.9',\n",
       " '9.3',\n",
       " '9.32',\n",
       " '9.37',\n",
       " '9.45',\n",
       " '9.5',\n",
       " '9.625',\n",
       " '9.75',\n",
       " '9.8',\n",
       " '9.82',\n",
       " '9.9',\n",
       " '92.9',\n",
       " '93.3',\n",
       " '93.9',\n",
       " '94.2',\n",
       " '94.8',\n",
       " '95.09',\n",
       " '96.4',\n",
       " '98.3',\n",
       " '99.1',\n",
       " '99.3']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C$', 'US$']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('^[A-Z]+\\$$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1614',\n",
       " '1637',\n",
       " '1787',\n",
       " '1901',\n",
       " '1903',\n",
       " '1917',\n",
       " '1925',\n",
       " '1929',\n",
       " '1933',\n",
       " '1934',\n",
       " '1948',\n",
       " '1953',\n",
       " '1955',\n",
       " '1956',\n",
       " '1961',\n",
       " '1965',\n",
       " '1966',\n",
       " '1967',\n",
       " '1968',\n",
       " '1969',\n",
       " '1970',\n",
       " '1971',\n",
       " '1972',\n",
       " '1973',\n",
       " '1975',\n",
       " '1976',\n",
       " '1977',\n",
       " '1979',\n",
       " '1980',\n",
       " '1981',\n",
       " '1982',\n",
       " '1983',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1987',\n",
       " '1988',\n",
       " '1989',\n",
       " '1990',\n",
       " '1991',\n",
       " '1992',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '2000',\n",
       " '2005',\n",
       " '2009',\n",
       " '2017',\n",
       " '2019',\n",
       " '2029',\n",
       " '3057',\n",
       " '8300']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('^[0-9]{4}$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10-day',\n",
       " '10-lap',\n",
       " '10-year',\n",
       " '100-share',\n",
       " '12-point',\n",
       " '12-year',\n",
       " '14-hour',\n",
       " '15-day',\n",
       " '150-point',\n",
       " '190-point',\n",
       " '20-point',\n",
       " '20-stock',\n",
       " '21-month',\n",
       " '237-seat',\n",
       " '240-page',\n",
       " '27-year',\n",
       " '30-day',\n",
       " '30-point',\n",
       " '30-share',\n",
       " '30-year',\n",
       " '300-day',\n",
       " '36-day',\n",
       " '36-store',\n",
       " '42-year',\n",
       " '50-state',\n",
       " '500-stock',\n",
       " '52-week',\n",
       " '69-point',\n",
       " '84-month',\n",
       " '87-store',\n",
       " '90-day']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['black-and-white',\n",
       " 'bread-and-butter',\n",
       " 'father-in-law',\n",
       " 'machine-gun-toting',\n",
       " 'savings-and-loan']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['62%-owned',\n",
       " 'Absorbed',\n",
       " 'According',\n",
       " 'Adopting',\n",
       " 'Advanced',\n",
       " 'Advancing',\n",
       " 'Alfred',\n",
       " 'Allied',\n",
       " 'Annualized',\n",
       " 'Anything',\n",
       " 'Arbitrage-related',\n",
       " 'Arbitraging',\n",
       " 'Asked',\n",
       " 'Assuming',\n",
       " 'Atlanta-based',\n",
       " 'Baking',\n",
       " 'Banking',\n",
       " 'Beginning',\n",
       " 'Beijing',\n",
       " 'Being',\n",
       " 'Bermuda-based',\n",
       " 'Betting',\n",
       " 'Boeing',\n",
       " 'Broadcasting',\n",
       " 'Bucking',\n",
       " 'Buying',\n",
       " 'Calif.-based',\n",
       " 'Change-ringing',\n",
       " 'Citing',\n",
       " 'Concerned',\n",
       " 'Confronted',\n",
       " 'Conn.based',\n",
       " 'Consolidated',\n",
       " 'Continued',\n",
       " 'Continuing',\n",
       " 'Declining',\n",
       " 'Defending',\n",
       " 'Depending',\n",
       " 'Designated',\n",
       " 'Determining',\n",
       " 'Developed',\n",
       " 'Died',\n",
       " 'During',\n",
       " 'Encouraged',\n",
       " 'Encouraging',\n",
       " 'English-speaking',\n",
       " 'Estimated',\n",
       " 'Everything',\n",
       " 'Excluding',\n",
       " 'Exxon-owned',\n",
       " 'Faulding',\n",
       " 'Fed',\n",
       " 'Feeding',\n",
       " 'Filling',\n",
       " 'Filmed',\n",
       " 'Financing',\n",
       " 'Following',\n",
       " 'Founded',\n",
       " 'Fracturing',\n",
       " 'Francisco-based',\n",
       " 'Fred',\n",
       " 'Funded',\n",
       " 'Funding',\n",
       " 'Generalized',\n",
       " 'Germany-based',\n",
       " 'Getting',\n",
       " 'Guaranteed',\n",
       " 'Having',\n",
       " 'Heating',\n",
       " 'Heightened',\n",
       " 'Holding',\n",
       " 'Housing',\n",
       " 'Illuminating',\n",
       " 'Indeed',\n",
       " 'Indexing',\n",
       " 'Irving',\n",
       " 'Jersey-based',\n",
       " 'Judging',\n",
       " 'Knowing',\n",
       " 'Learning',\n",
       " 'Legislating',\n",
       " 'Leming',\n",
       " 'Limited',\n",
       " 'London-based',\n",
       " 'Manfred',\n",
       " 'Manufacturing',\n",
       " 'Melamed',\n",
       " 'Miami-based',\n",
       " 'Mich.-based',\n",
       " 'Mining',\n",
       " 'Minneapolis-based',\n",
       " 'Mo.-based',\n",
       " 'Mortgage-Backed',\n",
       " 'Moving',\n",
       " 'Muzzling',\n",
       " 'N.J.-based',\n",
       " 'NBC-owned',\n",
       " 'NIH-appointed',\n",
       " 'Named',\n",
       " 'No-Smoking',\n",
       " 'Observing',\n",
       " 'Offering',\n",
       " 'Ohio-based',\n",
       " 'Orleans-based',\n",
       " 'Packaging',\n",
       " 'Performing',\n",
       " 'Philadelphia-based',\n",
       " 'Posted',\n",
       " 'Provided',\n",
       " 'Publishing',\n",
       " 'Purchasing',\n",
       " 'Rated',\n",
       " 'Reached',\n",
       " 'Red',\n",
       " 'Red-blooded',\n",
       " 'Reducing',\n",
       " 'Reed',\n",
       " 'Regarded',\n",
       " 'Rekindled',\n",
       " 'Related',\n",
       " 'Ringing',\n",
       " 'Rolling',\n",
       " 'Sacramento-based',\n",
       " 'Scoring',\n",
       " 'Seattle-based',\n",
       " 'Seed',\n",
       " 'Skilled',\n",
       " 'Smelting',\n",
       " 'Something',\n",
       " 'Spending',\n",
       " 'Standardized',\n",
       " 'Standing',\n",
       " 'Starting',\n",
       " 'Sterling',\n",
       " 'Taking',\n",
       " 'Texas-based',\n",
       " 'Toronto-based',\n",
       " 'Traded',\n",
       " 'Trading',\n",
       " 'Troubled',\n",
       " 'U.N.-supervised',\n",
       " 'U.S.-backed',\n",
       " 'United',\n",
       " 'Used',\n",
       " 'Varying',\n",
       " 'Washington-based',\n",
       " 'Whiting',\n",
       " 'Wilfred',\n",
       " 'Winning',\n",
       " 'Xiaoping',\n",
       " 'York-based',\n",
       " 'Zayed',\n",
       " 'abandoned',\n",
       " 'abating',\n",
       " 'abolishing',\n",
       " 'abortion-related',\n",
       " 'abounding',\n",
       " 'abridging',\n",
       " 'absorbed',\n",
       " 'acceded',\n",
       " 'accelerated',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'according',\n",
       " 'accounted',\n",
       " 'accounting',\n",
       " 'accrued',\n",
       " 'accumulated',\n",
       " 'accused',\n",
       " 'accusing',\n",
       " 'achieved',\n",
       " 'achieving',\n",
       " 'acknowledging',\n",
       " 'acquired',\n",
       " 'acquiring',\n",
       " 'acquisition-minded',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'adapted',\n",
       " 'adapting',\n",
       " 'added',\n",
       " 'adding',\n",
       " 'addressing',\n",
       " 'adjusted',\n",
       " 'adjusting',\n",
       " 'admitted',\n",
       " 'admitting',\n",
       " 'adopted',\n",
       " 'advanced',\n",
       " 'advancing',\n",
       " 'advertised',\n",
       " 'advertising',\n",
       " 'advised',\n",
       " 'advocated',\n",
       " 'advocating',\n",
       " 'affecting',\n",
       " 'afflicted',\n",
       " 'aggravated',\n",
       " 'agreed',\n",
       " 'agreeing',\n",
       " 'ailing',\n",
       " 'aimed',\n",
       " 'aiming',\n",
       " 'aired',\n",
       " 'airline-related',\n",
       " 'alarmed',\n",
       " 'alienated',\n",
       " 'alleged',\n",
       " 'alleging',\n",
       " 'allocated',\n",
       " 'allowed',\n",
       " 'altered',\n",
       " 'altering',\n",
       " 'amended',\n",
       " 'amending',\n",
       " 'amounted',\n",
       " 'amusing',\n",
       " 'angered',\n",
       " 'announced',\n",
       " 'annoyed',\n",
       " 'annualized',\n",
       " 'answered',\n",
       " 'anti-dumping',\n",
       " 'anticipated',\n",
       " 'anticipating',\n",
       " 'anything',\n",
       " 'apologizing',\n",
       " 'appealing',\n",
       " 'appeared',\n",
       " 'appearing',\n",
       " 'applied',\n",
       " 'appointed',\n",
       " 'approached',\n",
       " 'appropriated',\n",
       " 'approved',\n",
       " 'arched',\n",
       " 'argued',\n",
       " 'arguing',\n",
       " 'arising',\n",
       " 'armed',\n",
       " 'arranged',\n",
       " 'arrested',\n",
       " 'arrived',\n",
       " 'asbestos-related',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'assassinated',\n",
       " 'assembled',\n",
       " 'asserted',\n",
       " 'asserting',\n",
       " 'assessed',\n",
       " 'assigned',\n",
       " 'assisted',\n",
       " 'associated',\n",
       " 'assumed',\n",
       " 'assuming',\n",
       " 'assured',\n",
       " 'attached',\n",
       " 'attacking',\n",
       " 'attempted',\n",
       " 'attempting',\n",
       " 'attended',\n",
       " 'attending',\n",
       " 'attracted',\n",
       " 'attracting',\n",
       " 'attributed',\n",
       " 'auctioned',\n",
       " 'authorized',\n",
       " 'authorizing',\n",
       " 'automated',\n",
       " 'automotive-lighting',\n",
       " 'averaged',\n",
       " 'averted',\n",
       " 'avoiding',\n",
       " 'awarded',\n",
       " 'awarding',\n",
       " 'backed',\n",
       " 'backing',\n",
       " 'balanced',\n",
       " 'bald-faced',\n",
       " 'balkanized',\n",
       " 'balked',\n",
       " 'balloting',\n",
       " 'bank-backed',\n",
       " 'banking',\n",
       " 'banned',\n",
       " 'banning',\n",
       " 'barking',\n",
       " 'barred',\n",
       " 'based',\n",
       " 'battered',\n",
       " 'battery-operated',\n",
       " 'batting',\n",
       " 'bearing',\n",
       " 'becoming',\n",
       " 'bedding',\n",
       " 'befuddled',\n",
       " 'beginning',\n",
       " 'behaving',\n",
       " 'beheading',\n",
       " 'being',\n",
       " 'beleaguered',\n",
       " 'believed',\n",
       " 'bell-ringing',\n",
       " 'belonging',\n",
       " 'benefited',\n",
       " 'best-selling',\n",
       " 'betting',\n",
       " 'bickering',\n",
       " 'bidding',\n",
       " 'billed',\n",
       " 'billing',\n",
       " 'blamed',\n",
       " 'bled',\n",
       " 'blessing',\n",
       " 'blighted',\n",
       " 'blocked',\n",
       " 'blurred',\n",
       " 'boarding',\n",
       " 'bolstered',\n",
       " 'bombarding',\n",
       " 'booked',\n",
       " 'booming',\n",
       " 'boosted',\n",
       " 'boosting',\n",
       " 'borrowed',\n",
       " 'borrowing',\n",
       " 'botched',\n",
       " 'bothered',\n",
       " 'bounced',\n",
       " 'bowed',\n",
       " 'breaking',\n",
       " 'breathed',\n",
       " 'breathtaking',\n",
       " 'breed',\n",
       " 'bribed',\n",
       " 'bribing',\n",
       " 'briefing',\n",
       " 'brightened',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'broad-based',\n",
       " 'broadcasting',\n",
       " 'broadened',\n",
       " 'brokering',\n",
       " 'brushed',\n",
       " 'budding',\n",
       " 'building',\n",
       " 'bundling',\n",
       " 'buoyed',\n",
       " 'burned',\n",
       " 'buying',\n",
       " 'calculated',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'campaigning',\n",
       " 'cancer-causing',\n",
       " 'capitalized',\n",
       " 'capped',\n",
       " 'captivating',\n",
       " 'cared',\n",
       " 'carried',\n",
       " 'carrying',\n",
       " 'cascading',\n",
       " 'casting',\n",
       " 'caused',\n",
       " 'causing',\n",
       " 'cautioned',\n",
       " 'ceiling',\n",
       " 'centralized',\n",
       " 'certified',\n",
       " 'chaired',\n",
       " 'challenging',\n",
       " 'championing',\n",
       " 'change-ringing',\n",
       " 'changed',\n",
       " 'changing',\n",
       " 'characterized',\n",
       " 'characterizing',\n",
       " 'charged',\n",
       " 'charging',\n",
       " 'chastised',\n",
       " 'cheating',\n",
       " 'checking',\n",
       " 'cheerleading',\n",
       " 'chilled',\n",
       " 'choosing',\n",
       " 'chopped',\n",
       " 'circulated',\n",
       " 'cited',\n",
       " 'citing',\n",
       " 'citizen-sparked',\n",
       " 'city-owned',\n",
       " 'claimed',\n",
       " 'claiming',\n",
       " 'clamped',\n",
       " 'clarified',\n",
       " 'clashed',\n",
       " 'classed',\n",
       " 'classified',\n",
       " 'cleaned',\n",
       " 'cleaner-burning',\n",
       " 'cleared',\n",
       " 'clearing',\n",
       " 'clicked',\n",
       " 'climbed',\n",
       " 'climbing',\n",
       " 'clipped',\n",
       " 'clobbered',\n",
       " 'closed',\n",
       " 'closing',\n",
       " 'clothing',\n",
       " 'clouding',\n",
       " 'cluttered',\n",
       " 'co-founded',\n",
       " 'coaching',\n",
       " 'coal-fired',\n",
       " 'coated',\n",
       " 'codified',\n",
       " 'collaborated',\n",
       " 'collapsed',\n",
       " 'collected',\n",
       " 'collecting',\n",
       " 'collective-bargaining',\n",
       " 'colored',\n",
       " 'combined',\n",
       " 'coming',\n",
       " 'commanded',\n",
       " 'commenting',\n",
       " 'committed',\n",
       " 'committing',\n",
       " 'compared',\n",
       " 'compelling',\n",
       " 'competed',\n",
       " 'competing',\n",
       " 'compiled',\n",
       " 'complained',\n",
       " 'complaining',\n",
       " 'completed',\n",
       " 'completing',\n",
       " 'complicated',\n",
       " 'composed',\n",
       " 'composting',\n",
       " 'compressed',\n",
       " 'computer-aided',\n",
       " 'computer-assisted',\n",
       " 'computer-generated',\n",
       " 'computerized',\n",
       " 'computing',\n",
       " 'conceding',\n",
       " 'concentrated',\n",
       " 'concentrating',\n",
       " 'concerned',\n",
       " 'concluded',\n",
       " 'condemned',\n",
       " 'condemning',\n",
       " 'conducted',\n",
       " 'conducting',\n",
       " 'confined',\n",
       " 'confirmed',\n",
       " 'confused',\n",
       " 'connected',\n",
       " 'consented',\n",
       " 'considered',\n",
       " 'considering',\n",
       " 'consisting',\n",
       " 'construed',\n",
       " 'consulting',\n",
       " 'contacted',\n",
       " 'contained',\n",
       " 'containing',\n",
       " 'contesting',\n",
       " 'continued',\n",
       " 'continuing',\n",
       " 'contracted',\n",
       " 'contributed',\n",
       " 'contributing',\n",
       " 'controlled',\n",
       " 'controlling',\n",
       " 'converted',\n",
       " 'converting',\n",
       " 'convicted',\n",
       " 'convinced',\n",
       " 'cooled',\n",
       " 'cooperating',\n",
       " 'copied',\n",
       " 'copying',\n",
       " 'corn-buying',\n",
       " 'corrected',\n",
       " 'correcting',\n",
       " 'cost-cutting',\n",
       " 'cost-sharing',\n",
       " 'counseling',\n",
       " 'counting',\n",
       " 'coupled',\n",
       " 'court-ordered',\n",
       " 'covered',\n",
       " 'covering',\n",
       " 'cranked',\n",
       " 'crashing',\n",
       " 'created',\n",
       " 'creating',\n",
       " 'credit-rating',\n",
       " 'crippled',\n",
       " 'criticized',\n",
       " 'crossed',\n",
       " 'crossing',\n",
       " 'crowded',\n",
       " 'cruising',\n",
       " 'crushed',\n",
       " 'crying',\n",
       " 'cultivated',\n",
       " 'curbed',\n",
       " 'curbing',\n",
       " 'curled',\n",
       " 'current-carrying',\n",
       " 'curtailed',\n",
       " 'cushioned',\n",
       " 'customized',\n",
       " 'cutting',\n",
       " 'damaged',\n",
       " 'damaging',\n",
       " 'dancing',\n",
       " 'darned',\n",
       " 'dashed',\n",
       " 'dating',\n",
       " 'dead-eyed',\n",
       " 'dealing',\n",
       " 'decided',\n",
       " 'declared',\n",
       " 'declaring',\n",
       " 'declined',\n",
       " 'declining',\n",
       " 'decorated',\n",
       " 'decried',\n",
       " 'deducting',\n",
       " 'deemed',\n",
       " 'defeated',\n",
       " 'defended',\n",
       " 'defined',\n",
       " 'defying',\n",
       " 'delayed',\n",
       " 'deliberating',\n",
       " 'delisted',\n",
       " 'delivered',\n",
       " 'delivering',\n",
       " 'demanding',\n",
       " 'demonstrating',\n",
       " 'denied',\n",
       " 'denouncing',\n",
       " 'denying',\n",
       " 'depended',\n",
       " 'depending',\n",
       " 'depleted',\n",
       " 'depressed',\n",
       " 'deprived',\n",
       " 'derived',\n",
       " 'descending',\n",
       " 'described',\n",
       " 'deserving',\n",
       " 'designated',\n",
       " 'designed',\n",
       " 'designing',\n",
       " 'desired',\n",
       " 'despised',\n",
       " 'detailed',\n",
       " 'deteriorated',\n",
       " 'deteriorating',\n",
       " 'determined',\n",
       " 'deterring',\n",
       " 'devastating',\n",
       " 'developed',\n",
       " 'developing',\n",
       " 'devised',\n",
       " 'devoted',\n",
       " 'devouring',\n",
       " 'diagnosed',\n",
       " 'died',\n",
       " 'diluted',\n",
       " 'diming',\n",
       " 'diminished',\n",
       " 'directed',\n",
       " 'directing',\n",
       " 'disaffected',\n",
       " 'disagreed',\n",
       " 'disappointed',\n",
       " 'disappointing',\n",
       " 'disapproved',\n",
       " 'discarded',\n",
       " 'disciplined',\n",
       " 'disclosed',\n",
       " 'disclosing',\n",
       " 'discontinued',\n",
       " 'discontinuing',\n",
       " 'discouraging',\n",
       " 'discovered',\n",
       " 'discussed',\n",
       " 'discussing',\n",
       " 'disembodied',\n",
       " 'dismayed',\n",
       " 'dismissed',\n",
       " 'disposed',\n",
       " 'disputed',\n",
       " 'disseminating',\n",
       " 'distinguished',\n",
       " 'distorted',\n",
       " 'distributed',\n",
       " 'disturbing',\n",
       " 'diversified',\n",
       " 'diversifying',\n",
       " 'divided',\n",
       " 'dividing',\n",
       " 'documented',\n",
       " 'doing',\n",
       " 'doling',\n",
       " 'dollar-denominated',\n",
       " 'dominated',\n",
       " 'dominating',\n",
       " 'doubled',\n",
       " 'doubted',\n",
       " 'downgraded',\n",
       " 'downgrading',\n",
       " 'drafted',\n",
       " 'drawing',\n",
       " 'dreamed',\n",
       " 'dressed',\n",
       " 'drifted',\n",
       " 'drinking',\n",
       " 'driving',\n",
       " 'drooled',\n",
       " 'dropped',\n",
       " 'dubbed',\n",
       " 'duckling',\n",
       " 'dumbfounded',\n",
       " 'dumped',\n",
       " 'during',\n",
       " 'dwindling',\n",
       " 'earned',\n",
       " 'earning',\n",
       " 'eased',\n",
       " 'easing',\n",
       " 'eating',\n",
       " 'echoed',\n",
       " 'edged',\n",
       " 'editing',\n",
       " 'educated',\n",
       " 'elected',\n",
       " 'eliminated',\n",
       " 'eliminating',\n",
       " 'embarrassing',\n",
       " 'embroiled',\n",
       " 'emerged',\n",
       " 'emerging',\n",
       " 'emphasized',\n",
       " 'employed',\n",
       " 'empowered',\n",
       " 'enabled',\n",
       " 'enabling',\n",
       " 'enacted',\n",
       " 'encircling',\n",
       " 'enclosed',\n",
       " 'encouraging',\n",
       " 'encroaching',\n",
       " 'ended',\n",
       " 'ending',\n",
       " 'endorsed',\n",
       " 'engaged',\n",
       " 'engaging',\n",
       " 'engineered',\n",
       " 'engineering',\n",
       " 'enhanced',\n",
       " 'enjoyed',\n",
       " 'enjoying',\n",
       " 'enlarged',\n",
       " 'enraged',\n",
       " 'ensnarled',\n",
       " 'entangled',\n",
       " 'entered',\n",
       " 'entering',\n",
       " 'entertaining',\n",
       " 'enticed',\n",
       " 'entitled',\n",
       " 'entrenched',\n",
       " 'entrusted',\n",
       " 'equaling',\n",
       " 'equipped',\n",
       " 'escalated',\n",
       " 'escaped',\n",
       " 'established',\n",
       " 'establishing',\n",
       " 'estimated',\n",
       " 'evaluated',\n",
       " 'evaluating',\n",
       " 'evaporated',\n",
       " 'evening',\n",
       " 'everything',\n",
       " 'evoking',\n",
       " 'evolved',\n",
       " 'exacerbated',\n",
       " 'examined',\n",
       " 'exceed',\n",
       " 'exceeded',\n",
       " 'exceeding',\n",
       " 'exchanging',\n",
       " 'excited',\n",
       " 'exciting',\n",
       " 'executed',\n",
       " 'executing',\n",
       " 'exercised',\n",
       " 'exerting',\n",
       " 'exhausted',\n",
       " 'exhibited',\n",
       " 'existed',\n",
       " 'existing',\n",
       " 'expanded',\n",
       " 'expanding',\n",
       " 'expected',\n",
       " 'expecting',\n",
       " 'expedited',\n",
       " 'expelled',\n",
       " 'experienced',\n",
       " 'experiencing',\n",
       " 'expired',\n",
       " 'explained',\n",
       " 'explaining',\n",
       " 'exploded',\n",
       " 'export-oriented',\n",
       " 'exposed',\n",
       " 'expressed',\n",
       " 'expressing',\n",
       " 'expunged',\n",
       " 'extended',\n",
       " 'extending',\n",
       " 'exuded',\n",
       " 'eyeing',\n",
       " 'fabled',\n",
       " 'faced',\n",
       " 'facing',\n",
       " 'factoring',\n",
       " 'faded',\n",
       " 'failed',\n",
       " 'failing',\n",
       " 'fainting',\n",
       " 'falling',\n",
       " 'faltered',\n",
       " 'famed',\n",
       " 'family-planning',\n",
       " 'fared',\n",
       " 'fashioned',\n",
       " 'fast-growing',\n",
       " 'fastest-growing',\n",
       " 'fattened',\n",
       " 'favored',\n",
       " 'fawning',\n",
       " 'feared',\n",
       " 'featured',\n",
       " 'featuring',\n",
       " 'fed',\n",
       " 'feed',\n",
       " 'feeling',\n",
       " 'fetching',\n",
       " 'fielded',\n",
       " 'fighting',\n",
       " 'filed',\n",
       " 'filing',\n",
       " 'filled',\n",
       " 'filling',\n",
       " 'finalized',\n",
       " 'financed',\n",
       " 'financing',\n",
       " 'finding',\n",
       " 'fined',\n",
       " 'finished',\n",
       " 'fired',\n",
       " 'firmed',\n",
       " 'fixed',\n",
       " 'fizzled',\n",
       " 'fled',\n",
       " 'fledgling',\n",
       " 'fleeting',\n",
       " 'flirted',\n",
       " 'floated',\n",
       " 'flooded',\n",
       " 'focused',\n",
       " 'focusing',\n",
       " 'folded',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'forced',\n",
       " 'forcing',\n",
       " 'forecasting',\n",
       " 'foreign-led',\n",
       " 'formed',\n",
       " 'forthcoming',\n",
       " 'founded',\n",
       " 'foundering',\n",
       " 'fretted',\n",
       " 'frightened',\n",
       " 'frustrating',\n",
       " 'fueled',\n",
       " 'fueling',\n",
       " 'full-fledged',\n",
       " 'fuming',\n",
       " 'functioning',\n",
       " 'funded',\n",
       " 'funding',\n",
       " 'fundraising',\n",
       " 'futures-related',\n",
       " 'gained',\n",
       " 'gaining',\n",
       " 'galling',\n",
       " 'galvanized',\n",
       " 'gambling',\n",
       " 'gauging',\n",
       " 'generated',\n",
       " 'getting',\n",
       " 'giving',\n",
       " 'going',\n",
       " 'good-hearted',\n",
       " 'good-natured',\n",
       " 'gored',\n",
       " 'government-certified',\n",
       " 'government-funded',\n",
       " 'government-owned',\n",
       " 'graduated',\n",
       " 'granted',\n",
       " 'granting',\n",
       " 'greed',\n",
       " 'gripping',\n",
       " 'growing',\n",
       " 'guaranteed',\n",
       " 'guarding',\n",
       " 'guided',\n",
       " 'gut-wrenching',\n",
       " 'hailed',\n",
       " 'hailing',\n",
       " 'halted',\n",
       " 'hampered',\n",
       " 'handed',\n",
       " 'handled',\n",
       " 'handling',\n",
       " 'happened',\n",
       " 'happening',\n",
       " 'hard-charging',\n",
       " 'hard-drinking',\n",
       " 'hard-hitting',\n",
       " 'harmed',\n",
       " 'harped',\n",
       " 'harvested',\n",
       " 'hauled',\n",
       " 'hauling',\n",
       " 'having',\n",
       " 'headed',\n",
       " 'heading',\n",
       " 'headlined',\n",
       " 'healing',\n",
       " 'hearing',\n",
       " 'heated',\n",
       " 'heating',\n",
       " 'hedging',\n",
       " 'heightened',\n",
       " 'helped',\n",
       " 'helping',\n",
       " 'high-flying',\n",
       " 'high-minded',\n",
       " 'high-polluting',\n",
       " 'high-priced',\n",
       " 'high-rolling',\n",
       " 'high-speed',\n",
       " 'higher-salaried',\n",
       " 'highest-pitched',\n",
       " 'hired',\n",
       " 'hitting',\n",
       " 'holding',\n",
       " 'hoped',\n",
       " 'hosted',\n",
       " 'housing',\n",
       " 'hugging',\n",
       " 'hundred',\n",
       " 'hunted',\n",
       " 'hurting',\n",
       " 'identified',\n",
       " 'ignored',\n",
       " 'ignoring',\n",
       " 'impaired',\n",
       " 'impeding',\n",
       " 'impending',\n",
       " 'implemented',\n",
       " 'implied',\n",
       " 'imported',\n",
       " 'imposed',\n",
       " 'imposing',\n",
       " 'impressed',\n",
       " 'improved',\n",
       " 'improving',\n",
       " 'incentive-backed',\n",
       " 'inched',\n",
       " 'inching',\n",
       " 'included',\n",
       " 'including',\n",
       " 'incorporated',\n",
       " 'increased',\n",
       " 'increasing',\n",
       " 'incurred',\n",
       " 'indeed',\n",
       " 'index-related',\n",
       " 'indicated',\n",
       " 'indicating',\n",
       " 'indulging',\n",
       " 'industrialized',\n",
       " 'industry-supported',\n",
       " 'inflated',\n",
       " 'influenced',\n",
       " 'influencing',\n",
       " 'infringed',\n",
       " 'inherited',\n",
       " 'initialing',\n",
       " 'initiated',\n",
       " 'initiating',\n",
       " 'injecting',\n",
       " 'injuring',\n",
       " 'inkling',\n",
       " 'inquiring',\n",
       " 'inserted',\n",
       " 'insider-trading',\n",
       " 'insinuating',\n",
       " 'insisted',\n",
       " 'inspired',\n",
       " 'installed',\n",
       " 'installing',\n",
       " 'instituted',\n",
       " 'instructed',\n",
       " 'insured',\n",
       " 'integrated',\n",
       " 'intended',\n",
       " 'intentioned',\n",
       " 'interest-bearing',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'interrogated',\n",
       " 'interviewed',\n",
       " 'intriguing',\n",
       " 'introduced',\n",
       " 'introducing',\n",
       " 'invented',\n",
       " 'inverted',\n",
       " 'invested',\n",
       " 'investigating',\n",
       " 'investing',\n",
       " 'inviting',\n",
       " 'involved',\n",
       " 'involving',\n",
       " 'issued',\n",
       " 'issuing',\n",
       " 'jeopardizing',\n",
       " 'joined',\n",
       " 'joining',\n",
       " 'judged',\n",
       " 'jumped',\n",
       " 'jumping',\n",
       " 'justified',\n",
       " 'justifying',\n",
       " 'keeping',\n",
       " 'kicked',\n",
       " 'kidnapping',\n",
       " 'killed',\n",
       " 'killing',\n",
       " 'knitted',\n",
       " 'knocked',\n",
       " 'labeled',\n",
       " 'labeling',\n",
       " 'labor-backed',\n",
       " 'lacked',\n",
       " 'lagging',\n",
       " 'land-idling',\n",
       " 'landing',\n",
       " 'lasted',\n",
       " 'lasting',\n",
       " 'lauded',\n",
       " 'laughing',\n",
       " 'launched',\n",
       " 'lawmaking',\n",
       " 'laying',\n",
       " 'leading',\n",
       " 'learned',\n",
       " 'learning',\n",
       " 'leasing',\n",
       " 'leaving',\n",
       " 'led',\n",
       " 'lending',\n",
       " 'lengthened',\n",
       " 'lessening',\n",
       " 'letter-writing',\n",
       " 'letting',\n",
       " 'leveling',\n",
       " 'leveraged',\n",
       " 'leveraging',\n",
       " 'licensed',\n",
       " 'licensing',\n",
       " 'lifted',\n",
       " ...]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('(ed|ing)$', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:crimson\">[ì—°ìŠµğŸ˜‰3]</span> ê³„ì† ì½ê¸° ì „ì— ìœ„ì— ë‚˜ì—´ëœ ê¸°í˜¸ë“¤ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ì."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/re_cheatsheet1.png\" width=\"600\" style=\"margin-left: auto; margin-right: auto\">\n",
    "<p style=\"text-align: center;\">ì™€ì¼ë“œì¹´ë“œ, ë²”ìœ„ ë° í´ë¡œì €ë¥¼ í¬í•¨í•œ ê¸°ë³¸ ì •ê·œ í‘œí˜„ì‹ ë©”íƒ€ ë¬¸ì</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) ì •ê·œí‘œí˜„ì‹ì˜ ì‘ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Pieces ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u',\n",
       " 'e',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'e',\n",
       " 'i',\n",
       " 'a',\n",
       " 'i',\n",
       " 'o',\n",
       " 'i',\n",
       " 'o',\n",
       " 'u']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'supercalifragilisticexpialidocious'\n",
    "re.findall(r'[aeiou]', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(re.findall(r'[aeiou]', word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('io', 549),\n",
       " ('ea', 476),\n",
       " ('ie', 331),\n",
       " ('ou', 329),\n",
       " ('ai', 261),\n",
       " ('ia', 253),\n",
       " ('ee', 217),\n",
       " ('oo', 174),\n",
       " ('ua', 109),\n",
       " ('au', 106),\n",
       " ('ue', 105),\n",
       " ('ui', 95)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "fd = nltk.FreqDist(vs for word in wsj\n",
    "                      for vs in re.findall(r'[aeiou]{2,}', word))\n",
    "fd.most_common(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:crimson\">[ì—°ìŠµğŸ˜‰4]</span>** W3C Date Time Formatì—ì„œëŠ” ë‚ ì§œê°€ 2022-04-05 ì²˜ëŸ¼ í‘œí˜„ëœë‹¤. ì•„ë˜ íŒŒì´ì¬ ì½”ë“œì—ì„œ `?`ë¥¼ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ë°”ê¿”ì„œ string '2022-04-05'ë¥¼ ì •ìˆ˜ ë¦¬ìŠ¤íŠ¸ [2022, 4, 5] ê°€ ë˜ê²Œ í•˜ë¼.\n",
    "```\\\n",
    "[int(n) for n in re.findall(?, '2022-04-05')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Piecesë¥¼ ì‚¬ìš©í•œ ì¶”ê°€ ì‘ì—…\n",
    "- ì˜ì–´ í…ìŠ¤íŠ¸ëŠ” ì˜ˆì¸¡ê°€ëŠ¥ì„±ì´ ë†’ì•„ì„œ ë‹¨ì–´ ì¤‘ê°„ì˜ ëª¨ìŒë“¤ì„ ìƒëµí•´ë„ ì½ê¸°ê°€ ìš©ì´í•œ íŠ¹ì§•ì´ ìˆë‹¤\n",
    "- declaration -> dclrtn, inalienable -> inlnble\n",
    "- ì²« ëª¨ìŒ ë˜ëŠ” ë ëª¨ìŒì€ ê·¸ëŒ€ë¡œ ë‘”ë‹¤\n",
    "- ì²« ëª¨ìŒ ìˆœì„œ, ë ëª¨ìŒ ìˆœì„œ, ê·¸ ë‹¤ìŒì—ëŠ” ëª¨ë“  ììŒ ìˆœì„œê°€ ë§ëŠ” ê²ƒì„ ì°¾ëŠ”ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(word):\n",
    "    pieces = re.findall(regexp, word)\n",
    "    return ''.join(pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and\n",
      "of the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn\n",
      "of frdm , jstce and pce in the wrld , Whrs dsrgrd and cntmpt fr hmn\n",
      "rghts hve rsltd in brbrs acts whch hve outrgd the cnscnce of mnknd ,\n",
      "and the advnt of a wrld in whch hmn bngs shll enjy frdm of spch and\n"
     ]
    }
   ],
   "source": [
    "english_udhr = nltk.corpus.udhr.words('English-Latin1')\n",
    "print(nltk.tokenwrap(compress(w) for w in english_udhr[:75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìŒì€ ì •ê·œí‘œí˜„ê³¼ conditional frequency distributionì„ ê²°í•©í•œë‹¤. Rotokasì–´ì—ì„œ kaë‚˜ si ê°™ì€ ëª¨ë“  ììŒ-ëª¨ìŒ sequenceë¥¼ ì¶”ì¶œí•œë‹¤. ì´ê²ƒë“¤ ëª¨ë‘ê°€ ìŒì´ë¯€ë¡œ, conditional frequency distributionì„ ì´ˆê¸°í™”í•˜ëŠ”ë° ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ê° ìŒì˜ ë¹ˆë„ë¥¼ í‘œ í˜•íƒœë¡œ ì¶œë ¥í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a   e   i   o   u \n",
      "k 418 148  94 420 173 \n",
      "p  83  31 105  34  51 \n",
      "r 187  63  84  89  79 \n",
      "s   0   0 100   2   1 \n",
      "t  47   8   0 148  37 \n",
      "v  93  27 105  48  49 \n"
     ]
    }
   ],
   "source": [
    "rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')\n",
    "cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
    "cfd = nltk.ConditionalFreqDist(cvs)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 's'ì™€ 't' í–‰ì„ ë³´ë©´ ì´ë“¤ì´ ë¶€ë¶„ì ìœ¼ë¡œ \"ìƒë³´ì  ë¶„í¬\"ë¥¼ ê°–ëŠ”ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n",
    "- ì´ë“¤ì€ ì–¸ì–´ìƒ ìƒì´í•œ ìŒì†Œê°€ ì•„ë‹ˆë¼ëŠ” ì¦ê±°ì´ë‹¤.\n",
    "- ë”°ë¼ì„œ Rotokas ì•ŒíŒŒë²³ì—ì„œ së¥¼ ë¹¼ë²„ë¦¬ê³  t ë‹¤ìŒì— i ê°€ ë‚˜ì˜¤ë©´ të¥¼ së¡œ ë°œìŒí•˜ë„ë¡ í•˜ëŠ” ë°œìŒê·œì¹™ì„ ë§Œë“¤ìˆ˜ë„ ìˆì„ ê²ƒì´ë‹¤.\n",
    "- 'su'ëŠ” ë¹ˆë„ê°€ 1ì¸ë° ì´ê²ƒì€ *kasuari*ë¡œ ì˜ì–´ 'cassowary'ì—ì„œ ê°€ì ¸ì˜¨ ê²ƒì´ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¶”ê°€ì ì¸ ë¶„ì„ì„ ì›í•œë‹¤ë©´ ì£¼ì–´ì§„ consonant-vowel pairë¥¼ í¬í•¨í•˜ëŠ” ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¹ ë¥´ê²Œ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ì¸ë±ìŠ¤ë¥¼ ë§Œë“œëŠ” ê²ƒì´ í¸ë¦¬í•  ê²ƒì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kasuari']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_word_pairs = [(cv, w) for w in rotokas_words\n",
    "                         for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
    "cv_index = nltk.Index(cv_word_pairs)\n",
    "cv_index['su']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kaapo',\n",
       " 'kaapopato',\n",
       " 'kaipori',\n",
       " 'kaiporipie',\n",
       " 'kaiporivira',\n",
       " 'kapo',\n",
       " 'kapoa',\n",
       " 'kapokao',\n",
       " 'kapokapo',\n",
       " 'kapokapo',\n",
       " 'kapokapoa',\n",
       " 'kapokapoa',\n",
       " 'kapokapora',\n",
       " 'kapokapora',\n",
       " 'kapokaporo',\n",
       " 'kapokaporo',\n",
       " 'kapokari',\n",
       " 'kapokarito',\n",
       " 'kapokoa',\n",
       " 'kapoo',\n",
       " 'kapooto',\n",
       " 'kapoovira',\n",
       " 'kapopaa',\n",
       " 'kaporo',\n",
       " 'kaporo',\n",
       " 'kaporopa',\n",
       " 'kaporoto',\n",
       " 'kapoto',\n",
       " 'karokaropo',\n",
       " 'karopo',\n",
       " 'kepo',\n",
       " 'kepoi',\n",
       " 'keposi',\n",
       " 'kepoto']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_index['po']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë‹¨ì–´ ì–´ê°„ ì°¾ê¸°\n",
    "- ê²€ìƒ‰ ì—”ì§„ì— ì…ë ¥í•  ë•Œ ë‹¨ì–´ì˜ ëë¶€ë¶„ì´ ë‹¤ë¥¸ ê²ƒì„ ì‹ ê²½ì“°ì§€ ì•Šì•„ë„ ë˜ëŠ” ê²ƒì€ stemming ê¸°ëŠ¥ ë•Œë¬¸ì´ë‹¤.\n",
    "- ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ìœ¼ë¡œ suffixì²˜ëŸ¼ ë³´ì´ëŠ” ê²ƒì€ ë‹¤ ì˜ë¼ë²„ë¦¬ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•´ë³´ì.\n",
    "- ë¬¼ë¡  NLTKì—ëŠ” stemmerê°€ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- disjunction('OR'ing) of all suffixes\n",
    "- disjunctionì˜ ë²”ìœ„ë¥¼ í•œì •í•˜ê¸° ìœ„í•´ ê´„í˜¸ë¥¼ ì“´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ing']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- matchingì´ ëœ ë‹¨ì–´ 'processing'ì„ ë°˜í™˜í•˜ì§€ ì•Šê³  suffixë¥¼ ë°˜í™˜í•˜ëŠ” ì´ìœ ëŠ” ê´„í˜¸ () ë•Œë¬¸\n",
    "- ë‹¨ì–´ë¥¼ ì¶œë ¥í•˜ë ¤ë©´ `?:` ë¥¼ ì¶”ê°€í•´ì•¼ í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['processing']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stemê³¼ suffix ë¶„ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('process', 'ing')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ì¢‹ì•„ ë³´ì´ì§€ë§Œ ì•„ì§ ë¬¸ì œê°€ ìˆë‹¤.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('processe', 's')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'es' ëŒ€ì‹  's'ë¥¼ ì°¾ì•˜ë‹¤.\n",
    "- ì²«ë²ˆì§¸ ì—°ì‚°ìê°€ 'greedy'í•˜ì—¬ '.\\*' ë¶€ë¶„ì´ ê°€ëŠ¥í•œ ë§ì€ ì…ë ¥ì„ ì†Œë¹„í•˜ë ¤í•˜ê¸° ë•Œë¬¸\n",
    "- `*` ì—°ì‚°ìì˜ 'non-greedy' ë²„ì „ `*?`ì„ ì‚¬ìš©í•˜ì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('process', 'es')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‘ ë²ˆì§¸ ê´„í˜¸ë¥¼ optionìœ¼ë¡œ ë§Œë“¤ë©´ empty suffixì—ì„œë„ ì˜ ì‘ë™í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('language', '')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', 'language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì•„ì§ë„ ë¬¸ì œê°€ ë§ì§€ë§Œ ê·¸ëŒ€ë¡œ ì§„í–‰í•˜ì—¬, ì „ì²´ ë¬¸ì„œì— ì ìš©í•  stemmer í•¨ìˆ˜ë¥¼ ë§Œë“¤ì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DENNIS',\n",
       " ':',\n",
       " 'Listen',\n",
       " ',',\n",
       " 'strange',\n",
       " 'women',\n",
       " 'ly',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'i',\n",
       " 'no',\n",
       " 'basi',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'Supreme',\n",
       " 'execut',\n",
       " 'power',\n",
       " 'deriv',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mandate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcical',\n",
       " 'aquatic',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = word_tokenize(raw)\n",
    "[stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *ponds*ì—ì„œ *s*ë¥¼ ì œê±°í–ˆìœ¼ë‚˜, *is*ì™€ *basis*ì—ì„œë„ ì œê±°í–ˆë‹¤.\n",
    "- *distribut*ë‚˜ *deriv* ê°™ì€ ì´ìƒí•œ ë‹¨ì–´ë¥¼ ë§Œë“¤ì—ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í† í°í™”ëœ í…ìŠ¤íŠ¸ ê²€ìƒ‰\n",
    "- `< >`: token boundaryë¥¼ ë‚˜íƒ€ë‚´ë©° ê·¸ ì‚¬ì´ì˜ ê³µë°±ì€ ë¬´ì‹œí•œë‹¤.\n",
    "- `(<.*>)`: single token, ()ëŠ” ì¼ì¹˜í•˜ëŠ” ë‹¨ì–´(phrase ì•„ë‹ˆê³ )ë§Œ ì°¾ëŠ”ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      "mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      "pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      "brave; brave; brave\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg, nps_chat\n",
    "moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))\n",
    "moby.findall(r\"<a> (<.*>) <man>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      "mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      "pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      "brave; brave; brave\n"
     ]
    }
   ],
   "source": [
    "moby.findall(r'<a> (<.*>) <man>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = nltk.Text(nps_chat.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you rule bro; telling you bro; u twizted bro\n"
     ]
    }
   ],
   "source": [
    "chat.findall(r\"<.*> <.*> <bro>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la\n",
      "la la; lovely lol lol love; lol lol lol.; la la la; la la la\n"
     ]
    }
   ],
   "source": [
    "chat.findall(r\"<l.*>{3,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:crimson\">[ì—°ìŠµğŸ˜‰5]</span>** íŒ¨í„´ pì™€ ì¼ì¹˜í•˜ëŠ” ëª¨ë“  ìœ„ì¹˜ë¥¼ í‘œì‹œí•˜ê¸° ìœ„í•´ ë¬¸ìì—´ sì— ì£¼ì„ì„ ì¶”ê°€í•˜ëŠ” `nltk.re_show(p, s)`ì™€ ì •ê·œì‹ íƒìƒ‰ì„ ìœ„í•œ ê·¸ë˜í”½ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” nltk.app.nemo()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ê·œí‘œí˜„ì‹ì˜ íŒ¨í„´ê³¼ ëŒ€ì…ì— ëŒ€í•œ ì´í•´ë¥¼ í†µí•©í•˜ì‹œì˜¤. ë” ë§ì€ ì—°ìŠµì„ ìœ„í•´ ì´ ì¥ì˜ ëì— ìˆëŠ” ì •ê·œí‘œí˜„ì‹ ëª‡ ê°€ì§€ë¥¼ ì—°ìŠµí•´ ë³´ì."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ì¡°ì‚¬í•˜ë ¤ëŠ” ì–¸ì–´í•™ì  í˜„ìƒì´ íŠ¹ì • ë‹¨ì–´ë“¤ì— ì—®ì—¬ ìˆìœ¼ë©´ ê²€ìƒ‰ íŒ¨í„´ì„ ë§Œë“¤ê¸° ì‰¬ì›Œì§„ë‹¤.\n",
    "- \"*x and other ys*\"ë¼ëŠ” í‘œí˜„ì€ ëŒ€ê·œëª¨ ì½”í¼ìŠ¤ì—ì„œ hypernymì„ ì°¾ê¸° ì‰½ê²Œ í•´ì¤€ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed and other activities; water and other liquids; tomb and other\n",
      "landmarks; Statues and other monuments; pearls and other jewels;\n",
      "charts and other items; roads and other features; figures and other\n",
      "objects; military and other areas; demands and other factors;\n",
      "abstracts and other compilations; iron and other metals\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))\n",
    "hobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ê·¸ëŸ¬ë‚˜ í•­ìƒ false positiveê°€ ìˆìŒì„ ëª…ì‹¬í•˜ì. \n",
    "- í•­ìƒ false negative ë¬¸ì œë„ ìˆê¸° ë§ˆë ¨ì¸ë°, ê²€ìƒ‰ íŒ¨í„´ì— ì¼ì¹˜í•˜ëŠ” ê²ƒì´ ì—†ë‹¤ê³  ì–´ë–¤ ì–¸ì–´í•™ì  í˜„ìƒì´ ì½”í¼ìŠ¤ì— ì—†ë‹¤ê³  ê²°ë¡ ì„ ë‚´ë¦¬ëŠ” ê²ƒì€ ìœ„í—˜í•˜ë‹¤. ì í•©í•œ íŒ¨í„´ì— ëŒ€í•´ ë” ì‹ ì¤‘íˆ í–ˆì–´ì•¼ í–ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precision / recall ë¬¸ì œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) í…ìŠ¤íŠ¸ í† í°í™”ë¥¼ ìœ„í•œ ì •ê·œí‘œí˜„ì‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- í† í°í™”ëŠ” ë¬¸ìì—´ì„ ìª¼ê°œì–´ ì–¸ì–´ ë°ì´í„°ë¥¼ êµ¬ì„±í•˜ëŠ” ì‹ë³„ê°€ëŠ¥í•œ ì–¸ì–´í•™ì  ë‹¨ìœ„ë“¤ë¡œ ë§Œë“œëŠ” ì‘ì—…ì´ë‹¤\n",
    "- ì§€ê¸ˆê¹Œì§€ ë‹¤ë£¬ ë§ì€ ë§ë­‰ì¹˜ë“¤ì´ ì´ë¯¸ í† í°í™”ë˜ì–´ ìˆì—ˆìœ¼ë©°, nltkë„ ì•½ê°„ì˜ í† í°í™” ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤\n",
    "- ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•´ í† í°í™” ì‹œì¼œë³¸ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê°„ë‹¨í•œ í† í°í™” ë°©ë²•\n",
    "- ìš°ì„ , ê³µë°±ìœ¼ë¡œ ë‚˜ëˆ„ê¸°\n",
    "- í…ìŠ¤íŠ¸: Alice's Adventures in Wonderland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\n",
    "though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\n",
    "well without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone\\nthough),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very\\nwell', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(re.split(r' ', raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- split() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ `\\n`ê°€ í¬í•¨ë˜ë¯€ë¡œ ì›í•˜ë˜ ê²ƒì´ ì•„ë‹˜\n",
    "- space, tab, newline ëª¨ë‘ ë§¤ì¹˜ì‹œì¼œì•¼ í•œë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n"
     ]
    }
   ],
   "source": [
    "print(re.split(r'[ \\t\\n]+', raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- '(not' ê³¼ 'herself,' ê°™ì€ í† í°ì´ ë‚˜ì™”ë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', 'I', 'won', 't', 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', 'Soup', 'does', 'very', 'well', 'without', 'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered', '']\n"
     ]
    }
   ],
   "source": [
    "print(re.split(r'\\W+', raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ì‹œì‘ê³¼ ë ë¶€ë¶„ì— ë¹ˆ ë¬¸ìì—´ì´ ë‚˜ì˜´\n",
    "- `\\w+|\\S\\w*`: ì„ì˜ì˜ ë¬¸ìì—´ì„ ì°¾ê³  ì—†ìœ¼ë©´ (non-whitespace ë¬¸ì+ë‹¨ì–´ë¬¸ìì—´) í˜•íƒœë¥¼ ì°¾ìŒ\n",
    "- êµ¬ë‘£ì ì´ ì²˜ìŒì— ì˜¤ë©´ ë¬¸ìì—´ë¡œ ê°„ì£¼, ë‘ ê°œì´ìƒì˜ êµ¬ë‘£ì ì´ ì²˜ìŒì— ì˜¤ë©´ ë¶„ë¦¬ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'When\", 'I', \"'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'I\", 'won', \"'t\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '-', '-Maybe', 'it', \"'s\", 'always', 'pepper', 'that', 'makes', 'people', 'hot', '-tempered', ',', \"'\", '.', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'\\w+|\\S\\w*', raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `w+`ë¥¼ ì¼ë°˜í™”ì‹œì¼œ ë‹¨ì–´ ë‚´ë¶€ì˜ hyphens ì™€ apostrophesì„ í—ˆìš©í•´ë³´ì\n",
    "- `\\w+([-']\\w+)*`: ì„ì˜ì˜ ë‹¨ì–´ë¬¸ìì—´+ hyphens ë˜ëŠ” apostrophes + ì„ì˜ì˜ ë‹¨ì–´ë¬¸ìì—´\n",
    "- 'hot-tempered' ë‚˜ 'it's' ê°™ì€ ë‹¨ì–´ê°€ í¬í•¨ë  ê²ƒì´ë‹¤. \n",
    "- `?`ëŠ” ì•ì—ì„œ ë…¼ì˜í•œ ì´ìœ ë¡œ í¬í•¨ì‹œì¼œì•¼ í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", 'When', \"I'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'\", 'I', \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '--', 'Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', 'hot-tempered', ',', \"'\", '...']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `[-.(]+`: double hyphen, ellipsis ë° open parenthesisë¥¼ ë³„ë„ í† í° ì²˜ë¦¬í•œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/re_exp_symbols.png\" width=\"600\" style=\"margin-left: auto; margin-right: auto\">\n",
    "<p style=\"text-align: center;\">ì •ê·œí‘œí˜„ì‹ ì‹¬ë³¼</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTKì˜ ì •ê·œí‘œí˜„ì‹ Tokenizer\n",
    "- `nltk.regexp_tokenize()`: ê´„í˜¸ì˜ íŠ¹ë³„ì²˜ë¦¬ ë“±ì´ í•„ìš” ì—†ëŠ” ì  ë“± ë•Œë¬¸ì— `re.findall()` ë³´ë‹¤ ëŠ¥ë¥ ì . \n",
    "- verbose flag `(?x)`: ì •ê·œì‹ ë¬¸ìì—´ ë‚´ì˜ ê³µë°± ë° ì»¤ë©˜íŠ¸ë¥¼ ì—†ì•¤ë‹¤.\n",
    "- verbose flagì„ ì“°ë©´ ê³µë°±ì„ ì§€ì •í•˜ê¸° ìœ„í•´ 'ã€€'ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ê²Œ ë˜ë©°, \\së¥¼ ì‚¬ìš©í•´ì•¼ í•¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "pattern = r'''(?x)     # set flag to allow verbose regexps\n",
    "    (?:[A-Z]\\.)+       # abbreviations, e.g. U.S.A.\n",
    "  | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
    "  | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
    "  | \\.\\.\\.             # ellipsis\n",
    "  | [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
    "'''\n",
    "nltk.regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `set(tokens).difference(wordlist)`: worldlistì™€ ë¹„êµí•´ì„œ wordlistì— ì—†ëŠ” í† í°ì„ ë°˜í™˜í•˜ì—¬ **í† í°í™” ê²°ê³¼ ë¹„êµ** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í† í°í™” ê´€ë ¨ ì¶”ê°€ ë¬¸ì œë“¤\n",
    "- í† í°í™”ëŠ” ìƒê°ë³´ë‹¤ í›¨ì”¬ ì–´ë µë‹¤\n",
    "- í•œê°€ì§€ í•´ë²•ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ì—†ìœ¼ë©°, ë¬¸ì œ ì˜ì—­ì— ë”°ë¼ ê²°ì •ì„ í•´ì•¼ í•œë‹¤.\n",
    "- ì‘ì„±í•œ tokenizerì˜ ì¶œë ¥ì„ ê³ í’ˆì§ˆì˜ (gold-standard) í† í°ë“¤ê³¼ ë¹„êµí•˜ê¸° ìœ„í•´ raw textë¥¼ ê²€í† í•˜ëŠ” ê²ƒì´ ë°”ëŒì§\n",
    "- NLTKì—ëŠ” raw Wall Street Journal text (nltk.corpus.treebank_raw.raw())ì™€ í† í°í™”ëœ ë²„ì „(nltk.corpus.treebank.words())ì„ í¬í•¨í•˜ì—¬ Penn Treebank data ìƒ˜í”Œì´ í¬í•¨ë˜ì–´ ìˆë‹¤.\n",
    "- *didn't* ê°™ì€ ì¶•ì•½í˜•ì€ doì™€ not ë˜ëŠ” doì™€ n'të¡œ ë¶„ë¦¬í•˜ëŠ” ê²ƒì´ ì˜ë¯¸ë¥¼ ë¶„ì„í•˜ê¸°ì— ìœ ë¦¬í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ ìœ„í•´ lookup tableì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.7   Segmentation\n",
    "- Tokenizationì€ segmentation ë¬¸ì œì˜ ë”ìš± ì¼ë°˜í™”ëœ ì‚¬ë¡€ì´ë‹¤.\n",
    "- ì•ì„œ ë‹¤ë£¬ ë°©ë²•ë“¤ê³¼ëŠ” ì „í˜€ ë‹¤ë¥¸ ê¸°ë²•ë“¤ì˜ ì˜ˆë¥¼ ì‚´í´ ë³¸ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.250994070456922"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ì´ ì²˜ë¦¬ëŠ” ì½”í¼ìŠ¤ê°€ ë¬¸ì¥ êµ¬ë¶„ì´ ë˜ì–´ìˆë‹¤ëŠ” ì „ì œë¡œ ì²˜ë¦¬ ê°€ëŠ¥í–ˆë‹¤.\n",
    "- ì½”í¼ìŠ¤ê°€ ë¬¸ì ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë˜ì–´ ìˆëŠ” ê²½ìš°ê°€ í”í•˜ë‹¤.\n",
    "- NLTKëŠ” **Punkt sentence segmenter**([Kiss & Strunk, 2006](https://www.nltk.org/book/bibliography.html#kissstrunk2006))ë¥¼ ì œê³µí•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Nonsense!\"',\n",
      " 'said Gregory, who was very rational when anyone else\\nattempted paradox.',\n",
      " '\"Why do all the clerks and navvies in the\\n'\n",
      " 'railway trains look so sad and tired, so very sad and tired?',\n",
      " 'I will\\ntell you.',\n",
      " 'It is because they know that the train is going right.',\n",
      " 'It\\n'\n",
      " 'is because they know that whatever place they have taken a ticket\\n'\n",
      " 'for that place they will reach.',\n",
      " 'It is because after they have\\n'\n",
      " 'passed Sloane Square they know that the next station must be\\n'\n",
      " 'Victoria, and nothing but Victoria.',\n",
      " 'Oh, their wild rapture!',\n",
      " 'oh,\\n'\n",
      " 'their eyes like stars and their souls again in Eden, if the next\\n'\n",
      " 'station were unaccountably Baker Street!\"',\n",
      " '\"It is you who are unpoetical,\" replied the poet Syme.']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n",
    "sents = nltk.sent_tokenize(text)\n",
    "pprint.pprint(sents[79:89])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentence segmentationì€ ë§ˆì¹¨í‘œ(`.`) ë•Œë¬¸ì— íŠ¹íˆ ì–´ë µë‹¤\n",
    "- ë¬¸ì¥ ì¢…ë£Œì™€ ì•½ì–´ í‘œì‹œ ì™¸ì—ë„ ì–´ë–¤ ê²½ìš° ì•½ì–´ì™€ ì¢…ë£Œë¥¼ ê²¸í•˜ê¸°ë„ í•¨\n",
    "- ë˜ ë‹¤ë¥¸ ì ‘ê·¼ë²•ì€ Chapter 6ì˜ Section 2 Further Examples of Supervised Classificationì„ ì°¸ì¡°í•˜ì‹œì˜¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Word Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ë„ì–´ì“°ê¸°ê°€ ì—†ëŠ” ì–¸ì–´ì—ì„œëŠ” íŠ¹íˆ í† í°í™”ê°€ ì–´ë µë‹¤.\n",
    "- ì¤‘êµ­ì–´ì—ì„œ, çˆ±å›½äºº(ai4 \"love\" (verb), guo2 \"country\", ren2 \"person\")ëŠ” (çˆ±å›½ + äºº)(country-loving person) ë˜ëŠ” (çˆ± + å›½äºº)(love country-person)ìœ¼ë¡œ í† í°í™”ë  ìˆ˜ ìˆë‹¤.\n",
    "- ë¹„ìŠ·í•œ ë¬¸ì œê°€ ìŒì„± ì–¸ì–´ ì²˜ë¦¬ì—ì„œë„ ì¼ì–´ë‚œë‹¤. ë“£ëŠ” ì‚¬ëŒì€ ì—°ì†ì ì¸ ìŒì„± íë¦„ì„ ë¶„í• í•´ì•¼ í•œë‹¤.\n",
    "- ë‹¨ì–´ë¥¼ ë¯¸ë¦¬ ì•Œê³  ìˆì§€ ì•Šì€ ê²½ìš° ë¬¸ì œê°€ ë” ì–´ë ¤ì›Œì§„ë‹¤.\n",
    "- ë‹¨ì–´ ê²½ê³„ë¥¼ ì—†ì•¤ ë‹¤ìŒ ì˜ˆë¥¼ ë³´ì:\n",
    "> (ì˜ˆ)  \n",
    "> a. doyouseethekitty  \n",
    "> b. seethedoggy  \n",
    "> c. doyoulikethekitty  \n",
    "> d. likethedoggy  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ê° ê¸€ìì— ëŒ€í•´ ê¸€ì ë‹¤ìŒì— word-breakê°€ ë‚˜íƒ€ë‚˜ëŠ”ê°€ ì—¬ë¶€ì— ë”°ë¼ 1 ë˜ëŠ” 0ì„ í‘œì‹œí•´ë‘”ë‹¤\n",
    "- ì´ ë°©ë²•ì€ Chapter 7ì˜ \"Chunking\"ì—ì„œ ì§‘ì¤‘ì ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤.\n",
    "- ê°€ì •: learnerì—ê²Œ utterance breakê°€ ì£¼ì–´ì§„ë‹¤(ì´ëŠ” ì‹¤ì œ ì¢…ì¢… *ëŠ˜ì–´ì§„ ë©ˆì¶¤*ì— ëŒ€ì‘ëœë‹¤)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"  # ì´ˆê¸° segmentation\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\"  # ëª©í‘œ segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(text, segs):\n",
    "    words = []\n",
    "    last = 0\n",
    "    for i in range(len(segs)):\n",
    "        if segs[i] == '1':\n",
    "            words.append(text[last:i+1])\n",
    "            last = i+1\n",
    "    words.append(text[last:])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n",
    "segment(text, seg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do',\n",
       " 'you',\n",
       " 'see',\n",
       " 'the',\n",
       " 'kitty',\n",
       " 'see',\n",
       " 'the',\n",
       " 'doggy',\n",
       " 'do',\n",
       " 'you',\n",
       " 'like',\n",
       " 'the',\n",
       " 'kitty',\n",
       " 'like',\n",
       " 'the',\n",
       " 'doggy']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment(text, seg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- learnerëŠ” ë‹¨ì–´ë¥¼ ì·¨ë“í•˜ì—¬ ë‚´ë¶€ lexiconì— ì €ì¥í•œë‹¤.\n",
    "- [Brent (1995)](https://www.nltk.org/book/bibliography.html#brent1995)ì˜ ë°©ë²•ì„ ë”°ë¼ **ëª©ì í•¨ìˆ˜**ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤(ê·¸ë¦¼ 3.8)\n",
    "- ëª©ì í•¨ìˆ˜ = ì‚¬ì „(lexicon) í¬ê¸°(ë‹¨ì–´ ë‚´ ê¸€ì ìˆ˜ + ê° ë‹¨ì–´ì˜ ëì„ ë‚˜íƒ€ë‚´ëŠ” ë³„ë„ì˜ êµ¬íš ë¬¸ì)ì™€ lexiconì—ì„œ ì›ë˜ í…ìŠ¤íŠ¸ë¥¼ ë³µì›í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì •ë³´ëŸ‰ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” scoring function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/segmentation_objective.png\" width=\"600\" style=\"margin-left: auto; margin-right: auto\">\n",
    "<p style=\"text-align: center;\">ì„¸í¬ë©˜í…Œì´ì…˜ ëª©ì í•¨ìˆ˜</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(text, segs):\n",
    "    words = segment(text, segs)\n",
    "    text_size = len(words)\n",
    "    lexicon_size = sum(len(word) + 1 for word in set(words))\n",
    "    return text_size + lexicon_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doyou',\n",
       " 'see',\n",
       " 'thekitt',\n",
       " 'y',\n",
       " 'see',\n",
       " 'thedogg',\n",
       " 'y',\n",
       " 'doyou',\n",
       " 'like',\n",
       " 'thekitt',\n",
       " 'y',\n",
       " 'like',\n",
       " 'thedogg',\n",
       " 'y']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n",
    "seg3 = \"0000100100000011001000000110000100010000001100010000001\"\n",
    "segment(text, seg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(text, seg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(text, seg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(text, seg1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ë§ˆì§€ë§‰ ë‹¨ê³„ëŠ” ëª©ì í•¨ìˆ˜ë¥¼ ìµœì†Œí™”ì‹œí‚¤ëŠ” 0ê³¼ 1ì˜ íŒ¨í„´ì„ ì°¾ëŠ” ê²ƒì´ë‹¤.\n",
    "- ìµœì í™”ëœ segmentationì— ë‹¨ì–´ *thekitty*ê°€ ë‚˜ì˜¤ëŠ” ì´ìœ ëŠ” ì´ê²ƒì„ ë” ë¶„ë¦¬í•˜ê¸°ì—ëŠ” ì •ë³´ê°€ ë¶€ì¡±í•˜ê¸° ë•Œë¬¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-Deterministic Search Using Simulated Annealing:**  \n",
    "- Phrase segmenationì—ë§Œ ì ìš©\n",
    "- \"temperature\"ì— ë¹„ë¡€í•˜ì—¬ 0ê³¼ 1ì„ ë¬´ì‘ìœ„ë¡œ êµë€ì‹œí‚¨ë‹¤\n",
    "- ë§¤ ë°˜ë³µë§ˆë‹¤ temperatureëŠ” ë‚´ë ¤ê°€ê³  ë‹¨ì–´ ê²½ê³„ì˜ êµë€ì€ ì¤„ì–´ë“ ë‹¤\n",
    "- ì•Œê³ ë¦¬ì¦˜ì´ non-deterministicì´ë¯€ë¡œ ê²°ê³¼ëŠ” ë§¤ë²ˆ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def flip(segs, pos):\n",
    "    return segs[:pos] + str(1-int(segs[pos])) + segs[pos+1:]\n",
    "\n",
    "def flip_n(segs, n):\n",
    "    for i in range(n):\n",
    "        segs = flip(segs, randint(0, len(segs)-1))\n",
    "    return segs\n",
    "\n",
    "def anneal(text, segs, iterations, cooling_rate):\n",
    "    temperature = float(len(segs))\n",
    "    while temperature > 0.5:\n",
    "        best_segs, best = segs, evaluate(text, segs)\n",
    "        for i in range(iterations):\n",
    "            guess = flip_n(segs, round(temperature))\n",
    "            score = evaluate(text, guess)\n",
    "            if score < best:\n",
    "                best, best_segs = score, guess\n",
    "        score, segs = best, best_segs\n",
    "        temperature = temperature / cooling_rate\n",
    "        print(evaluate(text, segs), segment(text, segs))\n",
    "    print()\n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyouseeth', 'ekitty', 'seethedogg', 'ydoyouliketh', 'ekitty', 'like', 'thedoggy']\n",
      "63 ['doyouseeth', 'ekitty', 'seethedogg', 'ydoyouliketh', 'ekitty', 'like', 'thedoggy']\n",
      "61 ['doyouseeth', 'ekitty', 'seet', 'hedoggy', 'doyouliketh', 'ekitty', 'lik', 'e', 't', 'hedoggy']\n",
      "60 ['doyousee', 'th', 'ekitty', 'seet', 'hedoggy', 'doyou', 'like', 't', 'h', 'ekitty', 'like', 't', 'hedoggy']\n",
      "57 ['doyouseeth', 'ekitty', 'se', 'et', 'hedoggy', 'doyou', 'liket', 'h', 'ekitty', 'liket', 'hedoggy']\n",
      "54 ['doyou', 'seeth', 'ekitty', 'seet', 'hedoggy', 'doyou', 'like', 'th', 'ekitty', 'like', 't', 'hedoggy']\n",
      "49 ['doyou', 'see', 'th', 'ekitty', 'see', 't', 'hedoggy', 'doyou', 'like', 'th', 'ekitty', 'like', 't', 'hedoggy']\n",
      "49 ['doyou', 'see', 'th', 'ekitty', 'see', 't', 'hedoggy', 'doyou', 'like', 'th', 'ekitty', 'like', 't', 'hedoggy']\n",
      "49 ['doyou', 'see', 'th', 'ekitty', 'see', 't', 'hedoggy', 'doyou', 'like', 'th', 'ekitty', 'like', 't', 'hedoggy']\n",
      "46 ['doyou', 'see', 'th', 'ekitty', 'see', 'thedoggy', 'doyou', 'like', 'th', 'ekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0000100100000001001000000010000100010000000100010000000'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "anneal(text, seg1, 5000, 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ë°ì´í„°ê°€ ì¶©ë¶„í•˜ë‹¤ë©´ ì–´ëŠ ì •ë„ì˜ ì •í™•ë„ë¥¼ ê°–ì„ ìˆ˜ ìˆë‹¤.\n",
    "- ì´ ë°©ë²•ì€ ì‹œê°ì ì¸ ë‹¨ì–´ ê²½ê³„ í‘œìƒì´ ì—†ëŠ” ì–¸ì–´ì²´ê³„ì— ëŒ€í•œ í† í°í™”ì— ì ìš©ë  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.8 ì •ìˆ˜ ì¸ì½”ë”©(Integer Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) dictionary ì‚¬ìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A barber is a person.',\n",
       " 'a barber is good person.',\n",
       " 'a barber is huge person.',\n",
       " 'he Knew A Secret!',\n",
       " 'The Secret He Kept is huge secret.',\n",
       " 'Huge secret.',\n",
       " 'His barber kept his word.',\n",
       " 'a barber kept his word.',\n",
       " 'His barber kept his secret.',\n",
       " 'But keeping and keeping such a huge secret to himself was driving the barber crazy.',\n",
       " 'the barber went up a huge mountain.']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¬¸ì¥ ë¶„í• \n",
    "sentences = sent_tokenize(raw_text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "preprocessed_sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for sentence in sentences:\n",
    "    # ë‹¨ì–´ í† í°í™”\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    result = []\n",
    "\n",
    "    for word in tokenized_sentence: \n",
    "        word = word.lower() # ëª¨ë“  ë‹¨ì–´ë¥¼ ì†Œë¬¸ìí™”í•˜ì—¬ ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì¤„ì¸ë‹¤.\n",
    "        if word not in stop_words: # ë‹¨ì–´ í† í°í™” ëœ ê²°ê³¼ì— ëŒ€í•´ì„œ ë¶ˆìš©ì–´ë¥¼ ì œê±°í•œë‹¤.\n",
    "            if len(word) > 2: # ë‹¨ì–´ ê¸¸ì´ê°€ 2ì´í•˜ì¸ ê²½ìš°ì— ëŒ€í•˜ì—¬ ì¶”ê°€ë¡œ ë‹¨ì–´ë¥¼ ì œê±°í•œë‹¤.\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0 \n",
    "                vocab[word] += 1\n",
    "    preprocessed_sentences.append(result) \n",
    "print(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"barber\"]) # 'barber'ë¼ëŠ” ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
     ]
    }
   ],
   "source": [
    "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab_sorted :\n",
    "    if frequency > 1 : # ë¹ˆë„ìˆ˜ê°€ ì‘ì€ ë‹¨ì–´ëŠ” ì œì™¸.\n",
    "        i = i + 1\n",
    "        word_to_index[word] = i\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "words_frequency = [word for word, index in word_to_index.items() if index >= vocab_size + 1] # ì¸ë±ìŠ¤ê°€ 5 ì´ˆê³¼ì¸ ë‹¨ì–´ ì œê±°\n",
    "for w in words_frequency:\n",
    "    del word_to_index[w] # í•´ë‹¹ ë‹¨ì–´ì— ëŒ€í•œ ì¸ë±ìŠ¤ ì •ë³´ë¥¼ ì‚­ì œ\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index['OOV'] = len(word_to_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barber', 'person'],\n",
       " ['barber', 'good', 'person'],\n",
       " ['barber', 'huge', 'person'],\n",
       " ['knew', 'secret'],\n",
       " ['secret', 'kept', 'huge', 'secret'],\n",
       " ['huge', 'secret'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'secret'],\n",
       " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
       " ['barber', 'went', 'huge', 'mountain']]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
     ]
    }
   ],
   "source": [
    "encoded_sentences = []\n",
    "for sentence in preprocessed_sentences:\n",
    "    encoded_sentence = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            encoded_sentence.append(word_to_index[word])\n",
    "        except KeyError:\n",
    "            encoded_sentence.append(word_to_index['OOV'])\n",
    "    encoded_sentences.append(encoded_sentence)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Counter ì‚¬ìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barber', 'person'],\n",
       " ['barber', 'good', 'person'],\n",
       " ['barber', 'huge', 'person'],\n",
       " ['knew', 'secret'],\n",
       " ['secret', 'kept', 'huge', 'secret'],\n",
       " ['huge', 'secret'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'secret'],\n",
       " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
       " ['barber', 'went', 'huge', 'mountain']]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "# words = np.hstack(preprocessed_sentences)ìœ¼ë¡œë„ ìˆ˜í–‰ ê°€ëŠ¥.\n",
    "all_words_list = sum(preprocessed_sentences, [])\n",
    "print(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    }
   ],
   "source": [
    "# íŒŒì´ì¬ì˜ Counter ëª¨ë“ˆì„ ì´ìš©í•˜ì—¬ ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ ì¹´ìš´íŠ¸\n",
    "vocab = Counter(all_words_list)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"barber\"]) # 'barber'ë¼ëŠ” ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ìƒìœ„ 5ê°œì˜ ë‹¨ì–´ë§Œ ì €ì¥\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab :\n",
    "    i = i + 1\n",
    "    word_to_index[word] = i\n",
    "    \n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) NLTKì˜ FreqDist ì‚¬ìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = FreqDist(np.hstack(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"barber\"]) # 'barber'ë¼ëŠ” ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ìƒìœ„ 5ê°œì˜ ë‹¨ì–´ë§Œ ì €ì¥\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
    "print(word_to_index)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tokenization.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "25380c4af315aecd0af37ce090a32ef5cc426e5e777cf7d051bf86d7e79aec54"
  },
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
