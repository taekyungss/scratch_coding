{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:2.5em\">2. 텍스트 전처리 및 토큰화 (1)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6Qj3ZjBA5j7"
   },
   "source": [
    "[참조]\n",
    "- [Natural Language Processing with Python (NLTK Books)](https://www.nltk.org/book/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color: crimson\">[Remark🐞]</span> 쥬피터 셀에서 파이썬 패키지 설치시 권장되는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import sys\n",
    "\n",
    "!conda install --yes --prefix {sys.prefix} package_to_install\n",
    "# 또는\n",
    "!{sys.executable} -m pip install package_to_install\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[참조] [Installing Python Packages from a Jupyter Notebook](https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3bN6iK49Gp4"
   },
   "source": [
    "# 2.1 단어 토큰화(Word Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sxtCj_x9Dtm"
   },
   "source": [
    "### Tokenizer 비교 - 선택의 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선택의 문제는 내가 사용하고자 하는 텍스트의 특성과 토큰화 목적에 따라 맞는 tokenizer를 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nzfDhsYm852i"
   },
   "outputs": [],
   "source": [
    "sentence = \"Mr. Jone's house isn't far from here, New York.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 띄어쓰기(space)로 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.', \"Jone's\", 'house', \"isn't\", 'far', 'from', 'here,', 'New', 'York.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['멀리', '가지', '마.', '김', '박사님', '너를', '보러', '오실거야.', '네가', '없으면', '당황스러울거야.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"멀리 가지 마. 김 박사님 너를 보러 오실거야. 네가 없으면 당황스러울거야.\"\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK `word_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\taekyung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\taekyung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Jone's house isn't far from here, New York.\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lTs0hQ8HRDYl",
    "outputId": "75e4ad63-1081-4446-b38f-29ffd0b457bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Jone', \"'s\", 'house', 'is', \"n't\", 'far', 'from', 'here', ',', 'New', 'York', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK `WordPunctTokenizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 word_tokenize와는 달리 구두점(') 을 별도의 토큰으로 구분해서 토큰화를 진행\n",
    "\n",
    "간단하고 빠르게 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WFDQA6JLR5HT",
    "outputId": "9b3a1f99-708e-4dd0-cf44-3d85b2a07c08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr', '.', 'Jone', \"'\", 's', 'house', 'isn', \"'\", 't', 'far', 'from', 'here', ',', 'New', 'York', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(WordPunctTokenizer().tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'do', 'uh', 'main', 'mainly', 'business', 'data', 'processing']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(\"I do uh main mainly business data processing\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK `TreebankWordTokenizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좀 더 정교한 토큰화를 제공합니다. \n",
    "\n",
    "예를 들어, \"don't\"와 \"won't\"과 같은 축약형에 대해서도 이를 올바르게 인식하여 \"do\", \"n't\", \"wo\", \"n't\"로 분리\n",
    "\n",
    " 구어체나 문법적으로 복잡한 텍스트를 처리할 때 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Jone', \"'s\", 'house', 'is', \"n't\", 'far', 'from', 'here', ',', 'New', 'York', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EhsH1n5uR6g7",
    "outputId": "26db1c27-6e07-4418-ea94-6e88cecf4443"
   },
   "source": [
    "#### PyTorch torchtext가 제공하는 tokenizer 중 `basic_english`\n",
    "- torchtext가 제공하는 tokenizer: `basic_english`, `spacy`, `moses`, `toktok`, `revtok`, `subword`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mr', '.', 'jone', \"'\", 's', 'house', 'isn', \"'\", 't', 'far', 'from', 'here', ',', 'new', 'york', '.']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "print(get_tokenizer(\"basic_english\")(sentence))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow `text_to_word_sequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\taekyung\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "['mr', \"jone's\", 'house', \"isn't\", 'far', 'from', 'here', 'new', 'york']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "print(text_to_word_sequence(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텐서플로우 tensorflow.keras.preprocessing.text의 `text_to_word_sequence` 나 `Tokenizer`는 `tf.Tensor`를 지원하지 않음\n",
    "- 권장 방법 참고: [텍스트 로드 하기](https://www.tensorflow.org/tutorials/load_data/text?hl=ko)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6lWTou49PLA"
   },
   "source": [
    "# 2.2. 문장 분할(Sentence Segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK `sent_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2WixyvHR9US",
    "outputId": "a5cb0c43-f899-4f99-812c-cde44051c209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't go far.\", 'Dr. Kim is coming to see you.', \"I'd be embarrassed without you.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Don't go far. Dr. Kim is coming to see you. I'd be embarrassed without you.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['멀리 가지 마.', '김 박사님 너를 보러 오실거야.', '네가 없으면 당황스러울거야.']\n"
     ]
    }
   ],
   "source": [
    "text = \"멀리 가지 마. 김 박사님 너를 보러 오실거야. 네가 없으면 당황스러울거야.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go to the IP 192.168.56.31 server, save the log file, and send the results to aaa@gmail.com.', \"After that, let's go have lunch.\"]\n"
     ]
    }
   ],
   "source": [
    "text = \"Go to the IP 192.168.56.31 server, save the log file, and send the results to aaa@gmail.com. After that, let's go have lunch.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 aaa@gmail.com 로 결과 좀 보내줘.', '그 후 점심 먹으러 가자.']\n"
     ]
    }
   ],
   "source": [
    "text = \"IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 aaa@gmail.com 로 결과 좀 보내줘. 그 후 점심 먹으러 가자.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KSS `split_sentences`\n",
    "- KSS(Korean Sentence Splitter) - 박상길"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KIDooFnDSM-7",
    "outputId": "d1ff8037-d3b5-4dda-d71a-5703a0c4386b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
      "For your information, Kss also supports mecab backend.\n",
      "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
      "Please refer to following web sites for details:\n",
      "- mecab: https://cleancode-ws.tistory.com/97\n",
      "- konlpy.tag.Mecab: https://uwgdqo.tistory.com/363\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['멀리 가지 마.', '김 박사님 너를 보러 오실거야.', '네가 없으면 당황스러울거야.']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "text = \"멀리 가지 마. 김 박사님 너를 보러 오실거야. 네가 없으면 당황스러울거야.\"\n",
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [MS Windows 에 MeCab 설치하는 방법](https://wonhwa.tistory.com/49)\n",
    "- [Windows Python 3.x에 MeCab 설치법](https://cleancode-ws.tistory.com/97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 aaa@gmail.com 로 결과 좀 보내줘.', '그 후 점심 먹으러 가자.']\n"
     ]
    }
   ],
   "source": [
    "text = \"IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 aaa@gmail.com 로 결과 좀 보내줘. 그 후 점심 먹으러 가자.\"\n",
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Go to the IP 192.168.56.31 server, save the log file, and send the results to aaa@gmail.com. After that, let's go have lunch.\"]\n"
     ]
    }
   ],
   "source": [
    "text = \"Go to the IP 192.168.56.31 server, save the log file, and send the results to aaa@gmail.com. After that, let's go have lunch.\"\n",
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문장 분할 실패!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8znWPo59hX9"
   },
   "source": [
    "# 2.3 토큰화 및 POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "MrD41LYKSQGb"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f-pa-nMwSibh",
    "outputId": "bf22c9b2-98de-4f81-af2d-812fb169a3fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화 : ['Do', \"n't\", 'go', 'far', '.', 'Dr.', 'Kim', 'is', 'coming', 'to', 'see', 'you', '.', 'I', \"'d\", 'be', 'embarrassed', 'without', 'you', '.']\n",
      "품사 태깅 : [('Do', 'VBP'), (\"n't\", 'RB'), ('go', 'VB'), ('far', 'RB'), ('.', '.'), ('Dr.', 'NNP'), ('Kim', 'NNP'), ('is', 'VBZ'), ('coming', 'VBG'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('.', '.'), ('I', 'PRP'), (\"'d\", 'MD'), ('be', 'VB'), ('embarrassed', 'VBN'), ('without', 'IN'), ('you', 'PRP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text = \"Don't go far. Dr. Kim is coming to see you. I'd be embarrassed without you.\"\n",
    "tokenized_sentence = word_tokenize(text)\n",
    "\n",
    "print('단어 토큰화 :',tokenized_sentence)\n",
    "print('품사 태깅 :',pos_tag(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'go', 'far', '.']\n",
      "[('Do', 'VBP'), (\"n't\", 'RB'), ('go', 'VB'), ('far', 'RB'), ('.', '.')]\n",
      "['Dr.', 'Kim', 'is', 'coming', 'to', 'see', 'you', '.']\n",
      "[('Dr.', 'NNP'), ('Kim', 'NNP'), ('is', 'VBZ'), ('coming', 'VBG'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('.', '.')]\n",
      "['I', \"'d\", 'be', 'embarrassed', 'without', 'you', '.']\n",
      "[('I', 'PRP'), (\"'d\", 'MD'), ('be', 'VB'), ('embarrassed', 'VBN'), ('without', 'IN'), ('you', 'PRP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "for sent in sentences:\n",
    "    tokenized = word_tokenize(sent)\n",
    "    print(tokenized)\n",
    "    print(pos_tag(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoNLPy\n",
    "- 지원하는 형태소 분석기: Okt(Open Korea Text), 메캅(Mecab), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "c9BJZNPZSnd7"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "GAYMyRBg9noI"
   },
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "kkma = Kkma()\n",
    "text = \"3월에 대학생들이 강한 이유는? 개강해서!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OKT(Open Korea Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oH4Ye8cg9q1H",
    "outputId": "672a8e4e-0452-454c-c881-80e184cd3c19",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT 형태소 분석 : ['3월', '에', '대학생', '들', '이', '강한', '이유', '는', '?', '개강', '해서', '!']\n",
      "OKT 품사 태깅 : [('3월', 'Number'), ('에', 'Foreign'), ('대학생', 'Noun'), ('들', 'Suffix'), ('이', 'Josa'), ('강한', 'Adjective'), ('이유', 'Noun'), ('는', 'Josa'), ('?', 'Punctuation'), ('개강', 'Noun'), ('해서', 'Verb'), ('!', 'Punctuation')]\n",
      "OKT 명사 추출 : ['대학생', '이유', '개강']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('OKT 형태소 분석 :',okt.morphs(text))\n",
    "print('OKT 품사 태깅 :',okt.pos(text))\n",
    "print('OKT 명사 추출 :',okt.nouns(text)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 꼬꼬마(kma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbuqewYRTXYT",
    "outputId": "48607d49-8368-4316-d752-e23fa2a2501d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "꼬꼬마 형태소 분석 : ['3', '월', '에', '대학생', '들', '이', '강하', 'ㄴ', '이유', '는', '?', '개강', '하', '어서', '!']\n",
      "꼬꼬마 품사 태깅 : [('3', 'NR'), ('월', 'NNM'), ('에', 'JKM'), ('대학생', 'NNG'), ('들', 'XSN'), ('이', 'JKS'), ('강하', 'VV'), ('ㄴ', 'ETD'), ('이유', 'NNG'), ('는', 'JX'), ('?', 'SF'), ('개강', 'NNG'), ('하', 'XSV'), ('어서', 'ECD'), ('!', 'SF')]\n",
      "꼬꼬마 명사 추출 : ['3', '3월', '월', '대학생', '이유', '개강']\n"
     ]
    }
   ],
   "source": [
    "print('꼬꼬마 형태소 분석 :',kkma.morphs(text))\n",
    "print('꼬꼬마 품사 태깅 :',kkma.pos(text))\n",
    "print('꼬꼬마 명사 추출 :',kkma.nouns(text))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 불용어(Stopwords) 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK `stopwords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\taekyung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\taekyung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words_list = stopwords.words('english')\n",
    "print(len(stop_words_list))\n",
    "print(stop_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'lying', 'in', 'ponds', 'distributing', 'swords', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'masses', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n",
      "['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'lying', 'ponds', 'distributing', 'swords', 'basis', 'system', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'mandate', 'masses', ',', 'farcical', 'aquatic', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "text =\"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "word_tokens = word_tokenize(text)\n",
    "result = []\n",
    "for word in word_tokens: \n",
    "    if word not in stop_words_list: \n",
    "        result.append(word) \n",
    "print(word_tokens)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'lying', 'in', 'ponds', 'distributing', 'swords', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'masses', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n",
      "['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'lying', 'ponds', 'distributing', 'swords', 'basis', 'system', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'mandate', 'masses', ',', 'farcical', 'aquatic', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "# 불용어 처리 하는 방법\n",
    "\n",
    "word_tokens = word_tokenize(text)\n",
    "result = []\n",
    "for i in word_tokens:\n",
    "    if i not in stop_words_list:\n",
    "        result.append(i)\n",
    "print(word_tokens)        \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoNLPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '\\n', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
      "불용어 제거 후 : ['고기', '구', '우려', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\n"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "text = \"\"\"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든.\n",
    "예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\"\"\n",
    "stop_words = \"\\n 를 아무렇게나 고 안 돼 같은 게 구울 때 는\"\n",
    "stop_words_list = set(stop_words.split(' '))\n",
    "word_tokens = okt.morphs(text)\n",
    "\n",
    "result = []\n",
    "for word in word_tokens: \n",
    "    if word not in stop_words_list: \n",
    "        result.append(word) \n",
    "\n",
    "print('불용어 제거 전 :',word_tokens) \n",
    "print('불용어 제거 후 :',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['고기', '구', '우려', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\n"
     ]
    }
   ],
   "source": [
    "# tokenizer 설정 : okt\n",
    "okt = Okt()\n",
    "text = \"\"\"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든.\n",
    "예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\"\"\n",
    "stop_words = \"\\n 를 아무렇게나 고 안 돼 같은 게 구울 때 는\"\n",
    "\n",
    "tokenized_sentences = okt.morphs(text)\n",
    "stop_words_set = stop_words.split(\" \")\n",
    "result = []\n",
    "for sentence in tokenized_sentences:\n",
    "    if sentence not in stop_words_set:\n",
    "        result.append(sentence)\n",
    "        \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `many-stop-words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지원 언어 :  22\n",
      "['ar', 'ca', 'cs', 'de', 'el', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja', 'kr', 'nl', 'no', 'pl', 'pt', 'ru', 'sk', 'sv', 'tr', 'zh']\n"
     ]
    }
   ],
   "source": [
    "from many_stop_words import get_stop_words, available_languages\n",
    "import many_stop_words\n",
    "print(\"지원 언어 : \", len(many_stop_words.available_languages))\n",
    "print(many_stop_words.available_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595\n",
      "{'입각하여', '비로소', '인 듯하다', '그저', '나머지는', '구', '위해서', '참나', '또한', '흐흐', '하면된다', '차라리', '어찌됏어', '이러이러하다', '그런 까닭에', '매번', '쳇', '하도다', '아래윗', '어느것', '한마디', '좋아', '한다면 몰라도', '틈타', '하기 위하여', '오호', '마치', '예하면', '요컨대', '년', '구토하다', '놀라다', '이와 같은', '이번', '와', '각각', '너희', '한적이있다', '그러한즉', '함께', '시초에', '휘익', '하도록하다', '아하', '이었다', '연이서', '뿐만아니라', '및', '요만한 것', '팔', '도착하다', '위에서 서술한바와같이', '설령', '도달하다', '콸콸', '에서', '이때', '한항목', '얼마나', '않기 위해서', '바와같이', '하게될것이다', '월', '그러므로', '그만이다', '만약', '만 못하다', '위하여', '일곱', '휴', '봐', '구체적으로', '누구', '설사', '든간에', '어느곳', '공동으로', '첫번째로', '일때', '제각기', '아홉', '어떤것', '무렵', '일반적으로', '참', '어떤것들', '습니까', '비걱거리다', '조차', '셋', '정도에 이르다', '만일', '혹은', '훨씬', '다섯', '까지', '타다', '이 정도의', '바꾸어서 한다면', '가', '이런', '하곤하였다', '들', '전후', '어떻게', '그렇지만', '으로써', '할만하다', '오로지', '기타', '이용하여', '칠', '으로 인하여', '을', '의해', '하지마', '하더라도', '어쩔수 없다', '오히려', '이상', '허', '비추어 보아', '것', '솨', '각자', '하지마라', '우르르', '이와 같다', '불문하고', '하기보다는', '이렇게말하자면', '따라서', '각종', '어', '불구하고', '할지라도', '할 지경이다', '기대여', '근거로', '막론하고', '대해 말하자면', '그위에', '다수', '전부', '어디', '게우다', '이래', '기준으로', '답다', '하면서', '와 같은 사람들', '하기에', '바로', '헉헉', '하고 있다', '딩동', '시간', '둘', '쿵', '영차', '이렇구나', '보는데서', '향하여', '가까스로', '과', '결론을 낼 수 있다', '하지 않도록', '하느니', '어떻해', '이봐', '자기집', '과연', '그렇지', '그리고', '우에 종합한것과같이', '남짓', '견지에서', '지만', '에 달려 있다', '령', '갖고말하자면', '다음에', '때문에', '아이', '얼마간', '이곳', '할뿐', '그러니', '그에 따르는', '대해서', '의해서', '아이고', '예를 들자면', '된바에야', '그렇게 함으로써', '다시말하면', '하는것도', '더욱더', '응당', '하기는한데', '보드득', '육', '자마자', '습니다', '해도좋다', '연관되다', '이 때문에', '조차도', '줄은모른다', '이리하여', '형식으로 쓰여', '자신', '약간', '무릎쓰고', '요만큼', '겸사겸사', '하려고하다', '무슨', '하도록시키다', '대하면', '아무도', '해도된다', '아이구', '로부터', '의', '아니나다를가', '그리하여', '으로', '하물며', '언제', '그들', '둥둥', '끼익', '거바', '다소', '에 한하다', '너희들', '이 밖에', '윙윙', '저것만큼', '통하여', '더욱이는', '그런데', '부터', '삐걱거리다', '하든지', '양자', '하하', '즉시', '줄은 몰랏다', '까닭으로', '비슷하다', '지든지', '그럼에도 불구하고', '쾅쾅', '뒤따라', '할수있다', '여섯', '어느해', '본대로', '하구나', '이 외에', '여기', '얼마큼', '시작하여', '삐걱', '더구나', '오', '자', '왜', '힘입어', '할때', '할수있어', '어찌하든지', '사', '했어요', '여부', '까지 미치다', '상대적으로 말하자면', '잠깐', '비길수 없다', '이렇게되면', '저', '해서는 안된다', '댕그', '저쪽', '그렇지 않으면', '로 인하여', '퍽', '하여금', '총적으로 말하면', '어찌', '고로', '좍좍', '어느', '옆사람', '메쓰겁다', '하지만', '그러면', '더불어', '한데', '알았어', '어이', '반대로 말하자면', '하게하다', '할 생각이다', '뿐이다', '중에서', '이렇게 많은 것', '같다', '다음으로', '제', '주저하지 않고', '하는것이 낫다', '그래도', '앗', '할 따름이다', '졸졸', '아니면', '두번째로', '등', '어기여차', '즉', '의해되다', '뒤이어', '이천육', '아', '다시 말하자면', '붕붕', '당장', '게다가', '만이 아니다', '마저', '따지지 않다', '누가 알겠는가', '탕탕', '어쨋든', '저것', '하여야', '몇', '일지라도', '아야', '마음대로', '다만', '부류의 사람들', '뚝뚝', '다른 방면으로', '그치지 않다', '아이쿠', '조금', '타인', '기점으로', '한 이유는', '이라면', '어찌하여', '아니었다면', '바꾸어말하면', '관련이 있다', '다음', '할줄알다', '제외하고', '만은 아니다', '바꾸어말하자면', '혹시', '요만한걸', '툭', '하는 편이 낫다', '일', '겨우', '지말고', '영', '개의치않고', '퉤', '때가 되어', '이천칠', '에 있다', '만약에', '만큼', '시키다', '언젠가', '때', '딱', '따라', '와아', '아니라면', '각', '보다더', '모두', '점에서 보아', '이천구', '앞에서', '할 줄 안다', '한 까닭에', '어느 년도', '여덟', '하겠는가', '일것이다', '오자마자', '매', '이러한', '앞의것', '우선', '전자', '예', '결과에 이르다', '이만큼', '엉엉', '여전히', '어때', '하는것만 못하다', '더라도', '동안', '잠시', '까악', '우리들', '어떠한', '예를 들면', '나', '끙끙', '에게', '말할것도 없고', '얼마든지', '어째서', '바꿔 말하면', '향해서', '이로 인하여', '그렇지 않다면', '의지하여', '남들', '않기 위하여', '그러니까', '이것', '관해서는', '소인', '여', '이어서', '어찌됏든', '하면 할수록', '비하면', '할망정', '삼', '임에 틀림없다', '쪽으로', '그럼', '운운', '잇따라', '좀', '대하여', '바꾸어서 말하면', '아무거나', '한 후', '허걱', '실로', '이지만', '더군다나', '무엇때문에', '총적으로 보면', '있다', '비록', '이와같다면', '이럴정도로', '아이야', '해야한다', '펄렁', '너', '논하지 않다', '관한', '야', '이외에도', '그러나', '넷', '여러분', '오르다', '또', '하자마자', '와르르', '이', '그래서', '얼마만큼', '어떤', '심지어', '할 힘이 있다', '네', '아니', '비교적', '물론', '아울러', '어느때', '설마', '곧', '에 가서', '까지도', '하고있었다', '로', '토하다', '륙', '하는바', '근거하여', '일단', '이천팔', '그때', '동시에', '해요', '하기 때문에', '꽈당', '반드시', '소생', '거의', '즈음하여', '오직', '이쪽', '저희', '무엇', '대로 하다', '것과 같이', '진짜로', '관계없이', '시각', '안 그러면', '하는 김에', '밖에 안된다', '저기', '한켠으로는', '하지 않는다면', '그렇지않으면', '라 해도', '당신', '이 되다', '관계가 있다', '된이상', '그', '그래', '왜냐하면', '중의하나', '주룩주룩', '흥', '얼마', '따위', '우리', '결국', '할지언정', '하', '헐떡헐떡', '것들', '에 대해', '이와 반대로', '하기만 하면', '버금', '여보시오', '등등', '얼마 안 되는 것', '생각한대로', '허허', '이젠', '이르기까지', '그중에서', '에', '예컨대', '를', '로써', '마저도', '같이', '쉿', '여차', '인젠', '알 수 있다', '향하다', '혼자', '이유만으로', '총적으로', '거니와', '단지', '그런즉', '하나', '관하여', '모', '외에도', '입장에서', '으로서', '뿐만 아니라', '말하자면', '한다면', '가령', '의거하여', '다른', '해봐요', '헉', '하마터면', '반대로', '어느쪽', '자기', '응', '팍', '봐라', '고려하면'}\n"
     ]
    }
   ],
   "source": [
    "print(len(many_stop_words.get_stop_words('kr')))\n",
    "print(many_stop_words.get_stop_words('kr'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Normalizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization은 다음과 같은 작업을 포함한다:\n",
    "- 텍스트를 모두 lowercase로 바꾸는 일\n",
    "- stemming을 통해 접사들을 떼어버리거나,\n",
    "- lemmatization을 통해 단어를 사전 표제어로 나타나는 형태로 바꾸는 작업들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'lying', 'in', 'ponds', 'distributing', 'swords', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'masses', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "text =\"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming (어간 추출)\n",
    "-  RE을 사용하여 수작업으로 할 수도 있으나, NLTK 내장 stemmer는 불규칙적인 경우들을 처리할 수 있으므로 더 효율적이다.\n",
    "- NLTK 내장 stemmer:\n",
    "  - Porter stemmer\n",
    "  - Lancaster stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "print([porter.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "lancaster = nltk.LancasterStemmer()\n",
    "print([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stemming은 잘 정의된 과정이 아니며 목적에 맞는 것을 선택해 사용한다\n",
    "- 텍스트 인텍싱 등 일반적인 NLP 작업에 있어 Porter stemmer 성능이 Lancaster stemmer 보다 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmer를 사용한 텍스트 인덱싱\n",
    "- `concordance` view는 주어진 단어가 텍스트 내에서 출현하는 것을 주변 단어들(context)과 함께 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedText(object):\n",
    "\n",
    "    def __init__(self, stemmer, text):\n",
    "        self._text = text\n",
    "        self._stemmer = stemmer\n",
    "        self._index = nltk.Index((self._stem(word), i) # nltk.Index source code: https://tedboy.github.io/nlps/_modules/nltk/util.html#Index\n",
    "                                for (i, word) in enumerate(text))  \n",
    "        \n",
    "    def concordance(self, word, width=40):\n",
    "        key = self._stem(word)\n",
    "        wc = int(width/4)                # words of context, 왜 4로 나눴을까?\n",
    "        for i in self._index[key]:\n",
    "            lcontext = ' '.join(self._text[i-wc:i])\n",
    "            rcontext = ' '.join(self._text[i:i+wc])\n",
    "            ldisplay = '{:>{width}}'.format(lcontext[-width:], width=width)  # ':>{width}' width 길이 만큼 표시하되 오른쪽 맞춤한다\n",
    "            rdisplay = '{:{width}}'.format(rcontext[:width], width=width)\n",
    "            print(ldisplay, rdisplay)\n",
    "\n",
    "    def _stem(self, word):\n",
    "        return self._stemmer.stem(word).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "grail = nltk.corpus.webtext.words('grail.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16967"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r king ! DENNIS : Listen , strange women lying in ponds distributing swords is no\n",
      " beat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of\n",
      "       Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded !   \n",
      "doctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well  \n",
      "ere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which \n",
      "   you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --\n",
      "h it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k\n",
      "not stop our fight ' til each one of you lies dead , and the Holy Grail returns t\n"
     ]
    }
   ],
   "source": [
    "text = IndexedText(porter, grail)\n",
    "text.concordance('lie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization (표제어 추출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\taekyung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\taekyung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"\"\"Reading stories is a great way to improve your vocabulary\n",
    "and we have lots of great stories for you to watch.\"\"\"\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reading', 'stories', 'is', 'a', 'great', 'way', 'to', 'improve', 'your', 'vocabulary', 'and', 'we', 'have', 'lots', 'of', 'great', 'stories', 'for', 'you', 'to', 'watch', '.']\n",
      "['Reading', 'story', 'is', 'a', 'great', 'way', 'to', 'improve', 'your', 'vocabulary', 'and', 'we', 'have', 'lot', 'of', 'great', 'story', 'for', 'you', 'to', 'watch', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "print(tokens)\n",
    "print([lemmatizer.lemmatize(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print(words)\n",
    "print([lemmatizer.lemmatize(t) for t in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- WordNet lemmatizer는 접사를 잘라낸 단어가 사전에 있을 때만 접사를 자른다\n",
    "- 추가 처리 때문에 stemmer 보다 느리다\n",
    "- 'lying'을 처리하지 않았지만 'women'은 'woman'으로 바꿨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 정규표현식 (Regular Expressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- regular expression을 표시할 때 «ed$» 처럼 «정규식» 기호를 사용할 것임.\n",
    "- Python에서의 regular expression은 `re` library를 사용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 기본 Meta-characters 사용하기\n",
    "- 'ed'로 끝나는 단어 찾기\n",
    "- `$`: end of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abaissed',\n",
       " 'abandoned',\n",
       " 'abased',\n",
       " 'abashed',\n",
       " 'abatised',\n",
       " 'abed',\n",
       " 'aborted',\n",
       " 'abridged',\n",
       " 'abscessed',\n",
       " 'absconded',\n",
       " 'absorbed',\n",
       " 'abstracted',\n",
       " 'abstricted',\n",
       " 'accelerated',\n",
       " 'accepted',\n",
       " 'accidented',\n",
       " 'accoladed',\n",
       " 'accolated',\n",
       " 'accomplished',\n",
       " 'accosted',\n",
       " 'accredited',\n",
       " 'accursed',\n",
       " 'accused',\n",
       " 'accustomed',\n",
       " 'acetated',\n",
       " 'acheweed',\n",
       " 'aciculated',\n",
       " 'aciliated',\n",
       " 'acknowledged',\n",
       " 'acorned',\n",
       " 'acquainted',\n",
       " 'acquired',\n",
       " 'acquisited',\n",
       " 'acred',\n",
       " 'aculeated',\n",
       " 'addebted',\n",
       " 'added',\n",
       " 'addicted',\n",
       " 'addlebrained',\n",
       " 'addleheaded',\n",
       " 'addlepated',\n",
       " 'addorsed',\n",
       " 'adempted',\n",
       " 'adfected',\n",
       " 'adjoined',\n",
       " 'admired',\n",
       " 'admitted',\n",
       " 'adnexed',\n",
       " 'adopted',\n",
       " 'adossed',\n",
       " 'adreamed',\n",
       " 'adscripted',\n",
       " 'aduncated',\n",
       " 'advanced',\n",
       " 'advised',\n",
       " 'aeried',\n",
       " 'aethered',\n",
       " 'afeared',\n",
       " 'affected',\n",
       " 'affectioned',\n",
       " 'affined',\n",
       " 'afflicted',\n",
       " 'affricated',\n",
       " 'affrighted',\n",
       " 'affronted',\n",
       " 'aforenamed',\n",
       " 'afterfeed',\n",
       " 'aftershafted',\n",
       " 'afterthoughted',\n",
       " 'afterwitted',\n",
       " 'agazed',\n",
       " 'aged',\n",
       " 'agglomerated',\n",
       " 'aggrieved',\n",
       " 'agminated',\n",
       " 'agnamed',\n",
       " 'agonied',\n",
       " 'agreed',\n",
       " 'agueweed',\n",
       " 'ahungered',\n",
       " 'aiguilletted',\n",
       " 'ailweed',\n",
       " 'airbrained',\n",
       " 'airified',\n",
       " 'aiseweed',\n",
       " 'aisled',\n",
       " 'alarmed',\n",
       " 'alated',\n",
       " 'alimonied',\n",
       " 'aliped',\n",
       " 'alleyed',\n",
       " 'allied',\n",
       " 'alligatored',\n",
       " 'allseed',\n",
       " 'almsdeed',\n",
       " 'aloed',\n",
       " 'altared',\n",
       " 'alveolated',\n",
       " 'amazed',\n",
       " 'ameed',\n",
       " 'amiced',\n",
       " 'amphitheatered',\n",
       " 'ampullated',\n",
       " 'amused',\n",
       " 'anchored',\n",
       " 'angled',\n",
       " 'anguiped',\n",
       " 'anguished',\n",
       " 'angulated',\n",
       " 'angulinerved',\n",
       " 'anhungered',\n",
       " 'animated',\n",
       " 'aniseed',\n",
       " 'annodated',\n",
       " 'annulated',\n",
       " 'anomaliped',\n",
       " 'anserated',\n",
       " 'anteflected',\n",
       " 'anteflexed',\n",
       " 'antimoniated',\n",
       " 'antimoniureted',\n",
       " 'antimoniuretted',\n",
       " 'antiquated',\n",
       " 'antired',\n",
       " 'antiweed',\n",
       " 'antlered',\n",
       " 'apertured',\n",
       " 'apexed',\n",
       " 'apicifixed',\n",
       " 'apiculated',\n",
       " 'apocopated',\n",
       " 'apostrophied',\n",
       " 'appearanced',\n",
       " 'appellatived',\n",
       " 'appendaged',\n",
       " 'appendiculated',\n",
       " 'applied',\n",
       " 'appressed',\n",
       " 'aralkylated',\n",
       " 'arbored',\n",
       " 'arched',\n",
       " 'architraved',\n",
       " 'arcked',\n",
       " 'arcuated',\n",
       " 'ared',\n",
       " 'areolated',\n",
       " 'ariled',\n",
       " 'arillated',\n",
       " 'armchaired',\n",
       " 'armed',\n",
       " 'armied',\n",
       " 'armillated',\n",
       " 'armored',\n",
       " 'armoried',\n",
       " 'arpeggiated',\n",
       " 'arpeggioed',\n",
       " 'arrased',\n",
       " 'arrowed',\n",
       " 'arrowheaded',\n",
       " 'arrowweed',\n",
       " 'arseneted',\n",
       " 'arsenetted',\n",
       " 'arseniureted',\n",
       " 'articled',\n",
       " 'articulated',\n",
       " 'ashamed',\n",
       " 'ashlared',\n",
       " 'ashweed',\n",
       " 'aspersed',\n",
       " 'asphyxied',\n",
       " 'assented',\n",
       " 'assessed',\n",
       " 'assigned',\n",
       " 'assistanted',\n",
       " 'associated',\n",
       " 'assonanced',\n",
       " 'assorted',\n",
       " 'assumed',\n",
       " 'assured',\n",
       " 'asteriated',\n",
       " 'astonied',\n",
       " 'aswooned',\n",
       " 'atrophiated',\n",
       " 'atrophied',\n",
       " 'attached',\n",
       " 'attired',\n",
       " 'attrited',\n",
       " 'augmented',\n",
       " 'aurated',\n",
       " 'auricled',\n",
       " 'auriculated',\n",
       " 'authorized',\n",
       " 'autoinhibited',\n",
       " 'autosensitized',\n",
       " 'autosled',\n",
       " 'averted',\n",
       " 'avowed',\n",
       " 'awearied',\n",
       " 'awned',\n",
       " 'awninged',\n",
       " 'axed',\n",
       " 'axhammered',\n",
       " 'axised',\n",
       " 'axled',\n",
       " 'axseed',\n",
       " 'axweed',\n",
       " 'azoted',\n",
       " 'azured',\n",
       " 'babied',\n",
       " 'babished',\n",
       " 'babyfied',\n",
       " 'baccated',\n",
       " 'backboned',\n",
       " 'backed',\n",
       " 'backhanded',\n",
       " 'backwatered',\n",
       " 'baconweed',\n",
       " 'badgerweed',\n",
       " 'bagged',\n",
       " 'bagwigged',\n",
       " 'baked',\n",
       " 'balanced',\n",
       " 'balconied',\n",
       " 'baldachined',\n",
       " 'baldricked',\n",
       " 'balled',\n",
       " 'ballweed',\n",
       " 'balsamweed',\n",
       " 'balustered',\n",
       " 'balustraded',\n",
       " 'bandannaed',\n",
       " 'banded',\n",
       " 'bandoleered',\n",
       " 'bangled',\n",
       " 'banked',\n",
       " 'bankweed',\n",
       " 'bannered',\n",
       " 'barbated',\n",
       " 'barbed',\n",
       " 'barebacked',\n",
       " 'bareboned',\n",
       " 'barefaced',\n",
       " 'barefooted',\n",
       " 'barehanded',\n",
       " 'bareheaded',\n",
       " 'barelegged',\n",
       " 'barenecked',\n",
       " 'barmybrained',\n",
       " 'barred',\n",
       " 'barreled',\n",
       " 'bartizaned',\n",
       " 'basebred',\n",
       " 'based',\n",
       " 'basehearted',\n",
       " 'basifixed',\n",
       " 'basilweed',\n",
       " 'basined',\n",
       " 'basinerved',\n",
       " 'basqued',\n",
       " 'bastioned',\n",
       " 'bated',\n",
       " 'bathroomed',\n",
       " 'battered',\n",
       " 'batteried',\n",
       " 'battled',\n",
       " 'battlemented',\n",
       " 'bayed',\n",
       " 'bayoneted',\n",
       " 'beached',\n",
       " 'beaded',\n",
       " 'beaked',\n",
       " 'bealtared',\n",
       " 'beamed',\n",
       " 'beanweed',\n",
       " 'beaproned',\n",
       " 'bearded',\n",
       " 'beautied',\n",
       " 'beavered',\n",
       " 'beballed',\n",
       " 'bebannered',\n",
       " 'bebed',\n",
       " 'bebelted',\n",
       " 'bebled',\n",
       " 'bebothered',\n",
       " 'bebouldered',\n",
       " 'bebuttoned',\n",
       " 'becassocked',\n",
       " 'bechained',\n",
       " 'bechignoned',\n",
       " 'becircled',\n",
       " 'becoiffed',\n",
       " 'becombed',\n",
       " 'becousined',\n",
       " 'becrinolined',\n",
       " 'becuffed',\n",
       " 'becurtained',\n",
       " 'becushioned',\n",
       " 'bed',\n",
       " 'bedaggered',\n",
       " 'bedangled',\n",
       " 'bedded',\n",
       " 'bediademed',\n",
       " 'bediamonded',\n",
       " 'beedged',\n",
       " 'beefheaded',\n",
       " 'beeheaded',\n",
       " 'beeswinged',\n",
       " 'beetled',\n",
       " 'beetleheaded',\n",
       " 'beetleweed',\n",
       " 'beeweed',\n",
       " 'befamilied',\n",
       " 'befanned',\n",
       " 'befathered',\n",
       " 'beferned',\n",
       " 'befetished',\n",
       " 'befezzed',\n",
       " 'befilleted',\n",
       " 'befilmed',\n",
       " 'beforested',\n",
       " 'befountained',\n",
       " 'befrocked',\n",
       " 'befrogged',\n",
       " 'befurbelowed',\n",
       " 'befurred',\n",
       " 'begabled',\n",
       " 'begarlanded',\n",
       " 'begartered',\n",
       " 'beggarweed',\n",
       " 'beglobed',\n",
       " 'begoggled',\n",
       " 'begowned',\n",
       " 'behatted',\n",
       " 'behaviored',\n",
       " 'beheadlined',\n",
       " 'behooped',\n",
       " 'beinked',\n",
       " 'bekilted',\n",
       " 'beknived',\n",
       " 'beknotted',\n",
       " 'belaced',\n",
       " 'belated',\n",
       " 'belatticed',\n",
       " 'belavendered',\n",
       " 'beledgered',\n",
       " 'belfried',\n",
       " 'beliked',\n",
       " 'belimousined',\n",
       " 'belled',\n",
       " 'bellied',\n",
       " 'bellmouthed',\n",
       " 'bellweed',\n",
       " 'beloved',\n",
       " 'belozenged',\n",
       " 'belted',\n",
       " 'bemazed',\n",
       " 'bemedaled',\n",
       " 'bemedalled',\n",
       " 'bemitered',\n",
       " 'bemitred',\n",
       " 'bemused',\n",
       " 'bemuslined',\n",
       " 'bended',\n",
       " 'beneaped',\n",
       " 'beneficed',\n",
       " 'beneighbored',\n",
       " 'benempted',\n",
       " 'benighted',\n",
       " 'bennetweed',\n",
       " 'benumbed',\n",
       " 'benweed',\n",
       " 'benzoated',\n",
       " 'benzoinated',\n",
       " 'bepastured',\n",
       " 'bepatched',\n",
       " 'beperiwigged',\n",
       " 'bepewed',\n",
       " 'bepillared',\n",
       " 'bepistoled',\n",
       " 'beplaided',\n",
       " 'beplumed',\n",
       " 'beribanded',\n",
       " 'beribboned',\n",
       " 'beringed',\n",
       " 'beringleted',\n",
       " 'berobed',\n",
       " 'berouged',\n",
       " 'berried',\n",
       " 'berthed',\n",
       " 'beruffed',\n",
       " 'beruffled',\n",
       " 'beshawled',\n",
       " 'besieged',\n",
       " 'beslushed',\n",
       " 'besotted',\n",
       " 'bespecked',\n",
       " 'bespectacled',\n",
       " 'besped',\n",
       " 'bespeed',\n",
       " 'bespelled',\n",
       " 'bespurred',\n",
       " 'bestatued',\n",
       " 'bestayed',\n",
       " 'bestrapped',\n",
       " 'bestubbled',\n",
       " 'besweatered',\n",
       " 'betattered',\n",
       " 'betaxed',\n",
       " 'betowered',\n",
       " 'betrothed',\n",
       " 'betrousered',\n",
       " 'betted',\n",
       " 'betuckered',\n",
       " 'beturbaned',\n",
       " 'betusked',\n",
       " 'betutored',\n",
       " 'betwattled',\n",
       " 'beuniformed',\n",
       " 'beveled',\n",
       " 'bevelled',\n",
       " 'bevesseled',\n",
       " 'bevesselled',\n",
       " 'bevined',\n",
       " 'bevoiled',\n",
       " 'bewaitered',\n",
       " 'bewhiskered',\n",
       " 'bewigged',\n",
       " 'bewildered',\n",
       " 'bewinged',\n",
       " 'bewired',\n",
       " 'bewrathed',\n",
       " 'biangulated',\n",
       " 'biarcuated',\n",
       " 'biarticulated',\n",
       " 'bicarbureted',\n",
       " 'biciliated',\n",
       " 'bicolored',\n",
       " 'bicorned',\n",
       " 'bidented',\n",
       " 'bifanged',\n",
       " 'bifidated',\n",
       " 'biflected',\n",
       " 'biforked',\n",
       " 'biformed',\n",
       " 'bifronted',\n",
       " 'bifurcated',\n",
       " 'bigeminated',\n",
       " 'bighearted',\n",
       " 'bigmouthed',\n",
       " 'bigoted',\n",
       " 'bigwigged',\n",
       " 'bilamellated',\n",
       " 'bilaminated',\n",
       " 'billed',\n",
       " 'bilobated',\n",
       " 'bilobed',\n",
       " 'bilsted',\n",
       " 'bimaculated',\n",
       " 'bimotored',\n",
       " 'bindweed',\n",
       " 'bineweed',\n",
       " 'binominated',\n",
       " 'binucleated',\n",
       " 'biparted',\n",
       " 'bipectinated',\n",
       " 'biped',\n",
       " 'bipennated',\n",
       " 'bipinnated',\n",
       " 'bipinnatiparted',\n",
       " 'bipinnatisected',\n",
       " 'biradiated',\n",
       " 'birdmouthed',\n",
       " 'birdseed',\n",
       " 'birdweed',\n",
       " 'birostrated',\n",
       " 'birthbed',\n",
       " 'bisexed',\n",
       " 'bishopweed',\n",
       " 'bistered',\n",
       " 'bistipuled',\n",
       " 'bisubstituted',\n",
       " 'bitted',\n",
       " 'bitterhearted',\n",
       " 'bitterweed',\n",
       " 'bituberculated',\n",
       " 'bitumed',\n",
       " 'bivalved',\n",
       " 'bivaulted',\n",
       " 'bivocalized',\n",
       " 'blackhearted',\n",
       " 'blackseed',\n",
       " 'blackshirted',\n",
       " 'bladderseed',\n",
       " 'bladderweed',\n",
       " 'bladed',\n",
       " 'blakeberyed',\n",
       " 'blamed',\n",
       " 'blanked',\n",
       " 'blanketed',\n",
       " 'blanketweed',\n",
       " 'blasted',\n",
       " 'bleached',\n",
       " 'bleared',\n",
       " 'bleed',\n",
       " 'blended',\n",
       " 'blessed',\n",
       " 'blighted',\n",
       " 'blinded',\n",
       " 'blindfolded',\n",
       " 'blindweed',\n",
       " 'blinked',\n",
       " 'blinkered',\n",
       " 'blistered',\n",
       " 'blisterweed',\n",
       " 'blithehearted',\n",
       " 'bloated',\n",
       " 'blobbed',\n",
       " 'blocked',\n",
       " 'blockheaded',\n",
       " 'blooded',\n",
       " 'bloodied',\n",
       " 'bloodshed',\n",
       " 'bloodstained',\n",
       " 'bloodweed',\n",
       " 'blossomed',\n",
       " 'blotched',\n",
       " 'bloused',\n",
       " 'blowzed',\n",
       " 'bludgeoned',\n",
       " 'bluebelled',\n",
       " 'bluehearted',\n",
       " 'blueweed',\n",
       " 'blunderheaded',\n",
       " 'blunthearted',\n",
       " 'blurred',\n",
       " 'bobbed',\n",
       " 'bobsled',\n",
       " 'bobtailed',\n",
       " 'bodiced',\n",
       " 'bodied',\n",
       " 'boiled',\n",
       " 'boldhearted',\n",
       " 'bolectioned',\n",
       " 'boled',\n",
       " 'boleweed',\n",
       " 'bolled',\n",
       " 'bombed',\n",
       " 'bonded',\n",
       " 'boned',\n",
       " 'boneheaded',\n",
       " 'bonneted',\n",
       " 'booked',\n",
       " 'booted',\n",
       " 'bootied',\n",
       " 'boozed',\n",
       " 'bordered',\n",
       " 'bordured',\n",
       " 'bosomed',\n",
       " 'bossed',\n",
       " 'bosselated',\n",
       " 'botched',\n",
       " 'botherheaded',\n",
       " 'bothsided',\n",
       " 'bottled',\n",
       " 'bottomed',\n",
       " 'boughed',\n",
       " 'bounded',\n",
       " 'bountied',\n",
       " 'bowed',\n",
       " 'boweled',\n",
       " 'bowlegged',\n",
       " 'bowstringed',\n",
       " 'braced',\n",
       " 'braceleted',\n",
       " 'brackened',\n",
       " 'bracted',\n",
       " 'braided',\n",
       " 'brambled',\n",
       " 'branched',\n",
       " 'branded',\n",
       " 'brandied',\n",
       " 'brangled',\n",
       " 'bravehearted',\n",
       " 'brawned',\n",
       " 'brazenfaced',\n",
       " 'breasted',\n",
       " 'breastweed',\n",
       " 'breathed',\n",
       " 'brecciated',\n",
       " 'bred',\n",
       " 'breeched',\n",
       " 'breed',\n",
       " 'breviped',\n",
       " 'bridebed',\n",
       " 'brideweed',\n",
       " 'bridged',\n",
       " 'bridled',\n",
       " 'briered',\n",
       " 'brimmed',\n",
       " 'bristled',\n",
       " 'broadhearted',\n",
       " 'brocaded',\n",
       " 'brocked',\n",
       " 'brokenhearted',\n",
       " 'bromoiodized',\n",
       " 'bronzed',\n",
       " 'brooked',\n",
       " 'brookweed',\n",
       " 'broomweed',\n",
       " 'broozled',\n",
       " 'browed',\n",
       " 'brownweed',\n",
       " 'bruckled',\n",
       " 'brushed',\n",
       " 'buboed',\n",
       " 'bucked',\n",
       " 'buckled',\n",
       " 'buckskinned',\n",
       " 'buffed',\n",
       " 'bugled',\n",
       " 'bugleweed',\n",
       " 'bugseed',\n",
       " 'bugweed',\n",
       " 'bulbed',\n",
       " 'bulked',\n",
       " 'bulkheaded',\n",
       " 'bullated',\n",
       " 'bulldogged',\n",
       " 'bulleted',\n",
       " 'bulletheaded',\n",
       " 'bullheaded',\n",
       " 'bullweed',\n",
       " 'bummed',\n",
       " 'bundlerooted',\n",
       " 'bundweed',\n",
       " 'bunted',\n",
       " 'buried',\n",
       " 'burled',\n",
       " 'burned',\n",
       " 'burnoosed',\n",
       " 'burntweed',\n",
       " 'burred',\n",
       " 'burroweed',\n",
       " 'burseed',\n",
       " 'burweed',\n",
       " 'bushed',\n",
       " 'busied',\n",
       " 'busked',\n",
       " 'buskined',\n",
       " 'busted',\n",
       " 'bustled',\n",
       " 'busybodied',\n",
       " 'buttered',\n",
       " 'butterfingered',\n",
       " 'butterweed',\n",
       " 'butteryfingered',\n",
       " 'buttocked',\n",
       " 'buttoned',\n",
       " 'buttonweed',\n",
       " 'cabled',\n",
       " 'caboshed',\n",
       " 'caddiced',\n",
       " 'caddised',\n",
       " 'cadenced',\n",
       " 'cadweed',\n",
       " 'caftaned',\n",
       " 'caged',\n",
       " 'cairned',\n",
       " 'caissoned',\n",
       " 'calced',\n",
       " 'calcified',\n",
       " 'calcined',\n",
       " 'calculated',\n",
       " 'calibered',\n",
       " 'calicoed',\n",
       " 'caligated',\n",
       " 'calpacked',\n",
       " 'calved',\n",
       " 'calycled',\n",
       " 'calyculated',\n",
       " 'camailed',\n",
       " 'camerated',\n",
       " 'cammed',\n",
       " 'campanulated',\n",
       " 'campshed',\n",
       " 'camused',\n",
       " 'canaliculated',\n",
       " 'cancellated',\n",
       " 'cancered',\n",
       " 'cancerweed',\n",
       " 'candied',\n",
       " 'candlelighted',\n",
       " 'candlesticked',\n",
       " 'candyweed',\n",
       " 'canioned',\n",
       " 'cankered',\n",
       " 'cankerweed',\n",
       " 'canned',\n",
       " 'cannelated',\n",
       " 'cannelured',\n",
       " 'cannoned',\n",
       " 'cannulated',\n",
       " 'canted',\n",
       " 'cantilevered',\n",
       " 'cantoned',\n",
       " 'cantred',\n",
       " 'caped',\n",
       " 'capernoited',\n",
       " 'capeweed',\n",
       " 'capitaled',\n",
       " 'capitated',\n",
       " 'capped',\n",
       " 'capriped',\n",
       " 'capsulated',\n",
       " 'capuched',\n",
       " 'carapaced',\n",
       " 'carbolated',\n",
       " 'carboyed',\n",
       " 'carbuncled',\n",
       " 'carcaneted',\n",
       " 'carded',\n",
       " 'carinated',\n",
       " 'carkled',\n",
       " 'carnaged',\n",
       " 'carnationed',\n",
       " 'carpetweed',\n",
       " 'carried',\n",
       " 'carrotweed',\n",
       " 'carucated',\n",
       " 'carunculated',\n",
       " 'cased',\n",
       " 'casemated',\n",
       " 'casemented',\n",
       " 'caseweed',\n",
       " 'casqued',\n",
       " 'castellated',\n",
       " 'castled',\n",
       " 'castorized',\n",
       " 'catamited',\n",
       " 'cataracted',\n",
       " 'catarrhed',\n",
       " 'catchweed',\n",
       " 'catenated',\n",
       " 'caterpillared',\n",
       " 'catfaced',\n",
       " 'catfooted',\n",
       " 'cathedraled',\n",
       " 'caudated',\n",
       " 'caverned',\n",
       " 'cavitied',\n",
       " 'cayenned',\n",
       " 'cedared',\n",
       " 'ceilinged',\n",
       " 'celebrated',\n",
       " 'cellated',\n",
       " 'celled',\n",
       " 'cellulated',\n",
       " 'celluloided',\n",
       " 'centered',\n",
       " 'centriffed',\n",
       " 'centuried',\n",
       " 'cerated',\n",
       " 'cered',\n",
       " 'certified',\n",
       " 'chafeweed',\n",
       " 'chaffseed',\n",
       " 'chaffweed',\n",
       " 'chafted',\n",
       " 'chained',\n",
       " 'chaliced',\n",
       " 'chambered',\n",
       " 'chamberleted',\n",
       " 'chamberletted',\n",
       " 'chanceled',\n",
       " 'channeled',\n",
       " 'channelled',\n",
       " 'chaped',\n",
       " 'chapleted',\n",
       " 'chapournetted',\n",
       " 'chapped',\n",
       " 'charioted',\n",
       " 'charqued',\n",
       " 'chartered',\n",
       " 'chasmed',\n",
       " 'chasteweed',\n",
       " 'chasubled',\n",
       " 'checked',\n",
       " 'checkered',\n",
       " 'checkrowed',\n",
       " 'cheered',\n",
       " 'cheliped',\n",
       " 'cherried',\n",
       " 'chickenbreasted',\n",
       " 'chickenhearted',\n",
       " 'chickenweed',\n",
       " 'chickweed',\n",
       " 'chicqued',\n",
       " 'chiggerweed',\n",
       " 'chignoned',\n",
       " 'childbed',\n",
       " 'childed',\n",
       " 'chilled',\n",
       " 'chined',\n",
       " 'chinned',\n",
       " 'chipped',\n",
       " 'chiseled',\n",
       " 'chitinized',\n",
       " 'chokered',\n",
       " 'chokeweed',\n",
       " 'cholterheaded',\n",
       " 'chopped',\n",
       " 'choppered',\n",
       " 'chorded',\n",
       " 'chowderheaded',\n",
       " 'christened',\n",
       " 'chubbed',\n",
       " 'chuckleheaded',\n",
       " 'churchified',\n",
       " 'churled',\n",
       " 'ciliated',\n",
       " 'cingulated',\n",
       " 'cinnamoned',\n",
       " 'cinquefoiled',\n",
       " 'circled',\n",
       " 'circumscribed',\n",
       " 'circumstanced',\n",
       " 'cirrated',\n",
       " 'cirrhosed',\n",
       " 'cirriped',\n",
       " 'cisted',\n",
       " 'citied',\n",
       " 'citified',\n",
       " 'citrated',\n",
       " 'civilized',\n",
       " 'clammed',\n",
       " 'clammyweed',\n",
       " 'clanned',\n",
       " 'clapped',\n",
       " 'classed',\n",
       " 'classified',\n",
       " 'clavated',\n",
       " 'clavellated',\n",
       " 'clawed',\n",
       " 'claybrained',\n",
       " 'clayweed',\n",
       " 'cleaded',\n",
       " 'cleanhanded',\n",
       " 'cleanhearted',\n",
       " 'clearheaded',\n",
       " 'clearhearted',\n",
       " 'clearweed',\n",
       " 'cled',\n",
       " 'cleeked',\n",
       " 'clefted',\n",
       " 'clerestoried',\n",
       " 'cliented',\n",
       " 'cliffed',\n",
       " 'cliffweed',\n",
       " 'clipped',\n",
       " 'cloaked',\n",
       " 'clocked',\n",
       " 'clodpated',\n",
       " 'cloistered',\n",
       " 'closed',\n",
       " 'closefisted',\n",
       " 'closehanded',\n",
       " 'closehearted',\n",
       " 'closemouthed',\n",
       " 'clotweed',\n",
       " 'clouded',\n",
       " 'clouted',\n",
       " 'clovered',\n",
       " 'clubbed',\n",
       " 'clubfisted',\n",
       " 'clubfooted',\n",
       " 'clubweed',\n",
       " 'clustered',\n",
       " 'coaged',\n",
       " 'coaggregated',\n",
       " 'coated',\n",
       " 'coattailed',\n",
       " 'cobbed',\n",
       " 'cocashweed',\n",
       " 'cochleated',\n",
       " 'cockaded',\n",
       " 'cocked',\n",
       " 'cockeyed',\n",
       " 'cockled',\n",
       " 'cockneybred',\n",
       " 'cockscombed',\n",
       " 'cockweed',\n",
       " 'codheaded',\n",
       " 'coed',\n",
       " 'coelongated',\n",
       " 'coembedded',\n",
       " 'coequated',\n",
       " 'coexpanded',\n",
       " 'coffeeweed',\n",
       " 'cogged',\n",
       " 'coifed',\n",
       " 'coiled',\n",
       " 'coldhearted',\n",
       " 'coleseed',\n",
       " 'colicweed',\n",
       " 'collared',\n",
       " 'collected',\n",
       " 'collied',\n",
       " 'colloped',\n",
       " 'colonnaded',\n",
       " 'colored',\n",
       " 'columnated',\n",
       " 'columned',\n",
       " 'combed',\n",
       " 'combined',\n",
       " 'compacted',\n",
       " 'complected',\n",
       " 'complexioned',\n",
       " 'complicated',\n",
       " 'componed',\n",
       " 'componented',\n",
       " 'composed',\n",
       " 'compressed',\n",
       " 'comprised',\n",
       " 'compulsed',\n",
       " 'conamed',\n",
       " 'concamerated',\n",
       " 'concealed',\n",
       " 'conceded',\n",
       " 'conceited',\n",
       " 'concentrated',\n",
       " 'concerned',\n",
       " 'concerted',\n",
       " 'conched',\n",
       " 'conchyliated',\n",
       " 'condemned',\n",
       " 'condensed',\n",
       " 'conditioned',\n",
       " 'conduplicated',\n",
       " 'coned',\n",
       " 'confated',\n",
       " 'conferted',\n",
       " 'confined',\n",
       " 'confirmed',\n",
       " 'conflated',\n",
       " 'confounded',\n",
       " 'confused',\n",
       " 'congested',\n",
       " 'conjoined',\n",
       " 'conjugated',\n",
       " 'connected',\n",
       " 'conred',\n",
       " 'consecrated',\n",
       " 'considered',\n",
       " 'consolidated',\n",
       " 'constrained',\n",
       " 'constricted',\n",
       " 'consumpted',\n",
       " 'contagioned',\n",
       " 'contented',\n",
       " 'contextured',\n",
       " 'continued',\n",
       " 'contorted',\n",
       " 'contortioned',\n",
       " 'contracted',\n",
       " 'contractured',\n",
       " 'contusioned',\n",
       " 'converted',\n",
       " 'convexed',\n",
       " 'convinced',\n",
       " 'convoluted',\n",
       " 'coolheaded',\n",
       " 'coolweed',\n",
       " 'copied',\n",
       " 'copleased',\n",
       " 'copped',\n",
       " 'coppernosed',\n",
       " 'copperytailed',\n",
       " 'coppiced',\n",
       " 'coppled',\n",
       " 'copsewooded',\n",
       " 'copygraphed',\n",
       " 'coraled',\n",
       " 'corded',\n",
       " 'corduroyed',\n",
       " 'cored',\n",
       " 'coreflexed',\n",
       " 'corked',\n",
       " 'cornered',\n",
       " 'cornified',\n",
       " 'cornuated',\n",
       " 'cornuted',\n",
       " 'corollated',\n",
       " 'coronaled',\n",
       " 'coronated',\n",
       " 'coroneted',\n",
       " 'coronetted',\n",
       " 'corpusculated',\n",
       " 'corrected',\n",
       " 'correlated',\n",
       " 'corridored',\n",
       " ...]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wordlist if re.search('ed$', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 8 글자 단어 중 3번째가 j 이고 6번째가 t인 단어 찾기 \n",
    "- `.`: any single character\n",
    "- `^`: start of a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abjectly',\n",
       " 'adjuster',\n",
       " 'dejected',\n",
       " 'dejectly',\n",
       " 'injector',\n",
       " 'majestic',\n",
       " 'objectee',\n",
       " 'objector',\n",
       " 'rejecter',\n",
       " 'rejector',\n",
       " 'unjilted',\n",
       " 'unjolted',\n",
       " 'unjustly']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wordlist if re.search('^..j..t..$', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:crimson\">[연습😉1]</span> 위의 예에서 `^` 기호를 없앤다면 결과는?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `?`: 앞의 글자는 option이라는 의미. (예) «^e-?mail$» 는 'email'과 'e-mail' 모두 일치한다. \n",
    "- 다음과 같은 코드로 이러한 단어의 총 빈도수를 셀수 있다.\n",
    "```python\n",
    "sum(1 for w in text if re.search('^e-?mail$', w))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 범위(range) 및 클로저(closure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/9key_pad.png\" width=\"200\" style=\"margin-left: auto; margin-right: auto\">\n",
    "<p style=\"text-align: center;\">키패드를 사용한 텍스트 입력</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **textonyms**: 같은 keystroke 순서로 입력되는 둘 또는 그 이상의 단어들\n",
    "- (예) hole과 gold의 키 순서는 4653\n",
    "- 이와 같은 키 순서의 다른 단어를 찾아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gold', 'golf', 'hold', 'hole']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:crimson\">[연습😉2]</span> 숫자 패드의 일부분만 사용하는 단어인 \"finger-twister\"를 찾아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예를 들어, «^\\[ghijklmno\\]\\+\\\\$» 또는 더 축약해서 «^[g-o]+\\\\$»는 가운데 행의 4, 5, 6 키만 사용하는 단어와 match할 것이다. «^[a-fj-o]+$» 는 2, 3, 5, 6 키들만 사용하는 단어들과 매치된다. 추가로, 여기서 `-` 와 `+`는 무슨 의미인가? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `+`: 바로 앞의 아이템이 하나 또는 그 이상 반복됨\n",
    "- `*`: 바로 앞의 아이템이 제로 또는 그 이상 반복됨\n",
    "- 이 두 기호를 <span style=\"color:blue\">**Kleene closures**</span> 또는 단순히 <span style=\"color: blue\">**closures**</span> 라고 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '!',\n",
       " '!!',\n",
       " '!!!',\n",
       " '!!!!',\n",
       " '!!!!!',\n",
       " '!!!!!!',\n",
       " '!!!!!!!',\n",
       " '!!!!!!!!',\n",
       " '!!!!!!!!!',\n",
       " '!!!!!!!!!!',\n",
       " '!!!!!!!!!!!',\n",
       " '!!!!!!!!!!!!!',\n",
       " '!!!!!!!!!!!!!!!!',\n",
       " '!!!!!!!!!!!!!!!!!!!!!!',\n",
       " '!!!!!!!!!!!!!!!!!!!!!!!',\n",
       " '!!!!!!!!!!!!!!!!!!!!!!!!!!!',\n",
       " '!!!!!!!!!!!!!!!!!!!!!!!!!!!!',\n",
       " '!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!',\n",
       " '!!!!!!.',\n",
       " '!!!!!.',\n",
       " '!!!!....',\n",
       " '!!!.',\n",
       " '!!.',\n",
       " '!!...',\n",
       " '!.',\n",
       " '!...',\n",
       " '!=',\n",
       " '!?',\n",
       " '!??',\n",
       " '!???',\n",
       " '\"',\n",
       " '\"...',\n",
       " '\"?',\n",
       " '\"s',\n",
       " '#',\n",
       " '###',\n",
       " '####',\n",
       " '#14-19teens',\n",
       " '#40sPlus',\n",
       " '#prideIsland',\n",
       " '#prideisland',\n",
       " '#talkcity-20s',\n",
       " '#talkcity_adults',\n",
       " '$',\n",
       " '$$',\n",
       " '$27',\n",
       " '&',\n",
       " '&^',\n",
       " \"'\",\n",
       " \"''\",\n",
       " \"'.\",\n",
       " \"'d\",\n",
       " \"'ello\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'n'\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " '(',\n",
       " '( o Y o )',\n",
       " '(((',\n",
       " '((((',\n",
       " '(((((',\n",
       " '((((((',\n",
       " '(((((((',\n",
       " '((((((((',\n",
       " '(((((((((',\n",
       " '((((((((((',\n",
       " '(((((((((((',\n",
       " '((((((((((((',\n",
       " '(((((((((((((',\n",
       " '((((((((((((((',\n",
       " '(((((((((((((((',\n",
       " '(((((((((((((((((',\n",
       " '((((((((((((((((((',\n",
       " '((((((((((((((((((((',\n",
       " '(((((((((((((((((((((',\n",
       " '(((((((((((((((((((((((',\n",
       " '((((((((((((((((((((((((',\n",
       " '(((((((((((((((((((((((((',\n",
       " '((((((((((((((((((((((((((',\n",
       " '(((((..',\n",
       " '(*&(^',\n",
       " '(.',\n",
       " '(__I__)',\n",
       " ')',\n",
       " ')))',\n",
       " '))))',\n",
       " ')))))',\n",
       " ')))))))',\n",
       " '))))))))',\n",
       " ')))))))))',\n",
       " '))))))))))',\n",
       " ')))))))))))',\n",
       " '))))))))))))',\n",
       " ')))))))))))))',\n",
       " '))))))))))))))',\n",
       " ')))))))))))))))',\n",
       " ')))))))))))))))))',\n",
       " ')))))))))))))))))))',\n",
       " ')))))))))))))))))))))',\n",
       " '))))))))))))))))))))))',\n",
       " '))))))))))))))))))))))))))))',\n",
       " ')))))))))))))))))))))))))))))))',\n",
       " ')?',\n",
       " '*',\n",
       " '******',\n",
       " '*VBS*',\n",
       " '*WOW*',\n",
       " '*blush*',\n",
       " '*drools*',\n",
       " '*grins*',\n",
       " '*hugs*',\n",
       " '*smewchies*',\n",
       " '*sniffs*',\n",
       " '*spank*',\n",
       " '*waves*',\n",
       " '+',\n",
       " '+*+*+*+*',\n",
       " '++',\n",
       " ',',\n",
       " ',,',\n",
       " ',,,',\n",
       " ',,,,',\n",
       " ',,,,,',\n",
       " ',,,,,,,',\n",
       " ',,,,,,,,,,,',\n",
       " '-',\n",
       " '-(',\n",
       " '--',\n",
       " '-------------',\n",
       " '--------------------',\n",
       " '--------->',\n",
       " '-->',\n",
       " '-...)...-',\n",
       " '-17',\n",
       " '-21',\n",
       " '-6',\n",
       " '-_-',\n",
       " '-o',\n",
       " '-s',\n",
       " '-stay-',\n",
       " '.',\n",
       " '. .',\n",
       " '. . .',\n",
       " '. ...',\n",
       " '.(.',\n",
       " '.(..(.vMp3 v1.7.4.).)',\n",
       " '.(..(.vMp3 vi.p.t.)..).',\n",
       " '.)',\n",
       " '.).',\n",
       " '..',\n",
       " '.. .',\n",
       " '..(..',\n",
       " '...',\n",
       " '....',\n",
       " '.....',\n",
       " '......',\n",
       " '.......',\n",
       " '........',\n",
       " '.........',\n",
       " '..........',\n",
       " '...........',\n",
       " '............',\n",
       " '.............',\n",
       " '................',\n",
       " '..................',\n",
       " '...................',\n",
       " '....................',\n",
       " '........................',\n",
       " '..............................',\n",
       " '.45',\n",
       " '.:',\n",
       " '.;)',\n",
       " '.A.n.a.c.?.n.?.a.',\n",
       " '.op.',\n",
       " '.owner.',\n",
       " '/',\n",
       " '//',\n",
       " '//www.wunderground.com/cgi-bin/findweather/getForecast?query=95953#FIR',\n",
       " '0',\n",
       " '05.',\n",
       " '06.',\n",
       " '1',\n",
       " '1-900-anal-sex',\n",
       " '1.98',\n",
       " '1.99',\n",
       " '10',\n",
       " '100',\n",
       " '100%',\n",
       " '1012.',\n",
       " '1016.',\n",
       " '102.6',\n",
       " '10:49',\n",
       " '10th',\n",
       " '11',\n",
       " '12',\n",
       " '12%',\n",
       " '1200',\n",
       " '121.7',\n",
       " '1299',\n",
       " '13',\n",
       " '138',\n",
       " '14',\n",
       " '14-16',\n",
       " '147.7',\n",
       " '15',\n",
       " '16',\n",
       " '16.',\n",
       " '17',\n",
       " '18',\n",
       " '185',\n",
       " '18ST',\n",
       " '19',\n",
       " '1900',\n",
       " '1930',\n",
       " '1980',\n",
       " '1985',\n",
       " '1996',\n",
       " '1cos',\n",
       " '2',\n",
       " '2.3',\n",
       " '20',\n",
       " '20.',\n",
       " '2006',\n",
       " '20S',\n",
       " '20s',\n",
       " '21',\n",
       " '22',\n",
       " '220',\n",
       " '224',\n",
       " '23',\n",
       " '24',\n",
       " '246',\n",
       " '247',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '280',\n",
       " '28147',\n",
       " '29',\n",
       " '29.88.',\n",
       " '295',\n",
       " '29803',\n",
       " '2:55',\n",
       " '2DAY',\n",
       " '2Pac',\n",
       " '2nd',\n",
       " '3',\n",
       " '30',\n",
       " '30.',\n",
       " '30.00.',\n",
       " '300',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '3333333',\n",
       " '33982',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '360',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '39.3',\n",
       " '396',\n",
       " '3:45',\n",
       " '3~<-..4@.',\n",
       " '4',\n",
       " '4.20',\n",
       " '41',\n",
       " '423',\n",
       " '43',\n",
       " '43.',\n",
       " '45',\n",
       " '45.5',\n",
       " '453',\n",
       " '46',\n",
       " '46.',\n",
       " '47',\n",
       " '47.',\n",
       " '49',\n",
       " '4:03',\n",
       " '5',\n",
       " '50',\n",
       " '51',\n",
       " '53',\n",
       " '55',\n",
       " '55%',\n",
       " '55.',\n",
       " '56',\n",
       " '56.',\n",
       " '57',\n",
       " '57401',\n",
       " '579',\n",
       " '59',\n",
       " '59%',\n",
       " '6',\n",
       " '60',\n",
       " '60s',\n",
       " '64.8',\n",
       " '65%',\n",
       " '68%',\n",
       " '69',\n",
       " '6:38',\n",
       " '6:41',\n",
       " '6:51',\n",
       " '6:53',\n",
       " '7',\n",
       " '70%',\n",
       " '700',\n",
       " '73%',\n",
       " '73042',\n",
       " '75',\n",
       " '75%',\n",
       " '76%',\n",
       " '77',\n",
       " '7:45',\n",
       " '8',\n",
       " '80',\n",
       " '8082653953',\n",
       " '818',\n",
       " '85%',\n",
       " '9',\n",
       " '9.53',\n",
       " '90',\n",
       " '92129',\n",
       " '92780',\n",
       " '93',\n",
       " '93%',\n",
       " '95953',\n",
       " '98.5',\n",
       " '98.6',\n",
       " '99',\n",
       " '99701',\n",
       " '99703',\n",
       " '9:10',\n",
       " ':',\n",
       " ':(',\n",
       " ':)',\n",
       " ':):):)',\n",
       " ':-(',\n",
       " ':-)',\n",
       " ':-@',\n",
       " ':-o',\n",
       " ':.',\n",
       " ':/',\n",
       " ':@',\n",
       " ':D',\n",
       " ':O',\n",
       " ':P',\n",
       " ':]',\n",
       " ':beer:',\n",
       " ':blush:',\n",
       " ':love:',\n",
       " ':o *',\n",
       " ':p',\n",
       " ':tongue:',\n",
       " ':|',\n",
       " ';',\n",
       " '; ..',\n",
       " ';)',\n",
       " ';-(',\n",
       " ';-)',\n",
       " ';0',\n",
       " ';]',\n",
       " ';p',\n",
       " '<',\n",
       " '<,',\n",
       " '<-',\n",
       " '<--',\n",
       " '<---',\n",
       " '<----',\n",
       " '<----------',\n",
       " '<3',\n",
       " \"<3's\",\n",
       " '<33',\n",
       " '<333',\n",
       " '<3333',\n",
       " '<33333',\n",
       " '<333333333',\n",
       " '<3333333333333333',\n",
       " '<33333333333333333',\n",
       " '<<',\n",
       " '<<<',\n",
       " '<<<<',\n",
       " '<<<<,',\n",
       " '<<<<<',\n",
       " '<<<<<<',\n",
       " '<<<<<<,',\n",
       " '<<<<<<<',\n",
       " '<<<<<<<<<<<<<<',\n",
       " '<empty>',\n",
       " '<perk>',\n",
       " '<~~~',\n",
       " '=',\n",
       " \"='s\",\n",
       " '=(',\n",
       " '=)',\n",
       " '=-\\\\',\n",
       " '=/',\n",
       " '=D',\n",
       " '=O',\n",
       " '=[',\n",
       " '=]',\n",
       " '=p',\n",
       " '>',\n",
       " '>.>',\n",
       " '>.>->',\n",
       " '>:->',\n",
       " '>>>',\n",
       " '>>>>>>>>>>',\n",
       " '>>>>>>>>>>>',\n",
       " '>>>>>>>>>>>>',\n",
       " '>?',\n",
       " '>_>',\n",
       " '?',\n",
       " '?!',\n",
       " '?!?!',\n",
       " '?!?!?',\n",
       " \"?'\",\n",
       " '?.',\n",
       " '?..',\n",
       " '?....',\n",
       " '??',\n",
       " '??!!',\n",
       " '??!?!??!',\n",
       " '???',\n",
       " '????',\n",
       " '?????',\n",
       " '??????',\n",
       " '???????',\n",
       " '????????',\n",
       " '?????????',\n",
       " '??@',\n",
       " '@',\n",
       " '@$$',\n",
       " \"@-,'~\",\n",
       " \"@..3-,'~.\",\n",
       " 'A',\n",
       " 'ABOUT',\n",
       " 'ACTION',\n",
       " 'AFK',\n",
       " 'AGAIN',\n",
       " 'AHAHH',\n",
       " 'AHAHHA',\n",
       " 'AHHAH',\n",
       " 'AI',\n",
       " 'AKDT',\n",
       " 'AKST',\n",
       " 'ALL',\n",
       " 'AM',\n",
       " 'AND',\n",
       " 'ANY',\n",
       " 'ANYONE',\n",
       " 'AOL.COM',\n",
       " 'ARE',\n",
       " 'AROUND',\n",
       " 'ASS',\n",
       " 'AWAY',\n",
       " 'Aberdeen',\n",
       " 'About',\n",
       " 'Ack',\n",
       " 'Actually',\n",
       " 'Added',\n",
       " 'Advisory',\n",
       " 'Again',\n",
       " 'Ah',\n",
       " 'Ahh',\n",
       " 'Ahhh',\n",
       " 'Ahhhh',\n",
       " 'Aiken',\n",
       " 'Alaska',\n",
       " 'Albany',\n",
       " 'Almost',\n",
       " 'Always',\n",
       " 'Amazingness',\n",
       " 'American',\n",
       " 'Americans',\n",
       " 'Amy',\n",
       " 'An',\n",
       " 'And',\n",
       " 'Any',\n",
       " 'Anyone',\n",
       " 'Anyway',\n",
       " 'Apocalypse',\n",
       " 'Apparently',\n",
       " 'Are',\n",
       " 'Ark',\n",
       " 'Arkansas',\n",
       " 'As',\n",
       " 'Ask',\n",
       " 'At',\n",
       " 'Average',\n",
       " 'Aw',\n",
       " 'Away',\n",
       " 'Aww',\n",
       " 'Awww',\n",
       " 'B',\n",
       " 'BAAAAALLLLLLLLIIIIIIINNNNNNNNNNN',\n",
       " 'BE',\n",
       " 'BIG',\n",
       " 'BLONDES',\n",
       " 'BOOTS',\n",
       " 'BOOTY',\n",
       " 'BOY',\n",
       " 'BUT',\n",
       " 'BUt',\n",
       " 'BYE',\n",
       " 'Back',\n",
       " 'Barbieee',\n",
       " 'Barometer',\n",
       " 'Beach',\n",
       " 'Because',\n",
       " 'Been',\n",
       " 'Ben',\n",
       " 'Benjamin',\n",
       " 'Better',\n",
       " 'Bible',\n",
       " 'Biiiiiitch',\n",
       " 'Biographys',\n",
       " 'Birdgang',\n",
       " 'Bloooooooood',\n",
       " 'Bloooooooooood',\n",
       " 'Bloooooooooooood',\n",
       " 'Bone',\n",
       " 'Bonus',\n",
       " 'Books',\n",
       " 'Boone',\n",
       " 'Booyah',\n",
       " 'Borat',\n",
       " 'Born',\n",
       " 'Box',\n",
       " 'Boyz',\n",
       " 'Break',\n",
       " 'Breaking',\n",
       " 'Broken',\n",
       " 'Bud',\n",
       " 'Burger',\n",
       " 'But',\n",
       " 'Bwhaha',\n",
       " 'Bye',\n",
       " 'C',\n",
       " 'CA',\n",
       " 'CALI',\n",
       " 'CAN',\n",
       " 'CAPS',\n",
       " 'CDT',\n",
       " 'CHAT',\n",
       " 'CHATHIDE',\n",
       " 'CHIPS',\n",
       " 'CHOCO',\n",
       " 'CO',\n",
       " 'COM',\n",
       " 'COME',\n",
       " 'CSI',\n",
       " 'CST',\n",
       " 'CT',\n",
       " 'CUZ',\n",
       " 'California',\n",
       " 'Came',\n",
       " 'Can',\n",
       " 'CanEhda',\n",
       " 'Cardinals',\n",
       " 'Cardnials',\n",
       " 'Cards',\n",
       " 'Care',\n",
       " 'Carolina',\n",
       " 'Catterick',\n",
       " 'Ceiling',\n",
       " 'Chamillionaire',\n",
       " 'Change',\n",
       " 'Changing',\n",
       " 'Chat',\n",
       " 'Check',\n",
       " 'Checked',\n",
       " 'Cheeeez',\n",
       " 'Chica',\n",
       " 'Chickens',\n",
       " 'Children',\n",
       " 'China',\n",
       " 'Chingy',\n",
       " 'Chop',\n",
       " 'Chris',\n",
       " 'Christianity',\n",
       " 'Ciara',\n",
       " 'City',\n",
       " 'Cleveland',\n",
       " 'Clock',\n",
       " 'Coincidence',\n",
       " 'Come',\n",
       " 'Compliments',\n",
       " 'Connected',\n",
       " 'Connecticutt',\n",
       " 'Considerably',\n",
       " 'Constitution',\n",
       " 'Cookies',\n",
       " 'Cool',\n",
       " 'Could',\n",
       " 'Course',\n",
       " 'Covered',\n",
       " 'Cradle',\n",
       " 'Craig',\n",
       " 'Crazy',\n",
       " 'Cream',\n",
       " 'Cry',\n",
       " 'Ct',\n",
       " 'Ctrl',\n",
       " 'Cum',\n",
       " 'Current',\n",
       " 'Cute',\n",
       " 'Cyber',\n",
       " 'D',\n",
       " 'DAMN',\n",
       " 'DAamn',\n",
       " 'DELIGHTFUL',\n",
       " 'DETROIT',\n",
       " 'DING',\n",
       " 'DIRTY',\n",
       " 'DJ',\n",
       " 'DO',\n",
       " 'DOES',\n",
       " 'DOING',\n",
       " 'DON',\n",
       " 'DONT',\n",
       " 'DOWNS',\n",
       " 'DVD',\n",
       " 'Dakota',\n",
       " 'Damn',\n",
       " 'Dang',\n",
       " 'Daniel',\n",
       " 'Daveeee',\n",
       " 'David',\n",
       " 'Dawn',\n",
       " 'Dawnstar',\n",
       " 'Days',\n",
       " 'Death',\n",
       " 'Deep',\n",
       " 'Define',\n",
       " 'Denver',\n",
       " 'Depends',\n",
       " 'Devil',\n",
       " 'Dew',\n",
       " 'Diary',\n",
       " 'Did',\n",
       " 'Diego',\n",
       " 'Dipset',\n",
       " 'Dixie',\n",
       " 'Do',\n",
       " 'Does',\n",
       " 'Doing',\n",
       " 'Dokken',\n",
       " 'Dolls',\n",
       " 'Dood',\n",
       " 'Down',\n",
       " 'Downy',\n",
       " 'Dr',\n",
       " 'Dr.',\n",
       " 'Dreams',\n",
       " 'Drew',\n",
       " 'Drive',\n",
       " 'Drop',\n",
       " 'Dude',\n",
       " 'Dustin',\n",
       " 'Dying',\n",
       " 'ELSE',\n",
       " 'ENOUGH',\n",
       " 'EST',\n",
       " 'EVEN',\n",
       " 'EVERYTHING',\n",
       " 'Earth',\n",
       " 'Easily',\n",
       " 'Eastern',\n",
       " 'Eddie',\n",
       " 'Edgewood',\n",
       " 'Eggs',\n",
       " 'Elev',\n",
       " 'Elle',\n",
       " 'End',\n",
       " 'Eticket',\n",
       " 'Evanescence',\n",
       " 'Even',\n",
       " 'Everyone',\n",
       " 'Everytime',\n",
       " 'Evil',\n",
       " 'Eyes',\n",
       " 'F',\n",
       " 'F5',\n",
       " 'FACE',\n",
       " 'FEMALE',\n",
       " 'FF',\n",
       " 'FINE',\n",
       " 'FL',\n",
       " 'FOLKS',\n",
       " 'FROM',\n",
       " 'Fade',\n",
       " 'Fails',\n",
       " 'Fairbanks',\n",
       " 'Favorite',\n",
       " 'Females',\n",
       " 'Fergalicious',\n",
       " 'Fergie',\n",
       " 'Fetish',\n",
       " 'Fighting',\n",
       " 'Figured',\n",
       " 'Filth',\n",
       " 'Finally',\n",
       " 'Finding',\n",
       " 'Finger',\n",
       " 'Fingers',\n",
       " 'First',\n",
       " 'Fisher',\n",
       " 'Fishers',\n",
       " 'Fix',\n",
       " 'Fixed',\n",
       " 'Flames',\n",
       " 'Flatts',\n",
       " 'Florida',\n",
       " 'Foley',\n",
       " 'Food',\n",
       " 'For',\n",
       " 'Fort',\n",
       " 'Foxwoods',\n",
       " 'FreesBee',\n",
       " 'Friday',\n",
       " 'From',\n",
       " 'Froogle',\n",
       " 'G',\n",
       " 'G-Mobile',\n",
       " 'GA',\n",
       " 'GIRL',\n",
       " 'GIRLS',\n",
       " 'GN',\n",
       " 'GNG',\n",
       " 'GOING',\n",
       " 'GOOD',\n",
       " 'GUYS',\n",
       " 'Gay',\n",
       " 'Geographic',\n",
       " 'Get',\n",
       " 'Ghetto',\n",
       " 'Girl',\n",
       " 'Go',\n",
       " 'God',\n",
       " 'Good',\n",
       " 'Gorda',\n",
       " 'Gosh',\n",
       " 'Gothic',\n",
       " 'Gracemont',\n",
       " 'Great',\n",
       " 'Greetings',\n",
       " 'GrlZ',\n",
       " 'Groups',\n",
       " 'Gs',\n",
       " 'Guess',\n",
       " 'Guy',\n",
       " 'H',\n",
       " 'H0rny',\n",
       " 'HAHA',\n",
       " 'HAHAHA',\n",
       " 'HALO',\n",
       " 'HAVE',\n",
       " 'HE',\n",
       " 'HELLO',\n",
       " 'HERE',\n",
       " 'HEY',\n",
       " 'HHEEYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY',\n",
       " 'HI',\n",
       " 'HOT',\n",
       " 'HOTT',\n",
       " 'HOW',\n",
       " 'HUGE',\n",
       " 'HUH',\n",
       " 'Ha',\n",
       " 'Haha',\n",
       " 'Hahaaaa',\n",
       " 'Hahhaa',\n",
       " 'Hail',\n",
       " 'Hallo',\n",
       " 'Hand',\n",
       " 'Hard',\n",
       " 'Harmony',\n",
       " 'Have',\n",
       " 'Hay',\n",
       " 'He',\n",
       " 'Hello',\n",
       " 'Help',\n",
       " 'Her',\n",
       " 'Here',\n",
       " 'Hero',\n",
       " 'Hey',\n",
       " 'Heya',\n",
       " 'Heys',\n",
       " 'Heyy',\n",
       " 'Heyyy',\n",
       " 'Heyyyyyyy',\n",
       " 'Hi',\n",
       " 'High',\n",
       " 'Highway',\n",
       " 'Hill',\n",
       " 'History',\n",
       " 'Hiya',\n",
       " 'Hmm',\n",
       " 'Hold',\n",
       " 'Holla',\n",
       " 'Holland',\n",
       " 'HolocaustYourMom',\n",
       " 'Holy',\n",
       " 'Home',\n",
       " 'Horace',\n",
       " 'Hott',\n",
       " 'How',\n",
       " 'Howdy',\n",
       " 'Hug',\n",
       " 'Hughes',\n",
       " 'Hugs',\n",
       " 'Huh',\n",
       " 'Humidity',\n",
       " 'Hummmm',\n",
       " 'Hungry',\n",
       " 'I',\n",
       " 'ID',\n",
       " 'IF',\n",
       " 'II',\n",
       " 'IL',\n",
       " 'IM',\n",
       " 'IN',\n",
       " 'INTERESTING',\n",
       " 'IRC',\n",
       " 'IS',\n",
       " 'IT',\n",
       " 'Ico',\n",
       " 'Id',\n",
       " 'If',\n",
       " 'Im',\n",
       " 'Ima',\n",
       " 'Images',\n",
       " 'In',\n",
       " 'In.',\n",
       " 'Indeed',\n",
       " 'Indiantown',\n",
       " 'Iowa',\n",
       " 'Is',\n",
       " 'It',\n",
       " 'Its',\n",
       " 'Ive',\n",
       " 'JESUS',\n",
       " 'JOIN',\n",
       " 'JRZ',\n",
       " 'JTo',\n",
       " 'JUST',\n",
       " 'Jam',\n",
       " 'James',\n",
       " 'Jane',\n",
       " 'Jason',\n",
       " 'Jayse',\n",
       " 'Jerketts',\n",
       " 'Jess',\n",
       " 'Jesus',\n",
       " 'Jeter',\n",
       " 'Joe',\n",
       " 'Joey',\n",
       " 'John',\n",
       " 'Jon',\n",
       " 'Jones',\n",
       " 'Jonesboro',\n",
       " 'Jordison',\n",
       " 'Joshy',\n",
       " 'Judy',\n",
       " 'Just',\n",
       " 'Justin',\n",
       " 'K',\n",
       " 'K-Fed',\n",
       " 'KNOW',\n",
       " 'Kansas',\n",
       " 'Kellogs',\n",
       " 'Kent',\n",
       " 'Kentucky',\n",
       " 'Kewl',\n",
       " 'Kick',\n",
       " 'Kids',\n",
       " 'King',\n",
       " 'Kiss',\n",
       " 'Kittie',\n",
       " 'KoOL',\n",
       " 'Kold',\n",
       " 'LA',\n",
       " 'LATE',\n",
       " 'LATER',\n",
       " 'LAst',\n",
       " 'LIVE',\n",
       " 'LIX',\n",
       " 'LMAO',\n",
       " 'LOL',\n",
       " 'LOLOLOLLL',\n",
       " 'LONG',\n",
       " 'LONLEY',\n",
       " 'LOUD',\n",
       " 'LOUDER',\n",
       " 'LOVES',\n",
       " 'LOl',\n",
       " 'LPN',\n",
       " 'Ladies',\n",
       " 'Laguna',\n",
       " 'Lampert',\n",
       " 'Last',\n",
       " 'Laters',\n",
       " 'Lay',\n",
       " 'Lee',\n",
       " 'Leeches',\n",
       " 'Length',\n",
       " 'Let',\n",
       " 'Lets',\n",
       " 'Liam',\n",
       " 'Lies',\n",
       " 'Life',\n",
       " 'Like',\n",
       " 'Lil',\n",
       " 'Lime',\n",
       " 'Lion',\n",
       " 'Lithium',\n",
       " 'Little',\n",
       " 'Live',\n",
       " 'Lives',\n",
       " 'Living',\n",
       " 'Lmao',\n",
       " 'Lmfao',\n",
       " 'LoL',\n",
       " 'LoVe',\n",
       " 'Lol',\n",
       " 'London',\n",
       " 'Long',\n",
       " 'Look',\n",
       " 'Looking',\n",
       " 'Lord',\n",
       " 'Louisville',\n",
       " 'Lousiana',\n",
       " 'Love',\n",
       " 'Lovely',\n",
       " 'LuverZ',\n",
       " 'M',\n",
       " 'MAN',\n",
       " 'MATCH',\n",
       " 'MD',\n",
       " 'ME',\n",
       " 'MISHAP',\n",
       " 'MODE',\n",
       " 'MORE',\n",
       " 'MOUTH',\n",
       " 'MP3',\n",
       " 'MRIs',\n",
       " 'MSN',\n",
       " 'MUAH',\n",
       " 'MY',\n",
       " 'Maidstone',\n",
       " 'Male',\n",
       " 'Man',\n",
       " 'Maps',\n",
       " 'Marlaya',\n",
       " 'Martian',\n",
       " 'Marvin',\n",
       " 'Mary',\n",
       " 'Matt',\n",
       " 'Max',\n",
       " 'Maybe',\n",
       " 'Me',\n",
       " 'Meep',\n",
       " 'Meh',\n",
       " 'Memory',\n",
       " 'Men',\n",
       " 'Mercy',\n",
       " 'Messaging',\n",
       " 'Metallica',\n",
       " 'Michigan',\n",
       " 'Midwest',\n",
       " 'Mine',\n",
       " 'Mmm',\n",
       " 'Mo',\n",
       " 'Mom',\n",
       " 'Money',\n",
       " 'Mono',\n",
       " 'Mooooooooooooooooooooooooooo',\n",
       " 'Morgan',\n",
       " 'Mp3',\n",
       " 'Ms',\n",
       " 'MsUtah',\n",
       " 'Muahz',\n",
       " 'Music',\n",
       " 'My',\n",
       " 'N',\n",
       " 'N\"T',\n",
       " \"N'T\",\n",
       " 'NAME',\n",
       " 'NC',\n",
       " 'NICK',\n",
       " 'NIght',\n",
       " ...]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\n",
    "chat_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee',\n",
       " 'miiiiiinnnnnnnnnneeeeeeee',\n",
       " 'mine',\n",
       " 'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in chat_words if re.search('^m+i+n+e+$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'aaaaaaaaaaaaaaaaa',\n",
       " 'aaahhhh',\n",
       " 'ah',\n",
       " 'ahah',\n",
       " 'ahahah',\n",
       " 'ahh',\n",
       " 'ahhahahaha',\n",
       " 'ahhh',\n",
       " 'ahhhh',\n",
       " 'ahhhhhh',\n",
       " 'ahhhhhhhhhhhhhh',\n",
       " 'h',\n",
       " 'ha',\n",
       " 'haaa',\n",
       " 'hah',\n",
       " 'haha',\n",
       " 'hahaaa',\n",
       " 'hahah',\n",
       " 'hahaha',\n",
       " 'hahahaa',\n",
       " 'hahahah',\n",
       " 'hahahaha',\n",
       " 'hahahahaaa',\n",
       " 'hahahahahaha',\n",
       " 'hahahahahahaha',\n",
       " 'hahahahahahahahahahahahahahahaha',\n",
       " 'hahahhahah',\n",
       " 'hahhahahaha']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in chat_words if re.search('^[ha]+$', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`^`가 square bracket 내의 첫글자에 있는 경우\n",
    "- (예) «[^aeiouAEIOU]» : 모음이 아닌 모든 글자\n",
    "- (예) «^[^aeiouAEIOU]+\\\\$» : 모음이 아닌 글자로만 된 단어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\\`, `{}`, `()`, `|` 의 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " \"''\",\n",
       " \"'30s\",\n",
       " \"'40s\",\n",
       " \"'50s\",\n",
       " \"'80s\",\n",
       " \"'82\",\n",
       " \"'86\",\n",
       " \"'S\",\n",
       " \"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " '*',\n",
       " '*-1',\n",
       " '*-10',\n",
       " '*-100',\n",
       " '*-101',\n",
       " '*-102',\n",
       " '*-103',\n",
       " '*-104',\n",
       " '*-105',\n",
       " '*-106',\n",
       " '*-107',\n",
       " '*-108',\n",
       " '*-109',\n",
       " '*-11',\n",
       " '*-110',\n",
       " '*-111',\n",
       " '*-112',\n",
       " '*-113',\n",
       " '*-114',\n",
       " '*-115',\n",
       " '*-116',\n",
       " '*-117',\n",
       " '*-118',\n",
       " '*-119',\n",
       " '*-12',\n",
       " '*-120',\n",
       " '*-121',\n",
       " '*-122',\n",
       " '*-123',\n",
       " '*-124',\n",
       " '*-125',\n",
       " '*-126',\n",
       " '*-127',\n",
       " '*-128',\n",
       " '*-129',\n",
       " '*-13',\n",
       " '*-130',\n",
       " '*-131',\n",
       " '*-132',\n",
       " '*-133',\n",
       " '*-134',\n",
       " '*-135',\n",
       " '*-136',\n",
       " '*-137',\n",
       " '*-138',\n",
       " '*-139',\n",
       " '*-14',\n",
       " '*-140',\n",
       " '*-141',\n",
       " '*-142',\n",
       " '*-144',\n",
       " '*-145',\n",
       " '*-146',\n",
       " '*-147',\n",
       " '*-149',\n",
       " '*-15',\n",
       " '*-150',\n",
       " '*-151',\n",
       " '*-152',\n",
       " '*-153',\n",
       " '*-154',\n",
       " '*-155',\n",
       " '*-156',\n",
       " '*-157',\n",
       " '*-158',\n",
       " '*-159',\n",
       " '*-16',\n",
       " '*-160',\n",
       " '*-161',\n",
       " '*-162',\n",
       " '*-163',\n",
       " '*-164',\n",
       " '*-165',\n",
       " '*-166',\n",
       " '*-17',\n",
       " '*-18',\n",
       " '*-19',\n",
       " '*-2',\n",
       " '*-20',\n",
       " '*-21',\n",
       " '*-22',\n",
       " '*-23',\n",
       " '*-24',\n",
       " '*-25',\n",
       " '*-26',\n",
       " '*-27',\n",
       " '*-28',\n",
       " '*-29',\n",
       " '*-3',\n",
       " '*-30',\n",
       " '*-31',\n",
       " '*-32',\n",
       " '*-33',\n",
       " '*-34',\n",
       " '*-35',\n",
       " '*-36',\n",
       " '*-37',\n",
       " '*-38',\n",
       " '*-39',\n",
       " '*-4',\n",
       " '*-40',\n",
       " '*-41',\n",
       " '*-42',\n",
       " '*-43',\n",
       " '*-44',\n",
       " '*-45',\n",
       " '*-46',\n",
       " '*-47',\n",
       " '*-48',\n",
       " '*-49',\n",
       " '*-5',\n",
       " '*-50',\n",
       " '*-51',\n",
       " '*-52',\n",
       " '*-53',\n",
       " '*-54',\n",
       " '*-55',\n",
       " '*-56',\n",
       " '*-57',\n",
       " '*-58',\n",
       " '*-59',\n",
       " '*-6',\n",
       " '*-60',\n",
       " '*-61',\n",
       " '*-62',\n",
       " '*-63',\n",
       " '*-64',\n",
       " '*-66',\n",
       " '*-67',\n",
       " '*-68',\n",
       " '*-69',\n",
       " '*-7',\n",
       " '*-70',\n",
       " '*-71',\n",
       " '*-72',\n",
       " '*-73',\n",
       " '*-74',\n",
       " '*-75',\n",
       " '*-76',\n",
       " '*-77',\n",
       " '*-78',\n",
       " '*-79',\n",
       " '*-8',\n",
       " '*-80',\n",
       " '*-81',\n",
       " '*-82',\n",
       " '*-83',\n",
       " '*-84',\n",
       " '*-85',\n",
       " '*-86',\n",
       " '*-87',\n",
       " '*-88',\n",
       " '*-89',\n",
       " '*-9',\n",
       " '*-90',\n",
       " '*-91',\n",
       " '*-92',\n",
       " '*-93',\n",
       " '*-94',\n",
       " '*-95',\n",
       " '*-96',\n",
       " '*-97',\n",
       " '*-98',\n",
       " '*-99',\n",
       " '*?*',\n",
       " '*EXP*-1',\n",
       " '*EXP*-2',\n",
       " '*EXP*-3',\n",
       " '*ICH*-1',\n",
       " '*ICH*-2',\n",
       " '*ICH*-3',\n",
       " '*ICH*-4',\n",
       " '*NOT*',\n",
       " '*PPA*-1',\n",
       " '*PPA*-2',\n",
       " '*PPA*-3',\n",
       " '*RNR*-1',\n",
       " '*RNR*-2',\n",
       " '*RNR*-4',\n",
       " '*T*-1',\n",
       " '*T*-10',\n",
       " '*T*-100',\n",
       " '*T*-101',\n",
       " '*T*-102',\n",
       " '*T*-103',\n",
       " '*T*-104',\n",
       " '*T*-105',\n",
       " '*T*-106',\n",
       " '*T*-107',\n",
       " '*T*-108',\n",
       " '*T*-109',\n",
       " '*T*-11',\n",
       " '*T*-110',\n",
       " '*T*-111',\n",
       " '*T*-112',\n",
       " '*T*-113',\n",
       " '*T*-114',\n",
       " '*T*-115',\n",
       " '*T*-116',\n",
       " '*T*-117',\n",
       " '*T*-118',\n",
       " '*T*-119',\n",
       " '*T*-12',\n",
       " '*T*-120',\n",
       " '*T*-121',\n",
       " '*T*-122',\n",
       " '*T*-123',\n",
       " '*T*-124',\n",
       " '*T*-125',\n",
       " '*T*-126',\n",
       " '*T*-127',\n",
       " '*T*-128',\n",
       " '*T*-129',\n",
       " '*T*-13',\n",
       " '*T*-130',\n",
       " '*T*-131',\n",
       " '*T*-132',\n",
       " '*T*-133',\n",
       " '*T*-134',\n",
       " '*T*-135',\n",
       " '*T*-136',\n",
       " '*T*-137',\n",
       " '*T*-138',\n",
       " '*T*-139',\n",
       " '*T*-14',\n",
       " '*T*-140',\n",
       " '*T*-141',\n",
       " '*T*-142',\n",
       " '*T*-143',\n",
       " '*T*-144',\n",
       " '*T*-145',\n",
       " '*T*-146',\n",
       " '*T*-147',\n",
       " '*T*-148',\n",
       " '*T*-149',\n",
       " '*T*-15',\n",
       " '*T*-150',\n",
       " '*T*-151',\n",
       " '*T*-152',\n",
       " '*T*-153',\n",
       " '*T*-154',\n",
       " '*T*-155',\n",
       " '*T*-156',\n",
       " '*T*-157',\n",
       " '*T*-158',\n",
       " '*T*-159',\n",
       " '*T*-16',\n",
       " '*T*-160',\n",
       " '*T*-161',\n",
       " '*T*-162',\n",
       " '*T*-163',\n",
       " '*T*-164',\n",
       " '*T*-165',\n",
       " '*T*-166',\n",
       " '*T*-167',\n",
       " '*T*-168',\n",
       " '*T*-169',\n",
       " '*T*-17',\n",
       " '*T*-170',\n",
       " '*T*-171',\n",
       " '*T*-172',\n",
       " '*T*-173',\n",
       " '*T*-174',\n",
       " '*T*-175',\n",
       " '*T*-176',\n",
       " '*T*-177',\n",
       " '*T*-178',\n",
       " '*T*-179',\n",
       " '*T*-18',\n",
       " '*T*-180',\n",
       " '*T*-181',\n",
       " '*T*-182',\n",
       " '*T*-183',\n",
       " '*T*-184',\n",
       " '*T*-185',\n",
       " '*T*-186',\n",
       " '*T*-187',\n",
       " '*T*-188',\n",
       " '*T*-189',\n",
       " '*T*-19',\n",
       " '*T*-190',\n",
       " '*T*-191',\n",
       " '*T*-192',\n",
       " '*T*-193',\n",
       " '*T*-194',\n",
       " '*T*-195',\n",
       " '*T*-196',\n",
       " '*T*-197',\n",
       " '*T*-198',\n",
       " '*T*-199',\n",
       " '*T*-2',\n",
       " '*T*-20',\n",
       " '*T*-200',\n",
       " '*T*-201',\n",
       " '*T*-202',\n",
       " '*T*-203',\n",
       " '*T*-204',\n",
       " '*T*-205',\n",
       " '*T*-206',\n",
       " '*T*-207',\n",
       " '*T*-208',\n",
       " '*T*-21',\n",
       " '*T*-210',\n",
       " '*T*-211',\n",
       " '*T*-212',\n",
       " '*T*-213',\n",
       " '*T*-214',\n",
       " '*T*-215',\n",
       " '*T*-216',\n",
       " '*T*-217',\n",
       " '*T*-218',\n",
       " '*T*-219',\n",
       " '*T*-22',\n",
       " '*T*-220',\n",
       " '*T*-221',\n",
       " '*T*-222',\n",
       " '*T*-223',\n",
       " '*T*-224',\n",
       " '*T*-225',\n",
       " '*T*-226',\n",
       " '*T*-227',\n",
       " '*T*-228',\n",
       " '*T*-229',\n",
       " '*T*-23',\n",
       " '*T*-230',\n",
       " '*T*-231',\n",
       " '*T*-232',\n",
       " '*T*-233',\n",
       " '*T*-234',\n",
       " '*T*-235',\n",
       " '*T*-236',\n",
       " '*T*-237',\n",
       " '*T*-238',\n",
       " '*T*-239',\n",
       " '*T*-24',\n",
       " '*T*-240',\n",
       " '*T*-241',\n",
       " '*T*-242',\n",
       " '*T*-243',\n",
       " '*T*-244',\n",
       " '*T*-245',\n",
       " '*T*-246',\n",
       " '*T*-247',\n",
       " '*T*-248',\n",
       " '*T*-249',\n",
       " '*T*-25',\n",
       " '*T*-250',\n",
       " '*T*-251',\n",
       " '*T*-252',\n",
       " '*T*-253',\n",
       " '*T*-254',\n",
       " '*T*-255',\n",
       " '*T*-256',\n",
       " '*T*-257',\n",
       " '*T*-258',\n",
       " '*T*-259',\n",
       " '*T*-26',\n",
       " '*T*-260',\n",
       " '*T*-27',\n",
       " '*T*-28',\n",
       " '*T*-29',\n",
       " '*T*-3',\n",
       " '*T*-30',\n",
       " '*T*-31',\n",
       " '*T*-32',\n",
       " '*T*-33',\n",
       " '*T*-34',\n",
       " '*T*-35',\n",
       " '*T*-36',\n",
       " '*T*-37',\n",
       " '*T*-38',\n",
       " '*T*-39',\n",
       " '*T*-4',\n",
       " '*T*-40',\n",
       " '*T*-41',\n",
       " '*T*-42',\n",
       " '*T*-43',\n",
       " '*T*-44',\n",
       " '*T*-45',\n",
       " '*T*-46',\n",
       " '*T*-47',\n",
       " '*T*-48',\n",
       " '*T*-49',\n",
       " '*T*-5',\n",
       " '*T*-50',\n",
       " '*T*-51',\n",
       " '*T*-52',\n",
       " '*T*-53',\n",
       " '*T*-54',\n",
       " '*T*-55',\n",
       " '*T*-56',\n",
       " '*T*-57',\n",
       " '*T*-58',\n",
       " '*T*-59',\n",
       " '*T*-6',\n",
       " '*T*-60',\n",
       " '*T*-61',\n",
       " '*T*-62',\n",
       " '*T*-63',\n",
       " '*T*-64',\n",
       " '*T*-65',\n",
       " '*T*-66',\n",
       " '*T*-67',\n",
       " '*T*-68',\n",
       " '*T*-69',\n",
       " '*T*-7',\n",
       " '*T*-70',\n",
       " '*T*-71',\n",
       " '*T*-72',\n",
       " '*T*-73',\n",
       " '*T*-74',\n",
       " '*T*-75',\n",
       " '*T*-76',\n",
       " '*T*-77',\n",
       " '*T*-78',\n",
       " '*T*-79',\n",
       " '*T*-8',\n",
       " '*T*-80',\n",
       " '*T*-81',\n",
       " '*T*-82',\n",
       " '*T*-83',\n",
       " '*T*-84',\n",
       " '*T*-85',\n",
       " '*T*-86',\n",
       " '*T*-87',\n",
       " '*T*-88',\n",
       " '*T*-89',\n",
       " '*T*-9',\n",
       " '*T*-90',\n",
       " '*T*-91',\n",
       " '*T*-92',\n",
       " '*T*-93',\n",
       " '*T*-94',\n",
       " '*T*-95',\n",
       " '*T*-96',\n",
       " '*T*-97',\n",
       " '*T*-98',\n",
       " '*T*-99',\n",
       " '*U*',\n",
       " ',',\n",
       " '-',\n",
       " '--',\n",
       " '-LCB-',\n",
       " '-LRB-',\n",
       " '-RCB-',\n",
       " '-RRB-',\n",
       " '.',\n",
       " '...',\n",
       " '0',\n",
       " '0.0085',\n",
       " '0.05',\n",
       " '0.1',\n",
       " '0.16',\n",
       " '0.2',\n",
       " '0.25',\n",
       " '0.28',\n",
       " '0.3',\n",
       " '0.4',\n",
       " '0.5',\n",
       " '0.50',\n",
       " '0.54',\n",
       " '0.56',\n",
       " '0.60',\n",
       " '0.7',\n",
       " '0.82',\n",
       " '0.84',\n",
       " '0.9',\n",
       " '0.95',\n",
       " '0.99',\n",
       " '1',\n",
       " '1,000',\n",
       " '1,050,000',\n",
       " '1,100',\n",
       " '1,200',\n",
       " '1,298',\n",
       " '1,400',\n",
       " '1,460',\n",
       " '1,500',\n",
       " '1,570',\n",
       " '1,620',\n",
       " '1,880',\n",
       " '1.01',\n",
       " '1.1',\n",
       " '1.125',\n",
       " '1.14',\n",
       " '1.1650',\n",
       " '1.17',\n",
       " '1.18',\n",
       " '1.19',\n",
       " '1.2',\n",
       " '1.20',\n",
       " '1.24',\n",
       " '1.25',\n",
       " '1.26',\n",
       " '1.28',\n",
       " '1.35',\n",
       " '1.39',\n",
       " '1.4',\n",
       " '1.457',\n",
       " '1.46',\n",
       " '1.49',\n",
       " '1.5',\n",
       " '1.50',\n",
       " '1.55',\n",
       " '1.56',\n",
       " '1.5755',\n",
       " '1.5805',\n",
       " '1.6',\n",
       " '1.61',\n",
       " '1.637',\n",
       " '1.64',\n",
       " '1.65',\n",
       " '1.7',\n",
       " '1.75',\n",
       " '1.76',\n",
       " '1.8',\n",
       " '1.82',\n",
       " '1.8415',\n",
       " '1.85',\n",
       " '1.8500',\n",
       " '1.9',\n",
       " '1.916',\n",
       " '1.92',\n",
       " '10',\n",
       " '10,000',\n",
       " '10-day',\n",
       " '10-lap',\n",
       " '10-year',\n",
       " '10.19',\n",
       " '10.2',\n",
       " '10.5',\n",
       " '100',\n",
       " '100,000',\n",
       " '100,980',\n",
       " '100-megabyte',\n",
       " '100-share',\n",
       " '101',\n",
       " '102',\n",
       " '103',\n",
       " '105',\n",
       " '106',\n",
       " '107',\n",
       " '107.03',\n",
       " '107.9',\n",
       " '108',\n",
       " '109.73',\n",
       " '10th',\n",
       " '11',\n",
       " '11,000',\n",
       " '11,390,000',\n",
       " '11,762',\n",
       " '11-month-old',\n",
       " '11.10',\n",
       " '11.5',\n",
       " '11.57',\n",
       " '11.6',\n",
       " '11.72',\n",
       " '11.95',\n",
       " '110',\n",
       " '111',\n",
       " '112.9',\n",
       " '113.2',\n",
       " '114',\n",
       " '115',\n",
       " '116.3',\n",
       " '116.4',\n",
       " '116.7',\n",
       " '116.9',\n",
       " '118',\n",
       " '118.6',\n",
       " '119',\n",
       " '11\\\\/16',\n",
       " '11th',\n",
       " '12',\n",
       " '12,252',\n",
       " '12-member',\n",
       " '12-point',\n",
       " '12-year',\n",
       " '12.09',\n",
       " '12.5',\n",
       " '12.52',\n",
       " '12.68',\n",
       " '12.7',\n",
       " '12.82',\n",
       " '12.97',\n",
       " '120',\n",
       " '120,000',\n",
       " '120-a-share',\n",
       " '120.7',\n",
       " '1206.26',\n",
       " '121.6',\n",
       " '125',\n",
       " '126,000',\n",
       " '126.1',\n",
       " '126.15',\n",
       " '127.03',\n",
       " '128',\n",
       " '129.91',\n",
       " '12\\\\/32',\n",
       " '13',\n",
       " '13,056',\n",
       " '13.1',\n",
       " '13.15',\n",
       " '13.5',\n",
       " '13.50',\n",
       " '13.625',\n",
       " '13.65',\n",
       " '13.73',\n",
       " '13.8',\n",
       " '13.90',\n",
       " '130',\n",
       " '130.6',\n",
       " '130.7',\n",
       " '131.01',\n",
       " '132',\n",
       " '132,000',\n",
       " '132.9',\n",
       " '133',\n",
       " '133.7',\n",
       " '133.8',\n",
       " '135',\n",
       " '138',\n",
       " '139',\n",
       " '13\\\\/16',\n",
       " '14',\n",
       " '14,821',\n",
       " '14-hour',\n",
       " '14.',\n",
       " '14.00',\n",
       " '14.13',\n",
       " '14.26',\n",
       " '14.28',\n",
       " '14.43',\n",
       " '14.5',\n",
       " '14.53',\n",
       " '14.54',\n",
       " '14.6',\n",
       " '14.75',\n",
       " '14.99',\n",
       " '140',\n",
       " '141.9',\n",
       " '142.84',\n",
       " '142.85',\n",
       " '143.08',\n",
       " '143.80',\n",
       " '143.93',\n",
       " '144',\n",
       " '145',\n",
       " '148',\n",
       " '148.9',\n",
       " '149',\n",
       " '149.9',\n",
       " '14\\\\/32',\n",
       " '15',\n",
       " '15,000',\n",
       " '15-day',\n",
       " '15.5',\n",
       " '150',\n",
       " '150,000',\n",
       " '150-point',\n",
       " '150.00',\n",
       " '152,000',\n",
       " '153.3',\n",
       " '154,240,000',\n",
       " '154.2',\n",
       " '155',\n",
       " '158,666',\n",
       " '16',\n",
       " '16,000',\n",
       " '16,072',\n",
       " '16.05',\n",
       " '16.09',\n",
       " '16.125',\n",
       " '16.2',\n",
       " '16.5',\n",
       " '16.68',\n",
       " '16.7',\n",
       " '16.9',\n",
       " '160',\n",
       " '1614',\n",
       " '1637',\n",
       " '169.9',\n",
       " '16\\\\/32',\n",
       " '17',\n",
       " '17-year-old',\n",
       " '17.3',\n",
       " '17.4',\n",
       " '17.5',\n",
       " '17.95',\n",
       " '170',\n",
       " '170,000',\n",
       " '170,262',\n",
       " '1738.1',\n",
       " '175',\n",
       " '176',\n",
       " '176.1',\n",
       " '177',\n",
       " '1787',\n",
       " '179',\n",
       " '18',\n",
       " '18,000',\n",
       " '18,444',\n",
       " '18-a-share',\n",
       " '18-year-old',\n",
       " '18.3',\n",
       " '18.6',\n",
       " '18.95',\n",
       " '180',\n",
       " '184',\n",
       " '185.9',\n",
       " '187',\n",
       " '188',\n",
       " '188.84',\n",
       " '19',\n",
       " '19-month-old',\n",
       " '19.3',\n",
       " '19.50',\n",
       " '19.6',\n",
       " '19.94',\n",
       " '19.95',\n",
       " '190',\n",
       " '190-point',\n",
       " '1901',\n",
       " '1903',\n",
       " '191.9',\n",
       " '1917',\n",
       " '1920s',\n",
       " '1925',\n",
       " '1928-33',\n",
       " '1929',\n",
       " '1933',\n",
       " '1934',\n",
       " '1937-40',\n",
       " '1940s',\n",
       " '1948',\n",
       " '195',\n",
       " '1950s',\n",
       " '1953',\n",
       " '1955',\n",
       " '1956',\n",
       " '1960s',\n",
       " '1961',\n",
       " '1965',\n",
       " '1966',\n",
       " '1967',\n",
       " '1968',\n",
       " '1969',\n",
       " '1970',\n",
       " '1970s',\n",
       " '1971',\n",
       " '1972',\n",
       " '1973',\n",
       " '1973-75',\n",
       " '1975',\n",
       " '1976',\n",
       " '1977',\n",
       " '1979',\n",
       " '198',\n",
       " '1980',\n",
       " '1980s',\n",
       " '1981',\n",
       " '1982',\n",
       " '1983',\n",
       " '1983-85',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1986-87',\n",
       " '1987',\n",
       " '1987-88',\n",
       " '1988',\n",
       " '1988-89',\n",
       " '1989',\n",
       " '1989-90',\n",
       " '1990',\n",
       " '1990-91',\n",
       " '1990s',\n",
       " '1991',\n",
       " '1991-1999',\n",
       " '1991-2000',\n",
       " '1992',\n",
       " '1992-1999',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '1:30',\n",
       " '1\\\\/10th',\n",
       " '1\\\\/2',\n",
       " '1\\\\/4',\n",
       " '1\\\\/8',\n",
       " '1st',\n",
       " '2',\n",
       " '2,000',\n",
       " '2,050-passenger',\n",
       " '2,099',\n",
       " '2,303,328',\n",
       " '2,410',\n",
       " '2,500',\n",
       " '2,700',\n",
       " '2-3',\n",
       " '2-8',\n",
       " '2.07',\n",
       " '2.1',\n",
       " '2.15',\n",
       " '2.19',\n",
       " '2.2',\n",
       " '2.25',\n",
       " '2.29',\n",
       " '2.3',\n",
       " '2.30',\n",
       " '2.35',\n",
       " '2.375',\n",
       " '2.4',\n",
       " '2.42',\n",
       " '2.44',\n",
       " '2.46',\n",
       " '2.47',\n",
       " '2.5',\n",
       " '2.50',\n",
       " '2.6',\n",
       " '2.62',\n",
       " '2.65',\n",
       " '2.7',\n",
       " '2.75',\n",
       " '2.8',\n",
       " '2.80',\n",
       " '2.87',\n",
       " '2.875',\n",
       " '2.9',\n",
       " '2.95',\n",
       " '20',\n",
       " '20,000',\n",
       " '20-point',\n",
       " '20-stock',\n",
       " '20.07',\n",
       " '20.5',\n",
       " '200',\n",
       " '200,000',\n",
       " '2000',\n",
       " '2003\\\\/2007',\n",
       " '2005',\n",
       " '2009',\n",
       " '2017',\n",
       " '2019',\n",
       " '2029',\n",
       " '203',\n",
       " '20s',\n",
       " '21',\n",
       " '21,000',\n",
       " '21-month',\n",
       " '21.1',\n",
       " '21.9',\n",
       " '210',\n",
       " '210,000',\n",
       " '212',\n",
       " '214',\n",
       " '2141.7',\n",
       " '2160.1',\n",
       " '2163.2',\n",
       " '22',\n",
       " '22.75',\n",
       " '220',\n",
       " '220.45',\n",
       " '221.4',\n",
       " '225',\n",
       " '225,000',\n",
       " '225.6',\n",
       " '226,570,380',\n",
       " '227',\n",
       " '228',\n",
       " '22\\\\/32',\n",
       " '23',\n",
       " '23,000',\n",
       " '23,403',\n",
       " '23.25',\n",
       " '23.4',\n",
       " '23.5',\n",
       " '23.72',\n",
       " '230-215',\n",
       " '234.4',\n",
       " '235',\n",
       " '236.74',\n",
       " '236.79',\n",
       " '237-seat',\n",
       " '238,000-circulation',\n",
       " '24',\n",
       " '24,000',\n",
       " '24.95',\n",
       " '240',\n",
       " '240,000',\n",
       " '240-page',\n",
       " '241',\n",
       " '244,000',\n",
       " '245',\n",
       " '25',\n",
       " '25,000',\n",
       " '25-year-old',\n",
       " '25.50',\n",
       " '25.6',\n",
       " '250',\n",
       " '250,000',\n",
       " '251.2',\n",
       " '257',\n",
       " '26',\n",
       " '26,000',\n",
       " '26,956',\n",
       " '26.2',\n",
       " '26.5',\n",
       " '26.8',\n",
       " '260',\n",
       " '263.07',\n",
       " '2645.90',\n",
       " '266',\n",
       " '2691.19',\n",
       " '27',\n",
       " '27-year',\n",
       " '27.1',\n",
       " '27.4',\n",
       " '270',\n",
       " '271,124',\n",
       " '271-147',\n",
       " '273.5',\n",
       " '274',\n",
       " '275',\n",
       " '278.7',\n",
       " '28',\n",
       " '28.25',\n",
       " '28.36',\n",
       " '28.4',\n",
       " '28.5',\n",
       " '28.53',\n",
       " '28.6',\n",
       " '280',\n",
       " '282',\n",
       " '286',\n",
       " '29',\n",
       " '29.3',\n",
       " '29.4',\n",
       " '29.9',\n",
       " '292.32',\n",
       " '295',\n",
       " '29year',\n",
       " '2\\\\/32',\n",
       " '3',\n",
       " '3,040,000',\n",
       " '3,250,000',\n",
       " '3,288,453',\n",
       " '3,500',\n",
       " '3,600',\n",
       " '3-4',\n",
       " '3.01',\n",
       " '3.04',\n",
       " '3.1',\n",
       " '3.16',\n",
       " '3.18',\n",
       " '3.19',\n",
       " '3.2',\n",
       " '3.20',\n",
       " '3.23',\n",
       " '3.253',\n",
       " '3.28',\n",
       " '3.3',\n",
       " '3.35',\n",
       " '3.375',\n",
       " '3.4',\n",
       " '3.42',\n",
       " '3.43',\n",
       " '3.5',\n",
       " '3.55',\n",
       " '3.6',\n",
       " '3.61',\n",
       " '3.625',\n",
       " '3.7',\n",
       " '3.75',\n",
       " '3.8',\n",
       " '3.80',\n",
       " ...]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "wsj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.0085',\n",
       " '0.05',\n",
       " '0.1',\n",
       " '0.16',\n",
       " '0.2',\n",
       " '0.25',\n",
       " '0.28',\n",
       " '0.3',\n",
       " '0.4',\n",
       " '0.5',\n",
       " '0.50',\n",
       " '0.54',\n",
       " '0.56',\n",
       " '0.60',\n",
       " '0.7',\n",
       " '0.82',\n",
       " '0.84',\n",
       " '0.9',\n",
       " '0.95',\n",
       " '0.99',\n",
       " '1.01',\n",
       " '1.1',\n",
       " '1.125',\n",
       " '1.14',\n",
       " '1.1650',\n",
       " '1.17',\n",
       " '1.18',\n",
       " '1.19',\n",
       " '1.2',\n",
       " '1.20',\n",
       " '1.24',\n",
       " '1.25',\n",
       " '1.26',\n",
       " '1.28',\n",
       " '1.35',\n",
       " '1.39',\n",
       " '1.4',\n",
       " '1.457',\n",
       " '1.46',\n",
       " '1.49',\n",
       " '1.5',\n",
       " '1.50',\n",
       " '1.55',\n",
       " '1.56',\n",
       " '1.5755',\n",
       " '1.5805',\n",
       " '1.6',\n",
       " '1.61',\n",
       " '1.637',\n",
       " '1.64',\n",
       " '1.65',\n",
       " '1.7',\n",
       " '1.75',\n",
       " '1.76',\n",
       " '1.8',\n",
       " '1.82',\n",
       " '1.8415',\n",
       " '1.85',\n",
       " '1.8500',\n",
       " '1.9',\n",
       " '1.916',\n",
       " '1.92',\n",
       " '10.19',\n",
       " '10.2',\n",
       " '10.5',\n",
       " '107.03',\n",
       " '107.9',\n",
       " '109.73',\n",
       " '11.10',\n",
       " '11.5',\n",
       " '11.57',\n",
       " '11.6',\n",
       " '11.72',\n",
       " '11.95',\n",
       " '112.9',\n",
       " '113.2',\n",
       " '116.3',\n",
       " '116.4',\n",
       " '116.7',\n",
       " '116.9',\n",
       " '118.6',\n",
       " '12.09',\n",
       " '12.5',\n",
       " '12.52',\n",
       " '12.68',\n",
       " '12.7',\n",
       " '12.82',\n",
       " '12.97',\n",
       " '120.7',\n",
       " '1206.26',\n",
       " '121.6',\n",
       " '126.1',\n",
       " '126.15',\n",
       " '127.03',\n",
       " '129.91',\n",
       " '13.1',\n",
       " '13.15',\n",
       " '13.5',\n",
       " '13.50',\n",
       " '13.625',\n",
       " '13.65',\n",
       " '13.73',\n",
       " '13.8',\n",
       " '13.90',\n",
       " '130.6',\n",
       " '130.7',\n",
       " '131.01',\n",
       " '132.9',\n",
       " '133.7',\n",
       " '133.8',\n",
       " '14.00',\n",
       " '14.13',\n",
       " '14.26',\n",
       " '14.28',\n",
       " '14.43',\n",
       " '14.5',\n",
       " '14.53',\n",
       " '14.54',\n",
       " '14.6',\n",
       " '14.75',\n",
       " '14.99',\n",
       " '141.9',\n",
       " '142.84',\n",
       " '142.85',\n",
       " '143.08',\n",
       " '143.80',\n",
       " '143.93',\n",
       " '148.9',\n",
       " '149.9',\n",
       " '15.5',\n",
       " '150.00',\n",
       " '153.3',\n",
       " '154.2',\n",
       " '16.05',\n",
       " '16.09',\n",
       " '16.125',\n",
       " '16.2',\n",
       " '16.5',\n",
       " '16.68',\n",
       " '16.7',\n",
       " '16.9',\n",
       " '169.9',\n",
       " '17.3',\n",
       " '17.4',\n",
       " '17.5',\n",
       " '17.95',\n",
       " '1738.1',\n",
       " '176.1',\n",
       " '18.3',\n",
       " '18.6',\n",
       " '18.95',\n",
       " '185.9',\n",
       " '188.84',\n",
       " '19.3',\n",
       " '19.50',\n",
       " '19.6',\n",
       " '19.94',\n",
       " '19.95',\n",
       " '191.9',\n",
       " '2.07',\n",
       " '2.1',\n",
       " '2.15',\n",
       " '2.19',\n",
       " '2.2',\n",
       " '2.25',\n",
       " '2.29',\n",
       " '2.3',\n",
       " '2.30',\n",
       " '2.35',\n",
       " '2.375',\n",
       " '2.4',\n",
       " '2.42',\n",
       " '2.44',\n",
       " '2.46',\n",
       " '2.47',\n",
       " '2.5',\n",
       " '2.50',\n",
       " '2.6',\n",
       " '2.62',\n",
       " '2.65',\n",
       " '2.7',\n",
       " '2.75',\n",
       " '2.8',\n",
       " '2.80',\n",
       " '2.87',\n",
       " '2.875',\n",
       " '2.9',\n",
       " '2.95',\n",
       " '20.07',\n",
       " '20.5',\n",
       " '21.1',\n",
       " '21.9',\n",
       " '2141.7',\n",
       " '2160.1',\n",
       " '2163.2',\n",
       " '22.75',\n",
       " '220.45',\n",
       " '221.4',\n",
       " '225.6',\n",
       " '23.25',\n",
       " '23.4',\n",
       " '23.5',\n",
       " '23.72',\n",
       " '234.4',\n",
       " '236.74',\n",
       " '236.79',\n",
       " '24.95',\n",
       " '25.50',\n",
       " '25.6',\n",
       " '251.2',\n",
       " '26.2',\n",
       " '26.5',\n",
       " '26.8',\n",
       " '263.07',\n",
       " '2645.90',\n",
       " '2691.19',\n",
       " '27.1',\n",
       " '27.4',\n",
       " '273.5',\n",
       " '278.7',\n",
       " '28.25',\n",
       " '28.36',\n",
       " '28.4',\n",
       " '28.5',\n",
       " '28.53',\n",
       " '28.6',\n",
       " '29.3',\n",
       " '29.4',\n",
       " '29.9',\n",
       " '292.32',\n",
       " '3.01',\n",
       " '3.04',\n",
       " '3.1',\n",
       " '3.16',\n",
       " '3.18',\n",
       " '3.19',\n",
       " '3.2',\n",
       " '3.20',\n",
       " '3.23',\n",
       " '3.253',\n",
       " '3.28',\n",
       " '3.3',\n",
       " '3.35',\n",
       " '3.375',\n",
       " '3.4',\n",
       " '3.42',\n",
       " '3.43',\n",
       " '3.5',\n",
       " '3.55',\n",
       " '3.6',\n",
       " '3.61',\n",
       " '3.625',\n",
       " '3.7',\n",
       " '3.75',\n",
       " '3.8',\n",
       " '3.80',\n",
       " '3.9',\n",
       " '30.6',\n",
       " '30.9',\n",
       " '319.75',\n",
       " '32.8',\n",
       " '334.5',\n",
       " '34.625',\n",
       " '341.20',\n",
       " '3436.58',\n",
       " '35.2',\n",
       " '35.7',\n",
       " '352.7',\n",
       " '352.9',\n",
       " '35500.64',\n",
       " '35564.43',\n",
       " '36.9',\n",
       " '361.8',\n",
       " '3648.82',\n",
       " '37.3',\n",
       " '37.5',\n",
       " '372.14',\n",
       " '372.9',\n",
       " '374.19',\n",
       " '374.20',\n",
       " '377.60',\n",
       " '38.3',\n",
       " '38.375',\n",
       " '38.5',\n",
       " '38.875',\n",
       " '387.8',\n",
       " '4.1',\n",
       " '4.10',\n",
       " '4.2',\n",
       " '4.25',\n",
       " '4.3',\n",
       " '4.4',\n",
       " '4.5',\n",
       " '4.55',\n",
       " '4.6',\n",
       " '4.7',\n",
       " '4.75',\n",
       " '4.8',\n",
       " '4.875',\n",
       " '4.898',\n",
       " '4.9',\n",
       " '40.21',\n",
       " '41.60',\n",
       " '415.6',\n",
       " '415.8',\n",
       " '42.1',\n",
       " '42.5',\n",
       " '422.5',\n",
       " '43.875',\n",
       " '434.4',\n",
       " '436.01',\n",
       " '446.62',\n",
       " '449.04',\n",
       " '45.2',\n",
       " '45.3',\n",
       " '45.75',\n",
       " '456.64',\n",
       " '46.1',\n",
       " '47.1',\n",
       " '47.125',\n",
       " '47.5',\n",
       " '47.6',\n",
       " '49.9',\n",
       " '494.50',\n",
       " '497.34',\n",
       " '5.1',\n",
       " '5.2180',\n",
       " '5.276',\n",
       " '5.29',\n",
       " '5.3',\n",
       " '5.39',\n",
       " '5.4',\n",
       " '5.435',\n",
       " '5.5',\n",
       " '5.57',\n",
       " '5.6',\n",
       " '5.63',\n",
       " '5.7',\n",
       " '5.70',\n",
       " '5.8',\n",
       " '5.82',\n",
       " '5.9',\n",
       " '5.92',\n",
       " '50.1',\n",
       " '50.38',\n",
       " '50.45',\n",
       " '51.25',\n",
       " '51.6',\n",
       " '55.1',\n",
       " '566.54',\n",
       " '57.50',\n",
       " '57.6',\n",
       " '57.7',\n",
       " '58.64',\n",
       " '59.6',\n",
       " '59.9',\n",
       " '6.03',\n",
       " '6.1',\n",
       " '6.20',\n",
       " '6.21',\n",
       " '6.25',\n",
       " '6.4',\n",
       " '6.40',\n",
       " '6.44',\n",
       " '6.5',\n",
       " '6.50',\n",
       " '6.53',\n",
       " '6.6',\n",
       " '6.7',\n",
       " '6.70',\n",
       " '6.79',\n",
       " '6.84',\n",
       " '6.9',\n",
       " '60.36',\n",
       " '618.1',\n",
       " '62.1',\n",
       " '62.5',\n",
       " '62.625',\n",
       " '63.79',\n",
       " '630.9',\n",
       " '64.5',\n",
       " '66.5',\n",
       " '7.15',\n",
       " '7.2',\n",
       " '7.20',\n",
       " '7.272',\n",
       " '7.3',\n",
       " '7.4',\n",
       " '7.40',\n",
       " '7.422',\n",
       " '7.45',\n",
       " '7.458',\n",
       " '7.5',\n",
       " '7.50',\n",
       " '7.52',\n",
       " '7.55',\n",
       " '7.60',\n",
       " '7.62',\n",
       " '7.63',\n",
       " '7.65',\n",
       " '7.74',\n",
       " '7.78',\n",
       " '7.79',\n",
       " '7.8',\n",
       " '7.80',\n",
       " '7.84',\n",
       " '7.88',\n",
       " '7.90',\n",
       " '7.95',\n",
       " '70.2',\n",
       " '70.7',\n",
       " '705.6',\n",
       " '72.7',\n",
       " '734.9',\n",
       " '737.5',\n",
       " '77.56',\n",
       " '77.6',\n",
       " '77.70',\n",
       " '8.04',\n",
       " '8.06',\n",
       " '8.07',\n",
       " '8.1',\n",
       " '8.12',\n",
       " '8.14',\n",
       " '8.15',\n",
       " '8.19',\n",
       " '8.2',\n",
       " '8.22',\n",
       " '8.25',\n",
       " '8.30',\n",
       " '8.35',\n",
       " '8.45',\n",
       " '8.467',\n",
       " '8.47',\n",
       " '8.48',\n",
       " '8.5',\n",
       " '8.50',\n",
       " '8.53',\n",
       " '8.55',\n",
       " '8.56',\n",
       " '8.575',\n",
       " '8.60',\n",
       " '8.64',\n",
       " '8.65',\n",
       " '8.70',\n",
       " '8.75',\n",
       " '8.9',\n",
       " '80.50',\n",
       " '80.8',\n",
       " '81.8',\n",
       " '811.9',\n",
       " '83.4',\n",
       " '84.29',\n",
       " '84.9',\n",
       " '85.1',\n",
       " '85.7',\n",
       " '86.12',\n",
       " '87.5',\n",
       " '88.32',\n",
       " '89.7',\n",
       " '89.9',\n",
       " '9.3',\n",
       " '9.32',\n",
       " '9.37',\n",
       " '9.45',\n",
       " '9.5',\n",
       " '9.625',\n",
       " '9.75',\n",
       " '9.8',\n",
       " '9.82',\n",
       " '9.9',\n",
       " '92.9',\n",
       " '93.3',\n",
       " '93.9',\n",
       " '94.2',\n",
       " '94.8',\n",
       " '95.09',\n",
       " '96.4',\n",
       " '98.3',\n",
       " '99.1',\n",
       " '99.3']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C$', 'US$']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('^[A-Z]+\\$$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1614',\n",
       " '1637',\n",
       " '1787',\n",
       " '1901',\n",
       " '1903',\n",
       " '1917',\n",
       " '1925',\n",
       " '1929',\n",
       " '1933',\n",
       " '1934',\n",
       " '1948',\n",
       " '1953',\n",
       " '1955',\n",
       " '1956',\n",
       " '1961',\n",
       " '1965',\n",
       " '1966',\n",
       " '1967',\n",
       " '1968',\n",
       " '1969',\n",
       " '1970',\n",
       " '1971',\n",
       " '1972',\n",
       " '1973',\n",
       " '1975',\n",
       " '1976',\n",
       " '1977',\n",
       " '1979',\n",
       " '1980',\n",
       " '1981',\n",
       " '1982',\n",
       " '1983',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1987',\n",
       " '1988',\n",
       " '1989',\n",
       " '1990',\n",
       " '1991',\n",
       " '1992',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '2000',\n",
       " '2005',\n",
       " '2009',\n",
       " '2017',\n",
       " '2019',\n",
       " '2029',\n",
       " '3057',\n",
       " '8300']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('^[0-9]{4}$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10-day',\n",
       " '10-lap',\n",
       " '10-year',\n",
       " '100-share',\n",
       " '12-point',\n",
       " '12-year',\n",
       " '14-hour',\n",
       " '15-day',\n",
       " '150-point',\n",
       " '190-point',\n",
       " '20-point',\n",
       " '20-stock',\n",
       " '21-month',\n",
       " '237-seat',\n",
       " '240-page',\n",
       " '27-year',\n",
       " '30-day',\n",
       " '30-point',\n",
       " '30-share',\n",
       " '30-year',\n",
       " '300-day',\n",
       " '36-day',\n",
       " '36-store',\n",
       " '42-year',\n",
       " '50-state',\n",
       " '500-stock',\n",
       " '52-week',\n",
       " '69-point',\n",
       " '84-month',\n",
       " '87-store',\n",
       " '90-day']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['black-and-white',\n",
       " 'bread-and-butter',\n",
       " 'father-in-law',\n",
       " 'machine-gun-toting',\n",
       " 'savings-and-loan']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['62%-owned',\n",
       " 'Absorbed',\n",
       " 'According',\n",
       " 'Adopting',\n",
       " 'Advanced',\n",
       " 'Advancing',\n",
       " 'Alfred',\n",
       " 'Allied',\n",
       " 'Annualized',\n",
       " 'Anything',\n",
       " 'Arbitrage-related',\n",
       " 'Arbitraging',\n",
       " 'Asked',\n",
       " 'Assuming',\n",
       " 'Atlanta-based',\n",
       " 'Baking',\n",
       " 'Banking',\n",
       " 'Beginning',\n",
       " 'Beijing',\n",
       " 'Being',\n",
       " 'Bermuda-based',\n",
       " 'Betting',\n",
       " 'Boeing',\n",
       " 'Broadcasting',\n",
       " 'Bucking',\n",
       " 'Buying',\n",
       " 'Calif.-based',\n",
       " 'Change-ringing',\n",
       " 'Citing',\n",
       " 'Concerned',\n",
       " 'Confronted',\n",
       " 'Conn.based',\n",
       " 'Consolidated',\n",
       " 'Continued',\n",
       " 'Continuing',\n",
       " 'Declining',\n",
       " 'Defending',\n",
       " 'Depending',\n",
       " 'Designated',\n",
       " 'Determining',\n",
       " 'Developed',\n",
       " 'Died',\n",
       " 'During',\n",
       " 'Encouraged',\n",
       " 'Encouraging',\n",
       " 'English-speaking',\n",
       " 'Estimated',\n",
       " 'Everything',\n",
       " 'Excluding',\n",
       " 'Exxon-owned',\n",
       " 'Faulding',\n",
       " 'Fed',\n",
       " 'Feeding',\n",
       " 'Filling',\n",
       " 'Filmed',\n",
       " 'Financing',\n",
       " 'Following',\n",
       " 'Founded',\n",
       " 'Fracturing',\n",
       " 'Francisco-based',\n",
       " 'Fred',\n",
       " 'Funded',\n",
       " 'Funding',\n",
       " 'Generalized',\n",
       " 'Germany-based',\n",
       " 'Getting',\n",
       " 'Guaranteed',\n",
       " 'Having',\n",
       " 'Heating',\n",
       " 'Heightened',\n",
       " 'Holding',\n",
       " 'Housing',\n",
       " 'Illuminating',\n",
       " 'Indeed',\n",
       " 'Indexing',\n",
       " 'Irving',\n",
       " 'Jersey-based',\n",
       " 'Judging',\n",
       " 'Knowing',\n",
       " 'Learning',\n",
       " 'Legislating',\n",
       " 'Leming',\n",
       " 'Limited',\n",
       " 'London-based',\n",
       " 'Manfred',\n",
       " 'Manufacturing',\n",
       " 'Melamed',\n",
       " 'Miami-based',\n",
       " 'Mich.-based',\n",
       " 'Mining',\n",
       " 'Minneapolis-based',\n",
       " 'Mo.-based',\n",
       " 'Mortgage-Backed',\n",
       " 'Moving',\n",
       " 'Muzzling',\n",
       " 'N.J.-based',\n",
       " 'NBC-owned',\n",
       " 'NIH-appointed',\n",
       " 'Named',\n",
       " 'No-Smoking',\n",
       " 'Observing',\n",
       " 'Offering',\n",
       " 'Ohio-based',\n",
       " 'Orleans-based',\n",
       " 'Packaging',\n",
       " 'Performing',\n",
       " 'Philadelphia-based',\n",
       " 'Posted',\n",
       " 'Provided',\n",
       " 'Publishing',\n",
       " 'Purchasing',\n",
       " 'Rated',\n",
       " 'Reached',\n",
       " 'Red',\n",
       " 'Red-blooded',\n",
       " 'Reducing',\n",
       " 'Reed',\n",
       " 'Regarded',\n",
       " 'Rekindled',\n",
       " 'Related',\n",
       " 'Ringing',\n",
       " 'Rolling',\n",
       " 'Sacramento-based',\n",
       " 'Scoring',\n",
       " 'Seattle-based',\n",
       " 'Seed',\n",
       " 'Skilled',\n",
       " 'Smelting',\n",
       " 'Something',\n",
       " 'Spending',\n",
       " 'Standardized',\n",
       " 'Standing',\n",
       " 'Starting',\n",
       " 'Sterling',\n",
       " 'Taking',\n",
       " 'Texas-based',\n",
       " 'Toronto-based',\n",
       " 'Traded',\n",
       " 'Trading',\n",
       " 'Troubled',\n",
       " 'U.N.-supervised',\n",
       " 'U.S.-backed',\n",
       " 'United',\n",
       " 'Used',\n",
       " 'Varying',\n",
       " 'Washington-based',\n",
       " 'Whiting',\n",
       " 'Wilfred',\n",
       " 'Winning',\n",
       " 'Xiaoping',\n",
       " 'York-based',\n",
       " 'Zayed',\n",
       " 'abandoned',\n",
       " 'abating',\n",
       " 'abolishing',\n",
       " 'abortion-related',\n",
       " 'abounding',\n",
       " 'abridging',\n",
       " 'absorbed',\n",
       " 'acceded',\n",
       " 'accelerated',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'according',\n",
       " 'accounted',\n",
       " 'accounting',\n",
       " 'accrued',\n",
       " 'accumulated',\n",
       " 'accused',\n",
       " 'accusing',\n",
       " 'achieved',\n",
       " 'achieving',\n",
       " 'acknowledging',\n",
       " 'acquired',\n",
       " 'acquiring',\n",
       " 'acquisition-minded',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'adapted',\n",
       " 'adapting',\n",
       " 'added',\n",
       " 'adding',\n",
       " 'addressing',\n",
       " 'adjusted',\n",
       " 'adjusting',\n",
       " 'admitted',\n",
       " 'admitting',\n",
       " 'adopted',\n",
       " 'advanced',\n",
       " 'advancing',\n",
       " 'advertised',\n",
       " 'advertising',\n",
       " 'advised',\n",
       " 'advocated',\n",
       " 'advocating',\n",
       " 'affecting',\n",
       " 'afflicted',\n",
       " 'aggravated',\n",
       " 'agreed',\n",
       " 'agreeing',\n",
       " 'ailing',\n",
       " 'aimed',\n",
       " 'aiming',\n",
       " 'aired',\n",
       " 'airline-related',\n",
       " 'alarmed',\n",
       " 'alienated',\n",
       " 'alleged',\n",
       " 'alleging',\n",
       " 'allocated',\n",
       " 'allowed',\n",
       " 'altered',\n",
       " 'altering',\n",
       " 'amended',\n",
       " 'amending',\n",
       " 'amounted',\n",
       " 'amusing',\n",
       " 'angered',\n",
       " 'announced',\n",
       " 'annoyed',\n",
       " 'annualized',\n",
       " 'answered',\n",
       " 'anti-dumping',\n",
       " 'anticipated',\n",
       " 'anticipating',\n",
       " 'anything',\n",
       " 'apologizing',\n",
       " 'appealing',\n",
       " 'appeared',\n",
       " 'appearing',\n",
       " 'applied',\n",
       " 'appointed',\n",
       " 'approached',\n",
       " 'appropriated',\n",
       " 'approved',\n",
       " 'arched',\n",
       " 'argued',\n",
       " 'arguing',\n",
       " 'arising',\n",
       " 'armed',\n",
       " 'arranged',\n",
       " 'arrested',\n",
       " 'arrived',\n",
       " 'asbestos-related',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'assassinated',\n",
       " 'assembled',\n",
       " 'asserted',\n",
       " 'asserting',\n",
       " 'assessed',\n",
       " 'assigned',\n",
       " 'assisted',\n",
       " 'associated',\n",
       " 'assumed',\n",
       " 'assuming',\n",
       " 'assured',\n",
       " 'attached',\n",
       " 'attacking',\n",
       " 'attempted',\n",
       " 'attempting',\n",
       " 'attended',\n",
       " 'attending',\n",
       " 'attracted',\n",
       " 'attracting',\n",
       " 'attributed',\n",
       " 'auctioned',\n",
       " 'authorized',\n",
       " 'authorizing',\n",
       " 'automated',\n",
       " 'automotive-lighting',\n",
       " 'averaged',\n",
       " 'averted',\n",
       " 'avoiding',\n",
       " 'awarded',\n",
       " 'awarding',\n",
       " 'backed',\n",
       " 'backing',\n",
       " 'balanced',\n",
       " 'bald-faced',\n",
       " 'balkanized',\n",
       " 'balked',\n",
       " 'balloting',\n",
       " 'bank-backed',\n",
       " 'banking',\n",
       " 'banned',\n",
       " 'banning',\n",
       " 'barking',\n",
       " 'barred',\n",
       " 'based',\n",
       " 'battered',\n",
       " 'battery-operated',\n",
       " 'batting',\n",
       " 'bearing',\n",
       " 'becoming',\n",
       " 'bedding',\n",
       " 'befuddled',\n",
       " 'beginning',\n",
       " 'behaving',\n",
       " 'beheading',\n",
       " 'being',\n",
       " 'beleaguered',\n",
       " 'believed',\n",
       " 'bell-ringing',\n",
       " 'belonging',\n",
       " 'benefited',\n",
       " 'best-selling',\n",
       " 'betting',\n",
       " 'bickering',\n",
       " 'bidding',\n",
       " 'billed',\n",
       " 'billing',\n",
       " 'blamed',\n",
       " 'bled',\n",
       " 'blessing',\n",
       " 'blighted',\n",
       " 'blocked',\n",
       " 'blurred',\n",
       " 'boarding',\n",
       " 'bolstered',\n",
       " 'bombarding',\n",
       " 'booked',\n",
       " 'booming',\n",
       " 'boosted',\n",
       " 'boosting',\n",
       " 'borrowed',\n",
       " 'borrowing',\n",
       " 'botched',\n",
       " 'bothered',\n",
       " 'bounced',\n",
       " 'bowed',\n",
       " 'breaking',\n",
       " 'breathed',\n",
       " 'breathtaking',\n",
       " 'breed',\n",
       " 'bribed',\n",
       " 'bribing',\n",
       " 'briefing',\n",
       " 'brightened',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'broad-based',\n",
       " 'broadcasting',\n",
       " 'broadened',\n",
       " 'brokering',\n",
       " 'brushed',\n",
       " 'budding',\n",
       " 'building',\n",
       " 'bundling',\n",
       " 'buoyed',\n",
       " 'burned',\n",
       " 'buying',\n",
       " 'calculated',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'campaigning',\n",
       " 'cancer-causing',\n",
       " 'capitalized',\n",
       " 'capped',\n",
       " 'captivating',\n",
       " 'cared',\n",
       " 'carried',\n",
       " 'carrying',\n",
       " 'cascading',\n",
       " 'casting',\n",
       " 'caused',\n",
       " 'causing',\n",
       " 'cautioned',\n",
       " 'ceiling',\n",
       " 'centralized',\n",
       " 'certified',\n",
       " 'chaired',\n",
       " 'challenging',\n",
       " 'championing',\n",
       " 'change-ringing',\n",
       " 'changed',\n",
       " 'changing',\n",
       " 'characterized',\n",
       " 'characterizing',\n",
       " 'charged',\n",
       " 'charging',\n",
       " 'chastised',\n",
       " 'cheating',\n",
       " 'checking',\n",
       " 'cheerleading',\n",
       " 'chilled',\n",
       " 'choosing',\n",
       " 'chopped',\n",
       " 'circulated',\n",
       " 'cited',\n",
       " 'citing',\n",
       " 'citizen-sparked',\n",
       " 'city-owned',\n",
       " 'claimed',\n",
       " 'claiming',\n",
       " 'clamped',\n",
       " 'clarified',\n",
       " 'clashed',\n",
       " 'classed',\n",
       " 'classified',\n",
       " 'cleaned',\n",
       " 'cleaner-burning',\n",
       " 'cleared',\n",
       " 'clearing',\n",
       " 'clicked',\n",
       " 'climbed',\n",
       " 'climbing',\n",
       " 'clipped',\n",
       " 'clobbered',\n",
       " 'closed',\n",
       " 'closing',\n",
       " 'clothing',\n",
       " 'clouding',\n",
       " 'cluttered',\n",
       " 'co-founded',\n",
       " 'coaching',\n",
       " 'coal-fired',\n",
       " 'coated',\n",
       " 'codified',\n",
       " 'collaborated',\n",
       " 'collapsed',\n",
       " 'collected',\n",
       " 'collecting',\n",
       " 'collective-bargaining',\n",
       " 'colored',\n",
       " 'combined',\n",
       " 'coming',\n",
       " 'commanded',\n",
       " 'commenting',\n",
       " 'committed',\n",
       " 'committing',\n",
       " 'compared',\n",
       " 'compelling',\n",
       " 'competed',\n",
       " 'competing',\n",
       " 'compiled',\n",
       " 'complained',\n",
       " 'complaining',\n",
       " 'completed',\n",
       " 'completing',\n",
       " 'complicated',\n",
       " 'composed',\n",
       " 'composting',\n",
       " 'compressed',\n",
       " 'computer-aided',\n",
       " 'computer-assisted',\n",
       " 'computer-generated',\n",
       " 'computerized',\n",
       " 'computing',\n",
       " 'conceding',\n",
       " 'concentrated',\n",
       " 'concentrating',\n",
       " 'concerned',\n",
       " 'concluded',\n",
       " 'condemned',\n",
       " 'condemning',\n",
       " 'conducted',\n",
       " 'conducting',\n",
       " 'confined',\n",
       " 'confirmed',\n",
       " 'confused',\n",
       " 'connected',\n",
       " 'consented',\n",
       " 'considered',\n",
       " 'considering',\n",
       " 'consisting',\n",
       " 'construed',\n",
       " 'consulting',\n",
       " 'contacted',\n",
       " 'contained',\n",
       " 'containing',\n",
       " 'contesting',\n",
       " 'continued',\n",
       " 'continuing',\n",
       " 'contracted',\n",
       " 'contributed',\n",
       " 'contributing',\n",
       " 'controlled',\n",
       " 'controlling',\n",
       " 'converted',\n",
       " 'converting',\n",
       " 'convicted',\n",
       " 'convinced',\n",
       " 'cooled',\n",
       " 'cooperating',\n",
       " 'copied',\n",
       " 'copying',\n",
       " 'corn-buying',\n",
       " 'corrected',\n",
       " 'correcting',\n",
       " 'cost-cutting',\n",
       " 'cost-sharing',\n",
       " 'counseling',\n",
       " 'counting',\n",
       " 'coupled',\n",
       " 'court-ordered',\n",
       " 'covered',\n",
       " 'covering',\n",
       " 'cranked',\n",
       " 'crashing',\n",
       " 'created',\n",
       " 'creating',\n",
       " 'credit-rating',\n",
       " 'crippled',\n",
       " 'criticized',\n",
       " 'crossed',\n",
       " 'crossing',\n",
       " 'crowded',\n",
       " 'cruising',\n",
       " 'crushed',\n",
       " 'crying',\n",
       " 'cultivated',\n",
       " 'curbed',\n",
       " 'curbing',\n",
       " 'curled',\n",
       " 'current-carrying',\n",
       " 'curtailed',\n",
       " 'cushioned',\n",
       " 'customized',\n",
       " 'cutting',\n",
       " 'damaged',\n",
       " 'damaging',\n",
       " 'dancing',\n",
       " 'darned',\n",
       " 'dashed',\n",
       " 'dating',\n",
       " 'dead-eyed',\n",
       " 'dealing',\n",
       " 'decided',\n",
       " 'declared',\n",
       " 'declaring',\n",
       " 'declined',\n",
       " 'declining',\n",
       " 'decorated',\n",
       " 'decried',\n",
       " 'deducting',\n",
       " 'deemed',\n",
       " 'defeated',\n",
       " 'defended',\n",
       " 'defined',\n",
       " 'defying',\n",
       " 'delayed',\n",
       " 'deliberating',\n",
       " 'delisted',\n",
       " 'delivered',\n",
       " 'delivering',\n",
       " 'demanding',\n",
       " 'demonstrating',\n",
       " 'denied',\n",
       " 'denouncing',\n",
       " 'denying',\n",
       " 'depended',\n",
       " 'depending',\n",
       " 'depleted',\n",
       " 'depressed',\n",
       " 'deprived',\n",
       " 'derived',\n",
       " 'descending',\n",
       " 'described',\n",
       " 'deserving',\n",
       " 'designated',\n",
       " 'designed',\n",
       " 'designing',\n",
       " 'desired',\n",
       " 'despised',\n",
       " 'detailed',\n",
       " 'deteriorated',\n",
       " 'deteriorating',\n",
       " 'determined',\n",
       " 'deterring',\n",
       " 'devastating',\n",
       " 'developed',\n",
       " 'developing',\n",
       " 'devised',\n",
       " 'devoted',\n",
       " 'devouring',\n",
       " 'diagnosed',\n",
       " 'died',\n",
       " 'diluted',\n",
       " 'diming',\n",
       " 'diminished',\n",
       " 'directed',\n",
       " 'directing',\n",
       " 'disaffected',\n",
       " 'disagreed',\n",
       " 'disappointed',\n",
       " 'disappointing',\n",
       " 'disapproved',\n",
       " 'discarded',\n",
       " 'disciplined',\n",
       " 'disclosed',\n",
       " 'disclosing',\n",
       " 'discontinued',\n",
       " 'discontinuing',\n",
       " 'discouraging',\n",
       " 'discovered',\n",
       " 'discussed',\n",
       " 'discussing',\n",
       " 'disembodied',\n",
       " 'dismayed',\n",
       " 'dismissed',\n",
       " 'disposed',\n",
       " 'disputed',\n",
       " 'disseminating',\n",
       " 'distinguished',\n",
       " 'distorted',\n",
       " 'distributed',\n",
       " 'disturbing',\n",
       " 'diversified',\n",
       " 'diversifying',\n",
       " 'divided',\n",
       " 'dividing',\n",
       " 'documented',\n",
       " 'doing',\n",
       " 'doling',\n",
       " 'dollar-denominated',\n",
       " 'dominated',\n",
       " 'dominating',\n",
       " 'doubled',\n",
       " 'doubted',\n",
       " 'downgraded',\n",
       " 'downgrading',\n",
       " 'drafted',\n",
       " 'drawing',\n",
       " 'dreamed',\n",
       " 'dressed',\n",
       " 'drifted',\n",
       " 'drinking',\n",
       " 'driving',\n",
       " 'drooled',\n",
       " 'dropped',\n",
       " 'dubbed',\n",
       " 'duckling',\n",
       " 'dumbfounded',\n",
       " 'dumped',\n",
       " 'during',\n",
       " 'dwindling',\n",
       " 'earned',\n",
       " 'earning',\n",
       " 'eased',\n",
       " 'easing',\n",
       " 'eating',\n",
       " 'echoed',\n",
       " 'edged',\n",
       " 'editing',\n",
       " 'educated',\n",
       " 'elected',\n",
       " 'eliminated',\n",
       " 'eliminating',\n",
       " 'embarrassing',\n",
       " 'embroiled',\n",
       " 'emerged',\n",
       " 'emerging',\n",
       " 'emphasized',\n",
       " 'employed',\n",
       " 'empowered',\n",
       " 'enabled',\n",
       " 'enabling',\n",
       " 'enacted',\n",
       " 'encircling',\n",
       " 'enclosed',\n",
       " 'encouraging',\n",
       " 'encroaching',\n",
       " 'ended',\n",
       " 'ending',\n",
       " 'endorsed',\n",
       " 'engaged',\n",
       " 'engaging',\n",
       " 'engineered',\n",
       " 'engineering',\n",
       " 'enhanced',\n",
       " 'enjoyed',\n",
       " 'enjoying',\n",
       " 'enlarged',\n",
       " 'enraged',\n",
       " 'ensnarled',\n",
       " 'entangled',\n",
       " 'entered',\n",
       " 'entering',\n",
       " 'entertaining',\n",
       " 'enticed',\n",
       " 'entitled',\n",
       " 'entrenched',\n",
       " 'entrusted',\n",
       " 'equaling',\n",
       " 'equipped',\n",
       " 'escalated',\n",
       " 'escaped',\n",
       " 'established',\n",
       " 'establishing',\n",
       " 'estimated',\n",
       " 'evaluated',\n",
       " 'evaluating',\n",
       " 'evaporated',\n",
       " 'evening',\n",
       " 'everything',\n",
       " 'evoking',\n",
       " 'evolved',\n",
       " 'exacerbated',\n",
       " 'examined',\n",
       " 'exceed',\n",
       " 'exceeded',\n",
       " 'exceeding',\n",
       " 'exchanging',\n",
       " 'excited',\n",
       " 'exciting',\n",
       " 'executed',\n",
       " 'executing',\n",
       " 'exercised',\n",
       " 'exerting',\n",
       " 'exhausted',\n",
       " 'exhibited',\n",
       " 'existed',\n",
       " 'existing',\n",
       " 'expanded',\n",
       " 'expanding',\n",
       " 'expected',\n",
       " 'expecting',\n",
       " 'expedited',\n",
       " 'expelled',\n",
       " 'experienced',\n",
       " 'experiencing',\n",
       " 'expired',\n",
       " 'explained',\n",
       " 'explaining',\n",
       " 'exploded',\n",
       " 'export-oriented',\n",
       " 'exposed',\n",
       " 'expressed',\n",
       " 'expressing',\n",
       " 'expunged',\n",
       " 'extended',\n",
       " 'extending',\n",
       " 'exuded',\n",
       " 'eyeing',\n",
       " 'fabled',\n",
       " 'faced',\n",
       " 'facing',\n",
       " 'factoring',\n",
       " 'faded',\n",
       " 'failed',\n",
       " 'failing',\n",
       " 'fainting',\n",
       " 'falling',\n",
       " 'faltered',\n",
       " 'famed',\n",
       " 'family-planning',\n",
       " 'fared',\n",
       " 'fashioned',\n",
       " 'fast-growing',\n",
       " 'fastest-growing',\n",
       " 'fattened',\n",
       " 'favored',\n",
       " 'fawning',\n",
       " 'feared',\n",
       " 'featured',\n",
       " 'featuring',\n",
       " 'fed',\n",
       " 'feed',\n",
       " 'feeling',\n",
       " 'fetching',\n",
       " 'fielded',\n",
       " 'fighting',\n",
       " 'filed',\n",
       " 'filing',\n",
       " 'filled',\n",
       " 'filling',\n",
       " 'finalized',\n",
       " 'financed',\n",
       " 'financing',\n",
       " 'finding',\n",
       " 'fined',\n",
       " 'finished',\n",
       " 'fired',\n",
       " 'firmed',\n",
       " 'fixed',\n",
       " 'fizzled',\n",
       " 'fled',\n",
       " 'fledgling',\n",
       " 'fleeting',\n",
       " 'flirted',\n",
       " 'floated',\n",
       " 'flooded',\n",
       " 'focused',\n",
       " 'focusing',\n",
       " 'folded',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'forced',\n",
       " 'forcing',\n",
       " 'forecasting',\n",
       " 'foreign-led',\n",
       " 'formed',\n",
       " 'forthcoming',\n",
       " 'founded',\n",
       " 'foundering',\n",
       " 'fretted',\n",
       " 'frightened',\n",
       " 'frustrating',\n",
       " 'fueled',\n",
       " 'fueling',\n",
       " 'full-fledged',\n",
       " 'fuming',\n",
       " 'functioning',\n",
       " 'funded',\n",
       " 'funding',\n",
       " 'fundraising',\n",
       " 'futures-related',\n",
       " 'gained',\n",
       " 'gaining',\n",
       " 'galling',\n",
       " 'galvanized',\n",
       " 'gambling',\n",
       " 'gauging',\n",
       " 'generated',\n",
       " 'getting',\n",
       " 'giving',\n",
       " 'going',\n",
       " 'good-hearted',\n",
       " 'good-natured',\n",
       " 'gored',\n",
       " 'government-certified',\n",
       " 'government-funded',\n",
       " 'government-owned',\n",
       " 'graduated',\n",
       " 'granted',\n",
       " 'granting',\n",
       " 'greed',\n",
       " 'gripping',\n",
       " 'growing',\n",
       " 'guaranteed',\n",
       " 'guarding',\n",
       " 'guided',\n",
       " 'gut-wrenching',\n",
       " 'hailed',\n",
       " 'hailing',\n",
       " 'halted',\n",
       " 'hampered',\n",
       " 'handed',\n",
       " 'handled',\n",
       " 'handling',\n",
       " 'happened',\n",
       " 'happening',\n",
       " 'hard-charging',\n",
       " 'hard-drinking',\n",
       " 'hard-hitting',\n",
       " 'harmed',\n",
       " 'harped',\n",
       " 'harvested',\n",
       " 'hauled',\n",
       " 'hauling',\n",
       " 'having',\n",
       " 'headed',\n",
       " 'heading',\n",
       " 'headlined',\n",
       " 'healing',\n",
       " 'hearing',\n",
       " 'heated',\n",
       " 'heating',\n",
       " 'hedging',\n",
       " 'heightened',\n",
       " 'helped',\n",
       " 'helping',\n",
       " 'high-flying',\n",
       " 'high-minded',\n",
       " 'high-polluting',\n",
       " 'high-priced',\n",
       " 'high-rolling',\n",
       " 'high-speed',\n",
       " 'higher-salaried',\n",
       " 'highest-pitched',\n",
       " 'hired',\n",
       " 'hitting',\n",
       " 'holding',\n",
       " 'hoped',\n",
       " 'hosted',\n",
       " 'housing',\n",
       " 'hugging',\n",
       " 'hundred',\n",
       " 'hunted',\n",
       " 'hurting',\n",
       " 'identified',\n",
       " 'ignored',\n",
       " 'ignoring',\n",
       " 'impaired',\n",
       " 'impeding',\n",
       " 'impending',\n",
       " 'implemented',\n",
       " 'implied',\n",
       " 'imported',\n",
       " 'imposed',\n",
       " 'imposing',\n",
       " 'impressed',\n",
       " 'improved',\n",
       " 'improving',\n",
       " 'incentive-backed',\n",
       " 'inched',\n",
       " 'inching',\n",
       " 'included',\n",
       " 'including',\n",
       " 'incorporated',\n",
       " 'increased',\n",
       " 'increasing',\n",
       " 'incurred',\n",
       " 'indeed',\n",
       " 'index-related',\n",
       " 'indicated',\n",
       " 'indicating',\n",
       " 'indulging',\n",
       " 'industrialized',\n",
       " 'industry-supported',\n",
       " 'inflated',\n",
       " 'influenced',\n",
       " 'influencing',\n",
       " 'infringed',\n",
       " 'inherited',\n",
       " 'initialing',\n",
       " 'initiated',\n",
       " 'initiating',\n",
       " 'injecting',\n",
       " 'injuring',\n",
       " 'inkling',\n",
       " 'inquiring',\n",
       " 'inserted',\n",
       " 'insider-trading',\n",
       " 'insinuating',\n",
       " 'insisted',\n",
       " 'inspired',\n",
       " 'installed',\n",
       " 'installing',\n",
       " 'instituted',\n",
       " 'instructed',\n",
       " 'insured',\n",
       " 'integrated',\n",
       " 'intended',\n",
       " 'intentioned',\n",
       " 'interest-bearing',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'interrogated',\n",
       " 'interviewed',\n",
       " 'intriguing',\n",
       " 'introduced',\n",
       " 'introducing',\n",
       " 'invented',\n",
       " 'inverted',\n",
       " 'invested',\n",
       " 'investigating',\n",
       " 'investing',\n",
       " 'inviting',\n",
       " 'involved',\n",
       " 'involving',\n",
       " 'issued',\n",
       " 'issuing',\n",
       " 'jeopardizing',\n",
       " 'joined',\n",
       " 'joining',\n",
       " 'judged',\n",
       " 'jumped',\n",
       " 'jumping',\n",
       " 'justified',\n",
       " 'justifying',\n",
       " 'keeping',\n",
       " 'kicked',\n",
       " 'kidnapping',\n",
       " 'killed',\n",
       " 'killing',\n",
       " 'knitted',\n",
       " 'knocked',\n",
       " 'labeled',\n",
       " 'labeling',\n",
       " 'labor-backed',\n",
       " 'lacked',\n",
       " 'lagging',\n",
       " 'land-idling',\n",
       " 'landing',\n",
       " 'lasted',\n",
       " 'lasting',\n",
       " 'lauded',\n",
       " 'laughing',\n",
       " 'launched',\n",
       " 'lawmaking',\n",
       " 'laying',\n",
       " 'leading',\n",
       " 'learned',\n",
       " 'learning',\n",
       " 'leasing',\n",
       " 'leaving',\n",
       " 'led',\n",
       " 'lending',\n",
       " 'lengthened',\n",
       " 'lessening',\n",
       " 'letter-writing',\n",
       " 'letting',\n",
       " 'leveling',\n",
       " 'leveraged',\n",
       " 'leveraging',\n",
       " 'licensed',\n",
       " 'licensing',\n",
       " 'lifted',\n",
       " ...]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('(ed|ing)$', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:crimson\">[연습😉3]</span> 계속 읽기 전에 위에 나열된 기호들의 의미를 파악하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/re_cheatsheet1.png\" width=\"600\" style=\"margin-left: auto; margin-right: auto\">\n",
    "<p style=\"text-align: center;\">와일드카드, 범위 및 클로저를 포함한 기본 정규 표현식 메타 문자</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 정규표현식의 응용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Pieces 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u',\n",
       " 'e',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'e',\n",
       " 'i',\n",
       " 'a',\n",
       " 'i',\n",
       " 'o',\n",
       " 'i',\n",
       " 'o',\n",
       " 'u']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'supercalifragilisticexpialidocious'\n",
    "re.findall(r'[aeiou]', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(re.findall(r'[aeiou]', word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('io', 549),\n",
       " ('ea', 476),\n",
       " ('ie', 331),\n",
       " ('ou', 329),\n",
       " ('ai', 261),\n",
       " ('ia', 253),\n",
       " ('ee', 217),\n",
       " ('oo', 174),\n",
       " ('ua', 109),\n",
       " ('au', 106),\n",
       " ('ue', 105),\n",
       " ('ui', 95)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "fd = nltk.FreqDist(vs for word in wsj\n",
    "                      for vs in re.findall(r'[aeiou]{2,}', word))\n",
    "fd.most_common(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:crimson\">[연습😉4]</span>** W3C Date Time Format에서는 날짜가 2022-04-05 처럼 표현된다. 아래 파이썬 코드에서 `?`를 정규표현식으로 바꿔서 string '2022-04-05'를 정수 리스트 [2022, 4, 5] 가 되게 하라.\n",
    "```\\\n",
    "[int(n) for n in re.findall(?, '2022-04-05')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Pieces를 사용한 추가 작업\n",
    "- 영어 텍스트는 예측가능성이 높아서 단어 중간의 모음들을 생략해도 읽기가 용이한 특징이 있다\n",
    "- declaration -> dclrtn, inalienable -> inlnble\n",
    "- 첫 모음 또는 끝 모음은 그대로 둔다\n",
    "- 첫 모음 순서, 끝 모음 순서, 그 다음에는 모든 자음 순서가 맞는 것을 찾는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(word):\n",
    "    pieces = re.findall(regexp, word)\n",
    "    return ''.join(pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Universal', 'Declaration', 'of', 'Human', 'Rights', ...]\n",
      "Unvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and\n",
      "of the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn\n",
      "of frdm , jstce and pce in the wrld , Whrs dsrgrd and cntmpt fr hmn\n",
      "rghts hve rsltd in brbrs acts whch hve outrgd the cnscnce of mnknd ,\n",
      "and the advnt of a wrld in whch hmn bngs shll enjy frdm of spch and\n"
     ]
    }
   ],
   "source": [
    "english_udhr = nltk.corpus.udhr.words('English-Latin1')\n",
    "print(english_udhr)\n",
    "print(nltk.tokenwrap(compress(w) for w in english_udhr[:75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 정규표현과 conditional frequency distribution을 결합한다. Rotokas어에서 ka나 si 같은 모든 자음-모음 sequence를 추출한다. 이것들 모두가 쌍이므로, conditional frequency distribution을 초기화하는데 사용할 수 있다. 그런 다음 각 쌍의 빈도를 표 형태로 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a   e   i   o   u \n",
      "k 418 148  94 420 173 \n",
      "p  83  31 105  34  51 \n",
      "r 187  63  84  89  79 \n",
      "s   0   0 100   2   1 \n",
      "t  47   8   0 148  37 \n",
      "v  93  27 105  48  49 \n"
     ]
    }
   ],
   "source": [
    "rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')\n",
    "# print(rotokas_words)\n",
    "cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
    "cfd = nltk.ConditionalFreqDist(cvs)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 's'와 't' 행을 보면 이들이 부분적으로 \"상보적 분포\"를 갖는다는 것을 알 수 있다.\n",
    "- 이들은 언어상 상이한 음소가 아니라는 증거이다.\n",
    "- 따라서 Rotokas 알파벳에서 s를 빼버리고 t 다음에 i 가 나오면 t를 s로 발음하도록 하는 발음규칙을 만들수도 있을 것이다.\n",
    "- 'su'는 빈도가 1인데 이것은 *kasuari*로 영어 'cassowary'에서 가져온 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가적인 분석을 원한다면 주어진 consonant-vowel pair를 포함하는 단어 리스트를 빠르게 찾을 수 있도록 인덱스를 만드는 것이 편리할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kasuari']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_word_pairs = [(cv, w) for w in rotokas_words\n",
    "                         for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
    "cv_index = nltk.Index(cv_word_pairs)\n",
    "cv_index['su']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kaapo',\n",
       " 'kaapopato',\n",
       " 'kaipori',\n",
       " 'kaiporipie',\n",
       " 'kaiporivira',\n",
       " 'kapo',\n",
       " 'kapoa',\n",
       " 'kapokao',\n",
       " 'kapokapo',\n",
       " 'kapokapo',\n",
       " 'kapokapoa',\n",
       " 'kapokapoa',\n",
       " 'kapokapora',\n",
       " 'kapokapora',\n",
       " 'kapokaporo',\n",
       " 'kapokaporo',\n",
       " 'kapokari',\n",
       " 'kapokarito',\n",
       " 'kapokoa',\n",
       " 'kapoo',\n",
       " 'kapooto',\n",
       " 'kapoovira',\n",
       " 'kapopaa',\n",
       " 'kaporo',\n",
       " 'kaporo',\n",
       " 'kaporopa',\n",
       " 'kaporoto',\n",
       " 'kapoto',\n",
       " 'karokaropo',\n",
       " 'karopo',\n",
       " 'kepo',\n",
       " 'kepoi',\n",
       " 'keposi',\n",
       " 'kepoto']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_index['po']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 어간 찾기\n",
    "- 검색 엔진에 입력할 때 단어의 끝부분이 다른 것을 신경쓰지 않아도 되는 것은 stemming 기능 때문이다.\n",
    "- 가장 간단한 방법으로 suffix처럼 보이는 것은 다 잘라버리는 방법을 사용해보자.\n",
    "- 물론 NLTK에는 stemmer가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- disjunction('OR'ing) of all suffixes\n",
    "- disjunction의 범위를 한정하기 위해 괄호를 쓴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ing']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- matching이 된 단어 'processing'을 반환하지 않고 suffix를 반환하는 이유는 괄호 () 때문\n",
    "- 단어를 출력하려면 `?:` 를 추가해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['processing']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stem과 suffix 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('process', 'ing')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*좋아 보이지만 아직 문제가 있다.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('processe', 's')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'es' 대신 's'를 찾았다.\n",
    "- 첫번째 연산자가 'greedy'하여 '.\\*' 부분이 가능한 많은 입력을 소비하려하기 때문\n",
    "- `*` 연산자의 'non-greedy' 버전 `*?`을 사용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('process', 'es')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 번째 괄호를 option으로 만들면 empty suffix에서도 잘 작동한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('language', '')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', 'language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아직도 문제가 많지만 그대로 진행하여, 전체 문서에 적용할 stemmer 함수를 만들자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DENNIS',\n",
       " ':',\n",
       " 'Listen',\n",
       " ',',\n",
       " 'strange',\n",
       " 'women',\n",
       " 'ly',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'i',\n",
       " 'no',\n",
       " 'basi',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'Supreme',\n",
       " 'execut',\n",
       " 'power',\n",
       " 'deriv',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mandate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcical',\n",
       " 'aquatic',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = word_tokenize(raw)\n",
    "[stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *ponds*에서 *s*를 제거했으나, *is*와 *basis*에서도 제거했다.\n",
    "- *distribut*나 *deriv* 같은 이상한 단어를 만들었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화된 텍스트 검색\n",
    "- `< >`: token boundary를 나타내며 그 사이의 공백은 무시한다.\n",
    "- `(<.*>)`: single token, ()는 일치하는 단어(phrase 아니고)만 찾는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      "mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      "pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      "brave; brave; brave\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg, nps_chat\n",
    "moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))\n",
    "moby.findall(r\"<a> (<.*>) <man>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      "mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      "pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      "brave; brave; brave\n"
     ]
    }
   ],
   "source": [
    "moby.findall(r'<a> (<.*>) <man>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = nltk.Text(nps_chat.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you rule bro; telling you bro; u twizted bro\n"
     ]
    }
   ],
   "source": [
    "chat.findall(r\"<.*> <.*> <bro>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la\n",
      "la la; lovely lol lol love; lol lol lol.; la la la; la la la\n"
     ]
    }
   ],
   "source": [
    "chat.findall(r\"<l.*>{3,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:crimson\">[연습😉5]</span>** 패턴 p와 일치하는 모든 위치를 표시하기 위해 문자열 s에 주석을 추가하는 `nltk.re_show(p, s)`와 정규식 탐색을 위한 그래픽 인터페이스를 제공하는 nltk.app.nemo()를 사용하여 정규표현식의 패턴과 대입에 대한 이해를 통합하시오. 더 많은 연습을 위해 이 장의 끝에 있는 정규표현식 몇 가지를 연습해 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 조사하려는 언어학적 현상이 특정 단어들에 엮여 있으면 검색 패턴을 만들기 쉬워진다.\n",
    "- \"*x and other ys*\"라는 표현은 대규모 코퍼스에서 hypernym을 찾기 쉽게 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed and other activities; water and other liquids; tomb and other\n",
      "landmarks; Statues and other monuments; pearls and other jewels;\n",
      "charts and other items; roads and other features; figures and other\n",
      "objects; military and other areas; demands and other factors;\n",
      "abstracts and other compilations; iron and other metals\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))\n",
    "hobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그러나 항상 false positive가 있음을 명심하자. \n",
    "- 항상 false negative 문제도 있기 마련인데, 검색 패턴에 일치하는 것이 없다고 어떤 언어학적 현상이 코퍼스에 없다고 결론을 내리는 것은 위험하다. 적합한 패턴에 대해 더 신중히 했어야 했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precision / recall 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 텍스트 토큰화를 위한 정규표현식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 토큰화는 문자열을 쪼개어 언어 데이터를 구성하는 식별가능한 언어학적 단위들로 만드는 작업이다\n",
    "- 지금까지 다룬 많은 말뭉치들이 이미 토큰화되어 있었으며, nltk도 약간의 토큰화 기능을 제공한다\n",
    "- 정규표현식을 사용해 토큰화 시켜본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 간단한 토큰화 방법\n",
    "- 우선, 공백으로 나누기\n",
    "- 텍스트: Alice's Adventures in Wonderland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\n",
    "though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\n",
    "well without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone\\nthough),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very\\nwell', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(re.split(r' ', raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- split() 함수를 사용하면 `\\n`가 포함되므로 원하던 것이 아님\n",
    "- space, tab, newline 모두 매치시켜야 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n"
     ]
    }
   ],
   "source": [
    "print(re.split(r'[ \\t\\n]+', raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- '(not' 과 'herself,' 같은 토큰이 나왔다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', 'I', 'won', 't', 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', 'Soup', 'does', 'very', 'well', 'without', 'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered', '']\n"
     ]
    }
   ],
   "source": [
    "print(re.split(r'\\W+', raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시작과 끝 부분에 빈 문자열이 나옴\n",
    "- `\\w+|\\S\\w*`: 임의의 문자열을 찾고 없으면 (non-whitespace 문자+단어문자열) 형태를 찾음\n",
    "- 구둣점이 처음에 오면 문자열로 간주, 두 개이상의 구둣점이 처음에 오면 분리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'When\", 'I', \"'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'I\", 'won', \"'t\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '-', '-Maybe', 'it', \"'s\", 'always', 'pepper', 'that', 'makes', 'people', 'hot', '-tempered', ',', \"'\", '.', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'\\w+|\\S\\w*', raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `w+`를 일반화시켜 단어 내부의 hyphens 와 apostrophes을 허용해보자\n",
    "- `\\w+([-']\\w+)*`: 임의의 단어문자열+ hyphens 또는 apostrophes + 임의의 단어문자열\n",
    "- 'hot-tempered' 나 'it's' 같은 단어가 포함될 것이다. \n",
    "- `?`는 앞에서 논의한 이유로 포함시켜야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", 'When', \"I'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'\", 'I', \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '--', 'Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', 'hot-tempered', ',', \"'\", '...']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `[-.(]+`: double hyphen, ellipsis 및 open parenthesis를 별도 토큰 처리한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/re_exp_symbols.png\" width=\"600\" style=\"margin-left: auto; margin-right: auto\">\n",
    "<p style=\"text-align: center;\">정규표현식 심볼</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK의 정규표현식 Tokenizer\n",
    "- `nltk.regexp_tokenize()`: 괄호의 특별처리 등이 필요 없는 점 등 때문에 `re.findall()` 보다 능률적. \n",
    "- verbose flag `(?x)`: 정규식 문자열 내의 공백 및 커멘트를 없앤다.\n",
    "- verbose flag을 쓰면 공백을 지정하기 위해 '　'을 사용할 수 없게 되며, \\s를 사용해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "pattern = r'''(?x)     # set flag to allow verbose regexps\n",
    "    (?:[A-Z]\\.)+       # abbreviations, e.g. U.S.A.\n",
    "  | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
    "  | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
    "  | \\.\\.\\.             # ellipsis\n",
    "  | [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
    "'''\n",
    "nltk.regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `set(tokens).difference(wordlist)`: worldlist와 비교해서 wordlist에 없는 토큰을 반환하여 **토큰화 결과 비교** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화 관련 추가 문제들\n",
    "- 토큰화는 생각보다 훨씬 어렵다\n",
    "- 한가지 해법으로 해결할 수 없으며, 문제 영역에 따라 결정을 해야 한다.\n",
    "- 작성한 tokenizer의 출력을 고품질의 (gold-standard) 토큰들과 비교하기 위해 raw text를 검토하는 것이 바람직\n",
    "- NLTK에는 raw Wall Street Journal text (nltk.corpus.treebank_raw.raw())와 토큰화된 버전(nltk.corpus.treebank.words())을 포함하여 Penn Treebank data 샘플이 포함되어 있다.\n",
    "- *didn't* 같은 축약형은 do와 not 또는 do와 n't로 분리하는 것이 의미를 분석하기에 유리할 수 있다. 이를 위해 lookup table을 사용할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.7   Segmentation\n",
    "- Tokenization은 segmentation 문제의 더욱 일반화된 사례이다.\n",
    "- 앞서 다룬 방법들과는 전혀 다른 기법들의 예를 살펴 본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.250994070456922"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.corpus.brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.corpus.brown.sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 처리는 코퍼스가 문장 구분이 되어있다는 전제로 처리 가능했다.\n",
    "- 코퍼스가 문자 스트림으로 되어 있는 경우가 흔하다.\n",
    "- NLTK는 **Punkt sentence segmenter**([Kiss & Strunk, 2006](https://www.nltk.org/book/bibliography.html#kissstrunk2006))를 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Nonsense!\"',\n",
      " 'said Gregory, who was very rational when anyone else\\nattempted paradox.',\n",
      " '\"Why do all the clerks and navvies in the\\n'\n",
      " 'railway trains look so sad and tired, so very sad and tired?',\n",
      " 'I will\\ntell you.',\n",
      " 'It is because they know that the train is going right.',\n",
      " 'It\\n'\n",
      " 'is because they know that whatever place they have taken a ticket\\n'\n",
      " 'for that place they will reach.',\n",
      " 'It is because after they have\\n'\n",
      " 'passed Sloane Square they know that the next station must be\\n'\n",
      " 'Victoria, and nothing but Victoria.',\n",
      " 'Oh, their wild rapture!',\n",
      " 'oh,\\n'\n",
      " 'their eyes like stars and their souls again in Eden, if the next\\n'\n",
      " 'station were unaccountably Baker Street!\"',\n",
      " '\"It is you who are unpoetical,\" replied the poet Syme.']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n",
    "sents = nltk.sent_tokenize(text)\n",
    "pprint.pprint(sents[79:89])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentence segmentation은 마침표(`.`) 때문에 특히 어렵다\n",
    "- 문장 종료와 약어 표시 외에도 어떤 경우 약어와 종료를 겸하기도 함\n",
    "- 또 다른 접근법은 Chapter 6의 Section 2 Further Examples of Supervised Classification을 참조하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Word Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 띄어쓰기가 없는 언어에서는 특히 토큰화가 어렵다.\n",
    "- 중국어에서, 爱国人(ai4 \"love\" (verb), guo2 \"country\", ren2 \"person\")는 (爱国 + 人)(country-loving person) 또는 (爱 + 国人)(love country-person)으로 토큰화될 수 있다.\n",
    "- 비슷한 문제가 음성 언어 처리에서도 일어난다. 듣는 사람은 연속적인 음성 흐름을 분할해야 한다.\n",
    "- 단어를 미리 알고 있지 않은 경우 문제가 더 어려워진다.\n",
    "- 단어 경계를 없앤 다음 예를 보자:\n",
    "> (예)  \n",
    "> a. doyouseethekitty  \n",
    "> b. seethedoggy  \n",
    "> c. doyoulikethekitty  \n",
    "> d. likethedoggy  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 글자에 대해 글자 다음에 word-break가 나타나는가 여부에 따라 1 또는 0을 표시해둔다\n",
    "- 이 방법은 Chapter 7의 \"Chunking\"에서 집중적으로 사용된다.\n",
    "- 가정: learner에게 utterance break가 주어진다(이는 실제 종종 *늘어진 멈춤*에 대응된다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"  # 초기 segmentation\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\"  # 목표 segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(text, segs):\n",
    "    words = []\n",
    "    last = 0\n",
    "    for i in range(len(segs)):\n",
    "        if segs[i] == '1':\n",
    "            words.append(text[last:i+1])\n",
    "            last = i+1\n",
    "    words.append(text[last:])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n",
    "segment(text, seg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do',\n",
       " 'you',\n",
       " 'see',\n",
       " 'the',\n",
       " 'kitty',\n",
       " 'see',\n",
       " 'the',\n",
       " 'doggy',\n",
       " 'do',\n",
       " 'you',\n",
       " 'like',\n",
       " 'the',\n",
       " 'kitty',\n",
       " 'like',\n",
       " 'the',\n",
       " 'doggy']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment(text, seg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- learner는 단어를 취득하여 내부 lexicon에 저장한다.\n",
    "- [Brent (1995)](https://www.nltk.org/book/bibliography.html#brent1995)의 방법을 따라 **목적함수**를 만들 수 있다(그림 3.8)\n",
    "- 목적함수 = 사전(lexicon) 크기(단어 내 글자 수 + 각 단어의 끝을 나타내는 별도의 구획 문자)와 lexicon에서 원래 텍스트를 복원하기 위해 필요한 정보량을 기반으로 하는 scoring function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/segmentation_objective.png\" width=\"600\" style=\"margin-left: auto; margin-right: auto\">\n",
    "<p style=\"text-align: center;\">세크멘테이션 목적함수</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(text, segs):\n",
    "    words = segment(text, segs)\n",
    "    text_size = len(words)\n",
    "    lexicon_size = sum(len(word) + 1 for word in set(words))\n",
    "    return text_size + lexicon_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doyou',\n",
       " 'see',\n",
       " 'thekitt',\n",
       " 'y',\n",
       " 'see',\n",
       " 'thedogg',\n",
       " 'y',\n",
       " 'doyou',\n",
       " 'like',\n",
       " 'thekitt',\n",
       " 'y',\n",
       " 'like',\n",
       " 'thedogg',\n",
       " 'y']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n",
    "seg3 = \"0000100100000011001000000110000100010000001100010000001\"\n",
    "segment(text, seg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(text, seg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(text, seg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(text, seg1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 마지막 단계는 목적함수를 최소화시키는 0과 1의 패턴을 찾는 것이다.\n",
    "- 최적화된 segmentation에 단어 *thekitty*가 나오는 이유는 이것을 더 분리하기에는 정보가 부족하기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-Deterministic Search Using Simulated Annealing:**  \n",
    "- Phrase segmenation에만 적용\n",
    "- \"temperature\"에 비례하여 0과 1을 무작위로 교란시킨다\n",
    "- 매 반복마다 temperature는 내려가고 단어 경계의 교란은 줄어든다\n",
    "- 알고리즘이 non-deterministic이므로 결과는 매번 달라질 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def flip(segs, pos):\n",
    "    return segs[:pos] + str(1-int(segs[pos])) + segs[pos+1:]\n",
    "\n",
    "def flip_n(segs, n):\n",
    "    for i in range(n):\n",
    "        segs = flip(segs, randint(0, len(segs)-1))\n",
    "    return segs\n",
    "\n",
    "def anneal(text, segs, iterations, cooling_rate):\n",
    "    temperature = float(len(segs))\n",
    "    while temperature > 0.5:\n",
    "        best_segs, best = segs, evaluate(text, segs)\n",
    "        for i in range(iterations):\n",
    "            guess = flip_n(segs, round(temperature))\n",
    "            score = evaluate(text, guess)\n",
    "            if score < best:\n",
    "                best, best_segs = score, guess\n",
    "        score, segs = best, best_segs\n",
    "        temperature = temperature / cooling_rate\n",
    "        print(evaluate(text, segs), segment(text, segs))\n",
    "    print()\n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyouseeth', 'ekitty', 'seethedogg', 'ydoyouliketh', 'ekitty', 'like', 'thedoggy']\n",
      "63 ['doyouseeth', 'ekitty', 'seethedogg', 'ydoyouliketh', 'ekitty', 'like', 'thedoggy']\n",
      "61 ['doyouseeth', 'ekitty', 'seet', 'hedoggy', 'doyouliketh', 'ekitty', 'lik', 'e', 't', 'hedoggy']\n",
      "60 ['doyousee', 'th', 'ekitty', 'seet', 'hedoggy', 'doyou', 'like', 't', 'h', 'ekitty', 'like', 't', 'hedoggy']\n",
      "57 ['doyouseeth', 'ekitty', 'se', 'et', 'hedoggy', 'doyou', 'liket', 'h', 'ekitty', 'liket', 'hedoggy']\n",
      "54 ['doyou', 'seeth', 'ekitty', 'seet', 'hedoggy', 'doyou', 'like', 'th', 'ekitty', 'like', 't', 'hedoggy']\n",
      "49 ['doyou', 'see', 'th', 'ekitty', 'see', 't', 'hedoggy', 'doyou', 'like', 'th', 'ekitty', 'like', 't', 'hedoggy']\n",
      "49 ['doyou', 'see', 'th', 'ekitty', 'see', 't', 'hedoggy', 'doyou', 'like', 'th', 'ekitty', 'like', 't', 'hedoggy']\n",
      "49 ['doyou', 'see', 'th', 'ekitty', 'see', 't', 'hedoggy', 'doyou', 'like', 'th', 'ekitty', 'like', 't', 'hedoggy']\n",
      "46 ['doyou', 'see', 'th', 'ekitty', 'see', 'thedoggy', 'doyou', 'like', 'th', 'ekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0000100100000001001000000010000100010000000100010000000'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "anneal(text, seg1, 5000, 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터가 충분하다면 어느 정도의 정확도를 갖을 수 있다.\n",
    "- 이 방법은 시각적인 단어 경계 표상이 없는 언어체계에 대한 토큰화에 적용될 수 있을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.8 정수 인코딩(Integer Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) dictionary 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A barber is a person.',\n",
       " 'a barber is good person.',\n",
       " 'a barber is huge person.',\n",
       " 'he Knew A Secret!',\n",
       " 'The Secret He Kept is huge secret.',\n",
       " 'Huge secret.',\n",
       " 'His barber kept his word.',\n",
       " 'a barber kept his word.',\n",
       " 'His barber kept his secret.',\n",
       " 'But keeping and keeping such a huge secret to himself was driving the barber crazy.',\n",
       " 'the barber went up a huge mountain.']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 분할\n",
    "sentences = sent_tokenize(raw_text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "preprocessed_sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for sentence in sentences:\n",
    "    # 단어 토큰화\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    result = []\n",
    "\n",
    "    for word in tokenized_sentence: \n",
    "        word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄인다.\n",
    "        if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거한다.\n",
    "            if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거한다.\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0 \n",
    "                vocab[word] += 1\n",
    "    preprocessed_sentences.append(result) \n",
    "print(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'there', 'other', \"won't\", 'mustn', 'can', 'each', 'both', 'again', 'doesn', 'do', 'for', 'it', 'their', 'so', 'my', 'further', 'more', 'shouldn', 'd', 'an', 'what', 'as', 'up', 't', 'you', 'are', 'won', 'themselves', 'most', 'am', 'couldn', \"should've\", 'these', 'hadn', 'who', 'herself', 'her', 'if', \"weren't\", \"aren't\", 'at', 'of', 'this', 'me', 'him', 'here', \"hasn't\", 'against', \"doesn't\", 'our', \"you've\", \"isn't\", 'now', 'hers', \"that'll\", 'just', 'don', 'didn', 'aren', \"couldn't\", 'too', 'over', 'had', 'not', 're', 'weren', 'been', 'but', 'all', 'through', 'before', \"shouldn't\", 'they', 's', 'mightn', 'your', 'a', 'will', 'its', 'from', 'which', 'then', 'under', 'whom', 'during', 'why', 'ma', \"she's\", 'yours', 'when', 'i', 'his', 'that', \"mustn't\", 'has', 'no', 'm', 'isn', 'such', 'few', 'y', 'yourself', 'own', \"shan't\", 'once', 'and', 'than', 'to', \"don't\", 'while', 'very', 'himself', 'be', 'she', 'itself', 'between', 'some', 'did', 'we', 'below', 'should', 'in', \"haven't\", 'having', \"needn't\", \"hadn't\", 'only', 'them', 'he', 'about', 'the', 'is', \"didn't\", 'on', \"you'd\", 'with', 'where', 'll', 'theirs', 'shan', 'myself', 'because', 'needn', 'o', \"wasn't\", 'were', 'being', 'or', 'until', \"you're\", 'down', 'wasn', 'ours', 'off', 'have', 'ain', \"mightn't\", 'haven', 'how', 'nor', \"it's\", 'above', 'yourselves', 'was', 'does', 'doing', 'ourselves', \"wouldn't\", 'into', 'any', 've', \"you'll\", 'out', 'same', 'after', 'by', 'wouldn', 'those', 'hasn'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
     ]
    }
   ],
   "source": [
    "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab_sorted :\n",
    "    if frequency > 1 : # 빈도수가 작은 단어는 제외.\n",
    "        i = i + 1\n",
    "        word_to_index[word] = i\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "words_frequency = [word for word, index in word_to_index.items() if index >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n",
    "for w in words_frequency:\n",
    "    del word_to_index[w] # 해당 단어에 대한 인덱스 정보를 삭제\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index['OOV'] = len(word_to_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barber', 'person'],\n",
       " ['barber', 'good', 'person'],\n",
       " ['barber', 'huge', 'person'],\n",
       " ['knew', 'secret'],\n",
       " ['secret', 'kept', 'huge', 'secret'],\n",
       " ['huge', 'secret'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'secret'],\n",
       " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
       " ['barber', 'went', 'huge', 'mountain']]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
     ]
    }
   ],
   "source": [
    "encoded_sentences = []\n",
    "for sentence in preprocessed_sentences:\n",
    "    encoded_sentence = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            encoded_sentence.append(word_to_index[word])\n",
    "        except KeyError:\n",
    "            encoded_sentence.append(word_to_index['OOV'])\n",
    "    encoded_sentences.append(encoded_sentence)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Counter 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barber', 'person'],\n",
       " ['barber', 'good', 'person'],\n",
       " ['barber', 'huge', 'person'],\n",
       " ['knew', 'secret'],\n",
       " ['secret', 'kept', 'huge', 'secret'],\n",
       " ['huge', 'secret'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'secret'],\n",
       " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
       " ['barber', 'went', 'huge', 'mountain']]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "# words = np.hstack(preprocessed_sentences)으로도 수행 가능.\n",
    "all_words_list = sum(preprocessed_sentences, [])\n",
    "print(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    }
   ],
   "source": [
    "# 파이썬의 Counter 모듈을 이용하여 단어의 빈도수 카운트\n",
    "vocab = Counter(all_words_list)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab :\n",
    "    i = i + 1\n",
    "    word_to_index[word] = i\n",
    "    \n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) NLTK의 FreqDist 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = FreqDist(np.hstack(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
    "print(word_to_index)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tokenization.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "25380c4af315aecd0af37ce090a32ef5cc426e5e777cf7d051bf86d7e79aec54"
  },
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
